{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9 (2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvM7iEJYOkKe",
        "outputId": "0da20548-38df-4320-d954-152d0d6bfc99"
      },
      "source": [
        "#%% imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader,ConcatDataset\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "#%% importing dataset and dataloader\n",
        "batch_size = 512\n",
        "transform = transforms.Compose([transforms.ColorJitter(),transforms.RandomHorizontalFlip(),transforms.RandomVerticalFlip(),transforms.ToTensor()])\n",
        "train_dataset = datasets.CIFAR10(root = '/',transform=transforms.ToTensor(),download=True)\n",
        "augmented = datasets.CIFAR10(root = '/',transform=transform,download=True)\n",
        "train_dataset = ConcatDataset([train_dataset,augmented])\n",
        "train_loader = DataLoader(dataset = train_dataset,batch_size = batch_size,shuffle=True)\n",
        "test_dataset = datasets.CIFAR10(root = '/',train = False,transform=transforms.ToTensor(),download=True)\n",
        "test_loader = DataLoader(dataset = test_dataset,batch_size = int(len(test_dataset)/10),shuffle=True)\n",
        "device = torch.device('cuda')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gO10MA3PjMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a00f456-ab63-403d-bdd2-db8cfea235fd"
      },
      "source": [
        "#%% network\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):   \n",
        "        super().__init__()\n",
        "        self.l1 = nn.Conv2d(3,32,3,padding = 1)\n",
        "        self.n1 = nn.BatchNorm2d(32)\n",
        "        self.l2 = nn.Conv2d(32,64,3,padding = 1)\n",
        "        self.n2 = nn.BatchNorm2d(64)\n",
        "        self.l3 = nn.Conv2d(64,64,3,padding = 1)\n",
        "        self.n3 = nn.BatchNorm2d(64)\n",
        "        self.l4 = nn.Conv2d(64,128,3,padding = 1)\n",
        "        self.n4 = nn.BatchNorm2d(128)\n",
        "        self.l5 = nn.Conv2d(128,128,3,padding = 1)\n",
        "        self.n5 = nn.BatchNorm2d(128)\n",
        "        self.l6 = nn.Conv2d(128,128,3,padding = 1)\n",
        "        self.n6 = nn.BatchNorm2d(128)\n",
        "        self.l7 = nn.Conv2d(128,128,3,padding = 1)\n",
        "        self.n7 = nn.BatchNorm2d(128)\n",
        "        self.l8 = nn.Conv2d(128,128,3,padding = 1)\n",
        "        self.n8 = nn.BatchNorm2d(128)\n",
        "        self.l9 = nn.Linear(512,128)\n",
        "        self.l10 = nn.Linear(128,10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.drop = nn.Dropout(p = 0.2)\n",
        "        self.drop2d = nn.Dropout2d(p = 0.2)\n",
        "        self.softmax = nn.Softmax()\n",
        "        self.pool = nn.MaxPool2d(2,stride = 2)\n",
        "    def forward(self,xt,flag = False):\n",
        "      x = xt\n",
        "      x = self.l1(x)\n",
        "      x = self.n1(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.pool(x)\n",
        "      if(flag):\n",
        "        x = self.drop2d(x)\n",
        "      x = self.l2(x)\n",
        "      x = self.n2(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.l3(x)\n",
        "      x = self.n3(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.pool(x)\n",
        "      if(flag):\n",
        "        x = self.drop2d(x)\n",
        "      x = self.l4(x)\n",
        "      x = self.n4(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.l5(x)\n",
        "      x = self.n5(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.l6(x)\n",
        "      x = self.n6(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.pool(x)\n",
        "      if(flag):\n",
        "        x = self.drop2d(x)\n",
        "      x = self.l7(x)\n",
        "      x = self.n7(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.pool(x)\n",
        "      if(flag):\n",
        "        x = self.drop2d(x)\n",
        "      x = self.l8(x)\n",
        "      x = self.n8(x)\n",
        "      x = self.relu(x)\n",
        "      x = torch.reshape(x,(x.shape[0],-1,1,1))\n",
        "      x = torch.squeeze(x,dim = 2)\n",
        "      x = torch.squeeze(x,dim = 2)\n",
        "      x = self.l9(x)\n",
        "      x = self.relu(x)\n",
        "      if(flag):\n",
        "        x = self.drop(x)\n",
        "      x = self.l10(x)\n",
        "      x = self.softmax(x)\n",
        "      return x\n",
        "\n",
        "model = Network()\n",
        "model.to(device)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (l1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (n1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (l2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (n2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (l3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (n3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (l4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (n4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (l5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (n5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (l6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (n6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (l7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (n7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (l8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (n8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (l9): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (l10): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (drop): Dropout(p=0.2, inplace=False)\n",
              "  (drop2d): Dropout2d(p=0.2, inplace=False)\n",
              "  (softmax): Softmax(dim=None)\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXuNKjhSPrVK"
      },
      "source": [
        "#%% loss and optimiser\n",
        "cost = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 0.0001)\n",
        "lr_decay = torch.optim.lr_scheduler.StepLR(optimizer,2,gamma = 0.8)\n",
        "def test(data):\n",
        "    c = 0\n",
        "    s = 0\n",
        "    for x,y in (data):\n",
        "        with torch.no_grad():\n",
        "            x =x.to(device)\n",
        "            y = y.to(device)\n",
        "            yt = model(x)\n",
        "            yt = torch.argmax(yt, dim= 1)\n",
        "            c = (y == yt).sum()\n",
        "            s = y.shape[0]\n",
        "        break\n",
        "    return (100*c/s).item()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDiQHfOehZoR",
        "outputId": "053f9b72-318d-406d-a220-f6199d58ec3a"
      },
      "source": [
        "model.load_state_dict(torch.load('/model_state1'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msdts2EtPwlp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8586f627-ecf9-4a83-b08a-b6f5b12c4f1e"
      },
      "source": [
        "#%% epochs\n",
        "epochs = 100\n",
        "lr_decay = torch.optim.lr_scheduler.StepLR(optimizer,10,gamma = 0.5)\n",
        "flag = False\n",
        "from google.colab import files\n",
        "for j in range(epochs):\n",
        "    for i,(xt,yt) in enumerate(train_loader) :\n",
        "        xt = xt.to(device)\n",
        "        yt = yt.to(device)\n",
        "        y_pred = model(xt,True)\n",
        "        loss = cost(y_pred,yt)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if(i%20==0):\n",
        "          acc = test(test_loader)\n",
        "          print(f'epoch {j+1} step {i} loss {loss} test_accuracy {acc} train_accuracy {test(train_loader)}')\n",
        "          if(acc>84):\n",
        "            flag = True\n",
        "            break\n",
        "        else:\n",
        "            print(f'epoch {j+1} step {i} loss {loss}')\n",
        "    if(flag):\n",
        "      break\n",
        "    lr_decay.step()\n",
        "    torch.save(model.state_dict(), '/model_state1')\n",
        "    torch.save(optimizer.state_dict(), '/optim_state1')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "epoch 8 step 81 loss 1.6346886157989502\n",
            "epoch 8 step 82 loss 1.6426011323928833\n",
            "epoch 8 step 83 loss 1.639945387840271\n",
            "epoch 8 step 84 loss 1.6520259380340576\n",
            "epoch 8 step 85 loss 1.6772704124450684\n",
            "epoch 8 step 86 loss 1.6549545526504517\n",
            "epoch 8 step 87 loss 1.6781038045883179\n",
            "epoch 8 step 88 loss 1.655622959136963\n",
            "epoch 8 step 89 loss 1.6322286128997803\n",
            "epoch 8 step 90 loss 1.6545056104660034\n",
            "epoch 8 step 91 loss 1.6367263793945312\n",
            "epoch 8 step 92 loss 1.6509989500045776\n",
            "epoch 8 step 93 loss 1.6577428579330444\n",
            "epoch 8 step 94 loss 1.6584699153900146\n",
            "epoch 8 step 95 loss 1.6876848936080933\n",
            "epoch 8 step 96 loss 1.6580097675323486\n",
            "epoch 8 step 97 loss 1.6626014709472656\n",
            "epoch 8 step 98 loss 1.652592420578003\n",
            "epoch 8 step 99 loss 1.6738935708999634\n",
            "epoch 8 step 100 loss 1.6614277362823486 test_accuracy 78.4000015258789 train_accuracy 86.1328125\n",
            "epoch 8 step 101 loss 1.6533496379852295\n",
            "epoch 8 step 102 loss 1.6420512199401855\n",
            "epoch 8 step 103 loss 1.6856567859649658\n",
            "epoch 8 step 104 loss 1.64284348487854\n",
            "epoch 8 step 105 loss 1.6403923034667969\n",
            "epoch 8 step 106 loss 1.625609278678894\n",
            "epoch 8 step 107 loss 1.6446512937545776\n",
            "epoch 8 step 108 loss 1.6869494915008545\n",
            "epoch 8 step 109 loss 1.672689437866211\n",
            "epoch 8 step 110 loss 1.6819111108779907\n",
            "epoch 8 step 111 loss 1.628243088722229\n",
            "epoch 8 step 112 loss 1.6649281978607178\n",
            "epoch 8 step 113 loss 1.662367582321167\n",
            "epoch 8 step 114 loss 1.6654504537582397\n",
            "epoch 8 step 115 loss 1.643359661102295\n",
            "epoch 8 step 116 loss 1.6490631103515625\n",
            "epoch 8 step 117 loss 1.6512043476104736\n",
            "epoch 8 step 118 loss 1.6552317142486572\n",
            "epoch 8 step 119 loss 1.6343110799789429\n",
            "epoch 8 step 120 loss 1.6510558128356934 test_accuracy 81.4000015258789 train_accuracy 83.984375\n",
            "epoch 8 step 121 loss 1.6758273839950562\n",
            "epoch 8 step 122 loss 1.6614114046096802\n",
            "epoch 8 step 123 loss 1.676732063293457\n",
            "epoch 8 step 124 loss 1.667412519454956\n",
            "epoch 8 step 125 loss 1.6374443769454956\n",
            "epoch 8 step 126 loss 1.6576530933380127\n",
            "epoch 8 step 127 loss 1.670798897743225\n",
            "epoch 8 step 128 loss 1.6640338897705078\n",
            "epoch 8 step 129 loss 1.6501091718673706\n",
            "epoch 8 step 130 loss 1.6508280038833618\n",
            "epoch 8 step 131 loss 1.6667606830596924\n",
            "epoch 8 step 132 loss 1.655539631843567\n",
            "epoch 8 step 133 loss 1.6448830366134644\n",
            "epoch 8 step 134 loss 1.6567802429199219\n",
            "epoch 8 step 135 loss 1.6833229064941406\n",
            "epoch 8 step 136 loss 1.656121850013733\n",
            "epoch 8 step 137 loss 1.649195671081543\n",
            "epoch 8 step 138 loss 1.6543539762496948\n",
            "epoch 8 step 139 loss 1.6509504318237305\n",
            "epoch 8 step 140 loss 1.6772019863128662 test_accuracy 81.4000015258789 train_accuracy 86.71875\n",
            "epoch 8 step 141 loss 1.6390833854675293\n",
            "epoch 8 step 142 loss 1.621252417564392\n",
            "epoch 8 step 143 loss 1.6683181524276733\n",
            "epoch 8 step 144 loss 1.685236930847168\n",
            "epoch 8 step 145 loss 1.6627992391586304\n",
            "epoch 8 step 146 loss 1.6697341203689575\n",
            "epoch 8 step 147 loss 1.6567281484603882\n",
            "epoch 8 step 148 loss 1.6724871397018433\n",
            "epoch 8 step 149 loss 1.6713110208511353\n",
            "epoch 8 step 150 loss 1.6543153524398804\n",
            "epoch 8 step 151 loss 1.6467088460922241\n",
            "epoch 8 step 152 loss 1.6539729833602905\n",
            "epoch 8 step 153 loss 1.6680485010147095\n",
            "epoch 8 step 154 loss 1.656615972518921\n",
            "epoch 8 step 155 loss 1.6518168449401855\n",
            "epoch 8 step 156 loss 1.6266764402389526\n",
            "epoch 8 step 157 loss 1.6546063423156738\n",
            "epoch 8 step 158 loss 1.6420376300811768\n",
            "epoch 8 step 159 loss 1.6685541868209839\n",
            "epoch 8 step 160 loss 1.671541690826416 test_accuracy 79.30000305175781 train_accuracy 83.203125\n",
            "epoch 8 step 161 loss 1.6625409126281738\n",
            "epoch 8 step 162 loss 1.6554863452911377\n",
            "epoch 8 step 163 loss 1.6945993900299072\n",
            "epoch 8 step 164 loss 1.691003441810608\n",
            "epoch 8 step 165 loss 1.6545013189315796\n",
            "epoch 8 step 166 loss 1.6788517236709595\n",
            "epoch 8 step 167 loss 1.6510024070739746\n",
            "epoch 8 step 168 loss 1.665053129196167\n",
            "epoch 8 step 169 loss 1.6821174621582031\n",
            "epoch 8 step 170 loss 1.6633964776992798\n",
            "epoch 8 step 171 loss 1.645950436592102\n",
            "epoch 8 step 172 loss 1.6431549787521362\n",
            "epoch 8 step 173 loss 1.6592950820922852\n",
            "epoch 8 step 174 loss 1.6657373905181885\n",
            "epoch 8 step 175 loss 1.6621607542037964\n",
            "epoch 8 step 176 loss 1.6879634857177734\n",
            "epoch 8 step 177 loss 1.6694856882095337\n",
            "epoch 8 step 178 loss 1.6426812410354614\n",
            "epoch 8 step 179 loss 1.6815142631530762\n",
            "epoch 8 step 180 loss 1.680267572402954 test_accuracy 79.5 train_accuracy 85.7421875\n",
            "epoch 8 step 181 loss 1.653836727142334\n",
            "epoch 8 step 182 loss 1.6520702838897705\n",
            "epoch 8 step 183 loss 1.642196774482727\n",
            "epoch 8 step 184 loss 1.6575685739517212\n",
            "epoch 8 step 185 loss 1.6138750314712524\n",
            "epoch 8 step 186 loss 1.6597764492034912\n",
            "epoch 8 step 187 loss 1.6443183422088623\n",
            "epoch 8 step 188 loss 1.666167974472046\n",
            "epoch 8 step 189 loss 1.6535708904266357\n",
            "epoch 8 step 190 loss 1.6441655158996582\n",
            "epoch 8 step 191 loss 1.6364402770996094\n",
            "epoch 8 step 192 loss 1.6597336530685425\n",
            "epoch 8 step 193 loss 1.66115140914917\n",
            "epoch 8 step 194 loss 1.6529700756072998\n",
            "epoch 8 step 195 loss 1.667716383934021\n",
            "epoch 9 step 0 loss 1.6649460792541504 test_accuracy 80.20000457763672 train_accuracy 86.328125\n",
            "epoch 9 step 1 loss 1.6636672019958496\n",
            "epoch 9 step 2 loss 1.6529872417449951\n",
            "epoch 9 step 3 loss 1.6766796112060547\n",
            "epoch 9 step 4 loss 1.6508779525756836\n",
            "epoch 9 step 5 loss 1.6591343879699707\n",
            "epoch 9 step 6 loss 1.662948489189148\n",
            "epoch 9 step 7 loss 1.6427619457244873\n",
            "epoch 9 step 8 loss 1.6706674098968506\n",
            "epoch 9 step 9 loss 1.667656660079956\n",
            "epoch 9 step 10 loss 1.6489317417144775\n",
            "epoch 9 step 11 loss 1.6613012552261353\n",
            "epoch 9 step 12 loss 1.663886308670044\n",
            "epoch 9 step 13 loss 1.6474028825759888\n",
            "epoch 9 step 14 loss 1.6568992137908936\n",
            "epoch 9 step 15 loss 1.6544958353042603\n",
            "epoch 9 step 16 loss 1.6577730178833008\n",
            "epoch 9 step 17 loss 1.6420915126800537\n",
            "epoch 9 step 18 loss 1.647443175315857\n",
            "epoch 9 step 19 loss 1.6524993181228638\n",
            "epoch 9 step 20 loss 1.6790251731872559 test_accuracy 77.9000015258789 train_accuracy 86.71875\n",
            "epoch 9 step 21 loss 1.6448637247085571\n",
            "epoch 9 step 22 loss 1.6530120372772217\n",
            "epoch 9 step 23 loss 1.6931216716766357\n",
            "epoch 9 step 24 loss 1.6606630086898804\n",
            "epoch 9 step 25 loss 1.6560883522033691\n",
            "epoch 9 step 26 loss 1.6582170724868774\n",
            "epoch 9 step 27 loss 1.651951789855957\n",
            "epoch 9 step 28 loss 1.6593810319900513\n",
            "epoch 9 step 29 loss 1.6556146144866943\n",
            "epoch 9 step 30 loss 1.6538808345794678\n",
            "epoch 9 step 31 loss 1.634751796722412\n",
            "epoch 9 step 32 loss 1.648965835571289\n",
            "epoch 9 step 33 loss 1.6620533466339111\n",
            "epoch 9 step 34 loss 1.6582021713256836\n",
            "epoch 9 step 35 loss 1.6760914325714111\n",
            "epoch 9 step 36 loss 1.6525697708129883\n",
            "epoch 9 step 37 loss 1.646062970161438\n",
            "epoch 9 step 38 loss 1.6310943365097046\n",
            "epoch 9 step 39 loss 1.656491756439209\n",
            "epoch 9 step 40 loss 1.6669206619262695 test_accuracy 79.10000610351562 train_accuracy 84.375\n",
            "epoch 9 step 41 loss 1.6540253162384033\n",
            "epoch 9 step 42 loss 1.6730316877365112\n",
            "epoch 9 step 43 loss 1.6440277099609375\n",
            "epoch 9 step 44 loss 1.6346282958984375\n",
            "epoch 9 step 45 loss 1.6522001028060913\n",
            "epoch 9 step 46 loss 1.6639792919158936\n",
            "epoch 9 step 47 loss 1.68240487575531\n",
            "epoch 9 step 48 loss 1.6727529764175415\n",
            "epoch 9 step 49 loss 1.667182207107544\n",
            "epoch 9 step 50 loss 1.6598299741744995\n",
            "epoch 9 step 51 loss 1.6447691917419434\n",
            "epoch 9 step 52 loss 1.6697783470153809\n",
            "epoch 9 step 53 loss 1.678547739982605\n",
            "epoch 9 step 54 loss 1.6639724969863892\n",
            "epoch 9 step 55 loss 1.6496217250823975\n",
            "epoch 9 step 56 loss 1.6474263668060303\n",
            "epoch 9 step 57 loss 1.6663360595703125\n",
            "epoch 9 step 58 loss 1.6741161346435547\n",
            "epoch 9 step 59 loss 1.6594942808151245\n",
            "epoch 9 step 60 loss 1.6336281299591064 test_accuracy 82.00000762939453 train_accuracy 84.9609375\n",
            "epoch 9 step 61 loss 1.6752506494522095\n",
            "epoch 9 step 62 loss 1.6667981147766113\n",
            "epoch 9 step 63 loss 1.6722968816757202\n",
            "epoch 9 step 64 loss 1.6657377481460571\n",
            "epoch 9 step 65 loss 1.6524536609649658\n",
            "epoch 9 step 66 loss 1.655476689338684\n",
            "epoch 9 step 67 loss 1.636892318725586\n",
            "epoch 9 step 68 loss 1.6734610795974731\n",
            "epoch 9 step 69 loss 1.6529866456985474\n",
            "epoch 9 step 70 loss 1.6486543416976929\n",
            "epoch 9 step 71 loss 1.6712448596954346\n",
            "epoch 9 step 72 loss 1.6532130241394043\n",
            "epoch 9 step 73 loss 1.6656360626220703\n",
            "epoch 9 step 74 loss 1.665754795074463\n",
            "epoch 9 step 75 loss 1.688812017440796\n",
            "epoch 9 step 76 loss 1.6485947370529175\n",
            "epoch 9 step 77 loss 1.6641926765441895\n",
            "epoch 9 step 78 loss 1.6620091199874878\n",
            "epoch 9 step 79 loss 1.6700518131256104\n",
            "epoch 9 step 80 loss 1.6850746870040894 test_accuracy 80.60000610351562 train_accuracy 85.9375\n",
            "epoch 9 step 81 loss 1.654226303100586\n",
            "epoch 9 step 82 loss 1.6962831020355225\n",
            "epoch 9 step 83 loss 1.643599033355713\n",
            "epoch 9 step 84 loss 1.6852178573608398\n",
            "epoch 9 step 85 loss 1.656497597694397\n",
            "epoch 9 step 86 loss 1.651707649230957\n",
            "epoch 9 step 87 loss 1.6656992435455322\n",
            "epoch 9 step 88 loss 1.6456282138824463\n",
            "epoch 9 step 89 loss 1.6852318048477173\n",
            "epoch 9 step 90 loss 1.6771814823150635\n",
            "epoch 9 step 91 loss 1.6807761192321777\n",
            "epoch 9 step 92 loss 1.6624586582183838\n",
            "epoch 9 step 93 loss 1.6394797563552856\n",
            "epoch 9 step 94 loss 1.669500470161438\n",
            "epoch 9 step 95 loss 1.6509554386138916\n",
            "epoch 9 step 96 loss 1.6510478258132935\n",
            "epoch 9 step 97 loss 1.6798503398895264\n",
            "epoch 9 step 98 loss 1.650695562362671\n",
            "epoch 9 step 99 loss 1.670441746711731\n",
            "epoch 9 step 100 loss 1.6439989805221558 test_accuracy 81.10000610351562 train_accuracy 87.3046875\n",
            "epoch 9 step 101 loss 1.661326289176941\n",
            "epoch 9 step 102 loss 1.6616215705871582\n",
            "epoch 9 step 103 loss 1.7020535469055176\n",
            "epoch 9 step 104 loss 1.6643567085266113\n",
            "epoch 9 step 105 loss 1.6394658088684082\n",
            "epoch 9 step 106 loss 1.6689211130142212\n",
            "epoch 9 step 107 loss 1.6536442041397095\n",
            "epoch 9 step 108 loss 1.6698652505874634\n",
            "epoch 9 step 109 loss 1.6407790184020996\n",
            "epoch 9 step 110 loss 1.6248911619186401\n",
            "epoch 9 step 111 loss 1.641701102256775\n",
            "epoch 9 step 112 loss 1.6813009977340698\n",
            "epoch 9 step 113 loss 1.6559386253356934\n",
            "epoch 9 step 114 loss 1.6440335512161255\n",
            "epoch 9 step 115 loss 1.6925629377365112\n",
            "epoch 9 step 116 loss 1.6652694940567017\n",
            "epoch 9 step 117 loss 1.6760380268096924\n",
            "epoch 9 step 118 loss 1.6599125862121582\n",
            "epoch 9 step 119 loss 1.688372254371643\n",
            "epoch 9 step 120 loss 1.6307544708251953 test_accuracy 81.30000305175781 train_accuracy 85.7421875\n",
            "epoch 9 step 121 loss 1.667521357536316\n",
            "epoch 9 step 122 loss 1.6553304195404053\n",
            "epoch 9 step 123 loss 1.6586263179779053\n",
            "epoch 9 step 124 loss 1.6657350063323975\n",
            "epoch 9 step 125 loss 1.6431914567947388\n",
            "epoch 9 step 126 loss 1.6324213743209839\n",
            "epoch 9 step 127 loss 1.6675465106964111\n",
            "epoch 9 step 128 loss 1.6535183191299438\n",
            "epoch 9 step 129 loss 1.6648430824279785\n",
            "epoch 9 step 130 loss 1.664164662361145\n",
            "epoch 9 step 131 loss 1.6776646375656128\n",
            "epoch 9 step 132 loss 1.6700841188430786\n",
            "epoch 9 step 133 loss 1.6331896781921387\n",
            "epoch 9 step 134 loss 1.621750831604004\n",
            "epoch 9 step 135 loss 1.671758770942688\n",
            "epoch 9 step 136 loss 1.650022029876709\n",
            "epoch 9 step 137 loss 1.6506690979003906\n",
            "epoch 9 step 138 loss 1.6692346334457397\n",
            "epoch 9 step 139 loss 1.67339289188385\n",
            "epoch 9 step 140 loss 1.6413544416427612 test_accuracy 80.70000457763672 train_accuracy 84.5703125\n",
            "epoch 9 step 141 loss 1.6751848459243774\n",
            "epoch 9 step 142 loss 1.6338807344436646\n",
            "epoch 9 step 143 loss 1.6731595993041992\n",
            "epoch 9 step 144 loss 1.638629674911499\n",
            "epoch 9 step 145 loss 1.6437201499938965\n",
            "epoch 9 step 146 loss 1.6705245971679688\n",
            "epoch 9 step 147 loss 1.663947343826294\n",
            "epoch 9 step 148 loss 1.6580839157104492\n",
            "epoch 9 step 149 loss 1.670920491218567\n",
            "epoch 9 step 150 loss 1.6530311107635498\n",
            "epoch 9 step 151 loss 1.6873947381973267\n",
            "epoch 9 step 152 loss 1.6702314615249634\n",
            "epoch 9 step 153 loss 1.6371089220046997\n",
            "epoch 9 step 154 loss 1.654143214225769\n",
            "epoch 9 step 155 loss 1.6538015604019165\n",
            "epoch 9 step 156 loss 1.6468074321746826\n",
            "epoch 9 step 157 loss 1.6619205474853516\n",
            "epoch 9 step 158 loss 1.6549465656280518\n",
            "epoch 9 step 159 loss 1.6462944746017456\n",
            "epoch 9 step 160 loss 1.6547545194625854 test_accuracy 82.4000015258789 train_accuracy 83.7890625\n",
            "epoch 9 step 161 loss 1.6782057285308838\n",
            "epoch 9 step 162 loss 1.6603291034698486\n",
            "epoch 9 step 163 loss 1.6602340936660767\n",
            "epoch 9 step 164 loss 1.6448240280151367\n",
            "epoch 9 step 165 loss 1.6349265575408936\n",
            "epoch 9 step 166 loss 1.6468226909637451\n",
            "epoch 9 step 167 loss 1.676466703414917\n",
            "epoch 9 step 168 loss 1.6641863584518433\n",
            "epoch 9 step 169 loss 1.6457514762878418\n",
            "epoch 9 step 170 loss 1.6625562906265259\n",
            "epoch 9 step 171 loss 1.63967764377594\n",
            "epoch 9 step 172 loss 1.6595110893249512\n",
            "epoch 9 step 173 loss 1.6266496181488037\n",
            "epoch 9 step 174 loss 1.6635239124298096\n",
            "epoch 9 step 175 loss 1.6709877252578735\n",
            "epoch 9 step 176 loss 1.6688898801803589\n",
            "epoch 9 step 177 loss 1.651259422302246\n",
            "epoch 9 step 178 loss 1.6277821063995361\n",
            "epoch 9 step 179 loss 1.6668084859848022\n",
            "epoch 9 step 180 loss 1.6737046241760254 test_accuracy 79.9000015258789 train_accuracy 87.109375\n",
            "epoch 9 step 181 loss 1.6605150699615479\n",
            "epoch 9 step 182 loss 1.6629847288131714\n",
            "epoch 9 step 183 loss 1.6578047275543213\n",
            "epoch 9 step 184 loss 1.6362100839614868\n",
            "epoch 9 step 185 loss 1.6383577585220337\n",
            "epoch 9 step 186 loss 1.677746057510376\n",
            "epoch 9 step 187 loss 1.6839570999145508\n",
            "epoch 9 step 188 loss 1.6439498662948608\n",
            "epoch 9 step 189 loss 1.64409601688385\n",
            "epoch 9 step 190 loss 1.6444928646087646\n",
            "epoch 9 step 191 loss 1.629109263420105\n",
            "epoch 9 step 192 loss 1.6369911432266235\n",
            "epoch 9 step 193 loss 1.6751883029937744\n",
            "epoch 9 step 194 loss 1.653144359588623\n",
            "epoch 9 step 195 loss 1.6870065927505493\n",
            "epoch 10 step 0 loss 1.6612448692321777 test_accuracy 82.10000610351562 train_accuracy 85.15625\n",
            "epoch 10 step 1 loss 1.6584410667419434\n",
            "epoch 10 step 2 loss 1.6524829864501953\n",
            "epoch 10 step 3 loss 1.639251708984375\n",
            "epoch 10 step 4 loss 1.6785038709640503\n",
            "epoch 10 step 5 loss 1.6610462665557861\n",
            "epoch 10 step 6 loss 1.6515417098999023\n",
            "epoch 10 step 7 loss 1.6468472480773926\n",
            "epoch 10 step 8 loss 1.6530890464782715\n",
            "epoch 10 step 9 loss 1.6176519393920898\n",
            "epoch 10 step 10 loss 1.6307026147842407\n",
            "epoch 10 step 11 loss 1.652808427810669\n",
            "epoch 10 step 12 loss 1.6410400867462158\n",
            "epoch 10 step 13 loss 1.6545472145080566\n",
            "epoch 10 step 14 loss 1.649895191192627\n",
            "epoch 10 step 15 loss 1.6391505002975464\n",
            "epoch 10 step 16 loss 1.6538448333740234\n",
            "epoch 10 step 17 loss 1.6618096828460693\n",
            "epoch 10 step 18 loss 1.662806510925293\n",
            "epoch 10 step 19 loss 1.6533955335617065\n",
            "epoch 10 step 20 loss 1.6468050479888916 test_accuracy 81.20000457763672 train_accuracy 86.1328125\n",
            "epoch 10 step 21 loss 1.6507196426391602\n",
            "epoch 10 step 22 loss 1.6384388208389282\n",
            "epoch 10 step 23 loss 1.6389479637145996\n",
            "epoch 10 step 24 loss 1.6444363594055176\n",
            "epoch 10 step 25 loss 1.653692603111267\n",
            "epoch 10 step 26 loss 1.6630077362060547\n",
            "epoch 10 step 27 loss 1.634132981300354\n",
            "epoch 10 step 28 loss 1.6658120155334473\n",
            "epoch 10 step 29 loss 1.6594371795654297\n",
            "epoch 10 step 30 loss 1.6687184572219849\n",
            "epoch 10 step 31 loss 1.6483964920043945\n",
            "epoch 10 step 32 loss 1.6828242540359497\n",
            "epoch 10 step 33 loss 1.6434085369110107\n",
            "epoch 10 step 34 loss 1.6604359149932861\n",
            "epoch 10 step 35 loss 1.6563769578933716\n",
            "epoch 10 step 36 loss 1.6381438970565796\n",
            "epoch 10 step 37 loss 1.646920084953308\n",
            "epoch 10 step 38 loss 1.6566998958587646\n",
            "epoch 10 step 39 loss 1.6613413095474243\n",
            "epoch 10 step 40 loss 1.6523045301437378 test_accuracy 79.0 train_accuracy 86.328125\n",
            "epoch 10 step 41 loss 1.648228406906128\n",
            "epoch 10 step 42 loss 1.6883944272994995\n",
            "epoch 10 step 43 loss 1.6494320631027222\n",
            "epoch 10 step 44 loss 1.6579949855804443\n",
            "epoch 10 step 45 loss 1.645494818687439\n",
            "epoch 10 step 46 loss 1.6547982692718506\n",
            "epoch 10 step 47 loss 1.6514016389846802\n",
            "epoch 10 step 48 loss 1.6690599918365479\n",
            "epoch 10 step 49 loss 1.6505286693572998\n",
            "epoch 10 step 50 loss 1.6596403121948242\n",
            "epoch 10 step 51 loss 1.666398286819458\n",
            "epoch 10 step 52 loss 1.644135594367981\n",
            "epoch 10 step 53 loss 1.6591483354568481\n",
            "epoch 10 step 54 loss 1.6631364822387695\n",
            "epoch 10 step 55 loss 1.6642547845840454\n",
            "epoch 10 step 56 loss 1.6870484352111816\n",
            "epoch 10 step 57 loss 1.6438924074172974\n",
            "epoch 10 step 58 loss 1.684532642364502\n",
            "epoch 10 step 59 loss 1.6621625423431396\n",
            "epoch 10 step 60 loss 1.6658841371536255 test_accuracy 79.60000610351562 train_accuracy 85.3515625\n",
            "epoch 10 step 61 loss 1.6438626050949097\n",
            "epoch 10 step 62 loss 1.7017794847488403\n",
            "epoch 10 step 63 loss 1.6558377742767334\n",
            "epoch 10 step 64 loss 1.6766250133514404\n",
            "epoch 10 step 65 loss 1.6689162254333496\n",
            "epoch 10 step 66 loss 1.7010295391082764\n",
            "epoch 10 step 67 loss 1.6898797750473022\n",
            "epoch 10 step 68 loss 1.6530414819717407\n",
            "epoch 10 step 69 loss 1.6521971225738525\n",
            "epoch 10 step 70 loss 1.7008256912231445\n",
            "epoch 10 step 71 loss 1.6601109504699707\n",
            "epoch 10 step 72 loss 1.6235713958740234\n",
            "epoch 10 step 73 loss 1.6514966487884521\n",
            "epoch 10 step 74 loss 1.656747579574585\n",
            "epoch 10 step 75 loss 1.6500885486602783\n",
            "epoch 10 step 76 loss 1.663755178451538\n",
            "epoch 10 step 77 loss 1.6728169918060303\n",
            "epoch 10 step 78 loss 1.6477857828140259\n",
            "epoch 10 step 79 loss 1.645060420036316\n",
            "epoch 10 step 80 loss 1.636763334274292 test_accuracy 80.0 train_accuracy 84.9609375\n",
            "epoch 10 step 81 loss 1.6455512046813965\n",
            "epoch 10 step 82 loss 1.65239417552948\n",
            "epoch 10 step 83 loss 1.6493977308273315\n",
            "epoch 10 step 84 loss 1.6251940727233887\n",
            "epoch 10 step 85 loss 1.655416488647461\n",
            "epoch 10 step 86 loss 1.6449248790740967\n",
            "epoch 10 step 87 loss 1.6483389139175415\n",
            "epoch 10 step 88 loss 1.6468403339385986\n",
            "epoch 10 step 89 loss 1.669740915298462\n",
            "epoch 10 step 90 loss 1.6712795495986938\n",
            "epoch 10 step 91 loss 1.6516311168670654\n",
            "epoch 10 step 92 loss 1.6850659847259521\n",
            "epoch 10 step 93 loss 1.638352632522583\n",
            "epoch 10 step 94 loss 1.6373507976531982\n",
            "epoch 10 step 95 loss 1.6773970127105713\n",
            "epoch 10 step 96 loss 1.6461244821548462\n",
            "epoch 10 step 97 loss 1.6355093717575073\n",
            "epoch 10 step 98 loss 1.6732025146484375\n",
            "epoch 10 step 99 loss 1.6565001010894775\n",
            "epoch 10 step 100 loss 1.6729474067687988 test_accuracy 78.60000610351562 train_accuracy 83.0078125\n",
            "epoch 10 step 101 loss 1.660207748413086\n",
            "epoch 10 step 102 loss 1.6616837978363037\n",
            "epoch 10 step 103 loss 1.6323045492172241\n",
            "epoch 10 step 104 loss 1.6201273202896118\n",
            "epoch 10 step 105 loss 1.672537088394165\n",
            "epoch 10 step 106 loss 1.6697543859481812\n",
            "epoch 10 step 107 loss 1.6651058197021484\n",
            "epoch 10 step 108 loss 1.6716253757476807\n",
            "epoch 10 step 109 loss 1.6267482042312622\n",
            "epoch 10 step 110 loss 1.6514554023742676\n",
            "epoch 10 step 111 loss 1.6493796110153198\n",
            "epoch 10 step 112 loss 1.6487828493118286\n",
            "epoch 10 step 113 loss 1.6711353063583374\n",
            "epoch 10 step 114 loss 1.640245795249939\n",
            "epoch 10 step 115 loss 1.6576443910598755\n",
            "epoch 10 step 116 loss 1.652489423751831\n",
            "epoch 10 step 117 loss 1.6480791568756104\n",
            "epoch 10 step 118 loss 1.6473339796066284\n",
            "epoch 10 step 119 loss 1.64641535282135\n",
            "epoch 10 step 120 loss 1.6578867435455322 test_accuracy 78.4000015258789 train_accuracy 83.984375\n",
            "epoch 10 step 121 loss 1.660771131515503\n",
            "epoch 10 step 122 loss 1.6309850215911865\n",
            "epoch 10 step 123 loss 1.6598495244979858\n",
            "epoch 10 step 124 loss 1.6667603254318237\n",
            "epoch 10 step 125 loss 1.6748037338256836\n",
            "epoch 10 step 126 loss 1.662841558456421\n",
            "epoch 10 step 127 loss 1.659328818321228\n",
            "epoch 10 step 128 loss 1.6752665042877197\n",
            "epoch 10 step 129 loss 1.654992938041687\n",
            "epoch 10 step 130 loss 1.670845866203308\n",
            "epoch 10 step 131 loss 1.6492953300476074\n",
            "epoch 10 step 132 loss 1.6437666416168213\n",
            "epoch 10 step 133 loss 1.600935697555542\n",
            "epoch 10 step 134 loss 1.6670225858688354\n",
            "epoch 10 step 135 loss 1.6499958038330078\n",
            "epoch 10 step 136 loss 1.6454752683639526\n",
            "epoch 10 step 137 loss 1.6364796161651611\n",
            "epoch 10 step 138 loss 1.6501833200454712\n",
            "epoch 10 step 139 loss 1.6657025814056396\n",
            "epoch 10 step 140 loss 1.662227988243103 test_accuracy 80.20000457763672 train_accuracy 85.546875\n",
            "epoch 10 step 141 loss 1.6609138250350952\n",
            "epoch 10 step 142 loss 1.650971531867981\n",
            "epoch 10 step 143 loss 1.6659126281738281\n",
            "epoch 10 step 144 loss 1.647915005683899\n",
            "epoch 10 step 145 loss 1.6389532089233398\n",
            "epoch 10 step 146 loss 1.6823643445968628\n",
            "epoch 10 step 147 loss 1.6435056924819946\n",
            "epoch 10 step 148 loss 1.6707438230514526\n",
            "epoch 10 step 149 loss 1.6452817916870117\n",
            "epoch 10 step 150 loss 1.6829744577407837\n",
            "epoch 10 step 151 loss 1.6357582807540894\n",
            "epoch 10 step 152 loss 1.700559377670288\n",
            "epoch 10 step 153 loss 1.6467372179031372\n",
            "epoch 10 step 154 loss 1.6632201671600342\n",
            "epoch 10 step 155 loss 1.6799368858337402\n",
            "epoch 10 step 156 loss 1.6588945388793945\n",
            "epoch 10 step 157 loss 1.6442065238952637\n",
            "epoch 10 step 158 loss 1.653911828994751\n",
            "epoch 10 step 159 loss 1.6714376211166382\n",
            "epoch 10 step 160 loss 1.6571120023727417 test_accuracy 82.80000305175781 train_accuracy 86.5234375\n",
            "epoch 10 step 161 loss 1.6631691455841064\n",
            "epoch 10 step 162 loss 1.6862698793411255\n",
            "epoch 10 step 163 loss 1.6538053750991821\n",
            "epoch 10 step 164 loss 1.6571078300476074\n",
            "epoch 10 step 165 loss 1.6297264099121094\n",
            "epoch 10 step 166 loss 1.6637479066848755\n",
            "epoch 10 step 167 loss 1.6534619331359863\n",
            "epoch 10 step 168 loss 1.642573356628418\n",
            "epoch 10 step 169 loss 1.691495418548584\n",
            "epoch 10 step 170 loss 1.6458150148391724\n",
            "epoch 10 step 171 loss 1.6603076457977295\n",
            "epoch 10 step 172 loss 1.6173280477523804\n",
            "epoch 10 step 173 loss 1.6926792860031128\n",
            "epoch 10 step 174 loss 1.6370211839675903\n",
            "epoch 10 step 175 loss 1.6828696727752686\n",
            "epoch 10 step 176 loss 1.650866150856018\n",
            "epoch 10 step 177 loss 1.656798005104065\n",
            "epoch 10 step 178 loss 1.6521917581558228\n",
            "epoch 10 step 179 loss 1.682394027709961\n",
            "epoch 10 step 180 loss 1.6698867082595825 test_accuracy 79.10000610351562 train_accuracy 87.6953125\n",
            "epoch 10 step 181 loss 1.6554311513900757\n",
            "epoch 10 step 182 loss 1.6492457389831543\n",
            "epoch 10 step 183 loss 1.6907978057861328\n",
            "epoch 10 step 184 loss 1.670486569404602\n",
            "epoch 10 step 185 loss 1.6615540981292725\n",
            "epoch 10 step 186 loss 1.682094931602478\n",
            "epoch 10 step 187 loss 1.6664265394210815\n",
            "epoch 10 step 188 loss 1.6812684535980225\n",
            "epoch 10 step 189 loss 1.6453841924667358\n",
            "epoch 10 step 190 loss 1.6511520147323608\n",
            "epoch 10 step 191 loss 1.6569833755493164\n",
            "epoch 10 step 192 loss 1.6530585289001465\n",
            "epoch 10 step 193 loss 1.6746644973754883\n",
            "epoch 10 step 194 loss 1.6916793584823608\n",
            "epoch 10 step 195 loss 1.6504751443862915\n",
            "epoch 11 step 0 loss 1.6546403169631958 test_accuracy 79.10000610351562 train_accuracy 84.9609375\n",
            "epoch 11 step 1 loss 1.6454315185546875\n",
            "epoch 11 step 2 loss 1.6378682851791382\n",
            "epoch 11 step 3 loss 1.6637136936187744\n",
            "epoch 11 step 4 loss 1.6505271196365356\n",
            "epoch 11 step 5 loss 1.649402379989624\n",
            "epoch 11 step 6 loss 1.6659623384475708\n",
            "epoch 11 step 7 loss 1.6570274829864502\n",
            "epoch 11 step 8 loss 1.6588845252990723\n",
            "epoch 11 step 9 loss 1.6771669387817383\n",
            "epoch 11 step 10 loss 1.6484726667404175\n",
            "epoch 11 step 11 loss 1.6505874395370483\n",
            "epoch 11 step 12 loss 1.6329092979431152\n",
            "epoch 11 step 13 loss 1.6278321743011475\n",
            "epoch 11 step 14 loss 1.6605998277664185\n",
            "epoch 11 step 15 loss 1.6485745906829834\n",
            "epoch 11 step 16 loss 1.6452690362930298\n",
            "epoch 11 step 17 loss 1.6657135486602783\n",
            "epoch 11 step 18 loss 1.6485724449157715\n",
            "epoch 11 step 19 loss 1.632717490196228\n",
            "epoch 11 step 20 loss 1.6282650232315063 test_accuracy 82.10000610351562 train_accuracy 87.109375\n",
            "epoch 11 step 21 loss 1.6595377922058105\n",
            "epoch 11 step 22 loss 1.6634269952774048\n",
            "epoch 11 step 23 loss 1.6597555875778198\n",
            "epoch 11 step 24 loss 1.6618634462356567\n",
            "epoch 11 step 25 loss 1.6191632747650146\n",
            "epoch 11 step 26 loss 1.6481941938400269\n",
            "epoch 11 step 27 loss 1.6897015571594238\n",
            "epoch 11 step 28 loss 1.6518465280532837\n",
            "epoch 11 step 29 loss 1.653212547302246\n",
            "epoch 11 step 30 loss 1.6538039445877075\n",
            "epoch 11 step 31 loss 1.6811673641204834\n",
            "epoch 11 step 32 loss 1.6241627931594849\n",
            "epoch 11 step 33 loss 1.639687180519104\n",
            "epoch 11 step 34 loss 1.6655800342559814\n",
            "epoch 11 step 35 loss 1.6505131721496582\n",
            "epoch 11 step 36 loss 1.643797755241394\n",
            "epoch 11 step 37 loss 1.6659207344055176\n",
            "epoch 11 step 38 loss 1.6322224140167236\n",
            "epoch 11 step 39 loss 1.6251587867736816\n",
            "epoch 11 step 40 loss 1.6629490852355957 test_accuracy 80.50000762939453 train_accuracy 83.3984375\n",
            "epoch 11 step 41 loss 1.661935806274414\n",
            "epoch 11 step 42 loss 1.6484527587890625\n",
            "epoch 11 step 43 loss 1.6286839246749878\n",
            "epoch 11 step 44 loss 1.6225391626358032\n",
            "epoch 11 step 45 loss 1.6114871501922607\n",
            "epoch 11 step 46 loss 1.6644675731658936\n",
            "epoch 11 step 47 loss 1.6458244323730469\n",
            "epoch 11 step 48 loss 1.690922498703003\n",
            "epoch 11 step 49 loss 1.6323885917663574\n",
            "epoch 11 step 50 loss 1.6539807319641113\n",
            "epoch 11 step 51 loss 1.6373271942138672\n",
            "epoch 11 step 52 loss 1.6332045793533325\n",
            "epoch 11 step 53 loss 1.649354338645935\n",
            "epoch 11 step 54 loss 1.6604710817337036\n",
            "epoch 11 step 55 loss 1.658614993095398\n",
            "epoch 11 step 56 loss 1.6507267951965332\n",
            "epoch 11 step 57 loss 1.625546932220459\n",
            "epoch 11 step 58 loss 1.6604316234588623\n",
            "epoch 11 step 59 loss 1.6526602506637573\n",
            "epoch 11 step 60 loss 1.6502410173416138 test_accuracy 79.70000457763672 train_accuracy 87.3046875\n",
            "epoch 11 step 61 loss 1.6644269227981567\n",
            "epoch 11 step 62 loss 1.6554553508758545\n",
            "epoch 11 step 63 loss 1.6604825258255005\n",
            "epoch 11 step 64 loss 1.6264522075653076\n",
            "epoch 11 step 65 loss 1.6527864933013916\n",
            "epoch 11 step 66 loss 1.6486154794692993\n",
            "epoch 11 step 67 loss 1.6792657375335693\n",
            "epoch 11 step 68 loss 1.6408520936965942\n",
            "epoch 11 step 69 loss 1.671105146408081\n",
            "epoch 11 step 70 loss 1.6705753803253174\n",
            "epoch 11 step 71 loss 1.6311050653457642\n",
            "epoch 11 step 72 loss 1.6526459455490112\n",
            "epoch 11 step 73 loss 1.6661553382873535\n",
            "epoch 11 step 74 loss 1.670689582824707\n",
            "epoch 11 step 75 loss 1.6724696159362793\n",
            "epoch 11 step 76 loss 1.6538150310516357\n",
            "epoch 11 step 77 loss 1.6102471351623535\n",
            "epoch 11 step 78 loss 1.642916202545166\n",
            "epoch 11 step 79 loss 1.685250997543335\n",
            "epoch 11 step 80 loss 1.6478549242019653 test_accuracy 80.80000305175781 train_accuracy 87.3046875\n",
            "epoch 11 step 81 loss 1.6577001810073853\n",
            "epoch 11 step 82 loss 1.6412458419799805\n",
            "epoch 11 step 83 loss 1.6442101001739502\n",
            "epoch 11 step 84 loss 1.6676517724990845\n",
            "epoch 11 step 85 loss 1.6563514471054077\n",
            "epoch 11 step 86 loss 1.6718089580535889\n",
            "epoch 11 step 87 loss 1.6412773132324219\n",
            "epoch 11 step 88 loss 1.650508165359497\n",
            "epoch 11 step 89 loss 1.6385213136672974\n",
            "epoch 11 step 90 loss 1.675468921661377\n",
            "epoch 11 step 91 loss 1.6548128128051758\n",
            "epoch 11 step 92 loss 1.6739332675933838\n",
            "epoch 11 step 93 loss 1.655389428138733\n",
            "epoch 11 step 94 loss 1.655938982963562\n",
            "epoch 11 step 95 loss 1.6480742692947388\n",
            "epoch 11 step 96 loss 1.649289846420288\n",
            "epoch 11 step 97 loss 1.6171488761901855\n",
            "epoch 11 step 98 loss 1.6323204040527344\n",
            "epoch 11 step 99 loss 1.704756498336792\n",
            "epoch 11 step 100 loss 1.648899793624878 test_accuracy 80.10000610351562 train_accuracy 84.1796875\n",
            "epoch 11 step 101 loss 1.6272636651992798\n",
            "epoch 11 step 102 loss 1.645621418952942\n",
            "epoch 11 step 103 loss 1.680254340171814\n",
            "epoch 11 step 104 loss 1.6684008836746216\n",
            "epoch 11 step 105 loss 1.687913417816162\n",
            "epoch 11 step 106 loss 1.6887255907058716\n",
            "epoch 11 step 107 loss 1.663335919380188\n",
            "epoch 11 step 108 loss 1.6564370393753052\n",
            "epoch 11 step 109 loss 1.6240482330322266\n",
            "epoch 11 step 110 loss 1.6241079568862915\n",
            "epoch 11 step 111 loss 1.6649545431137085\n",
            "epoch 11 step 112 loss 1.6485713720321655\n",
            "epoch 11 step 113 loss 1.6507271528244019\n",
            "epoch 11 step 114 loss 1.6521213054656982\n",
            "epoch 11 step 115 loss 1.6710848808288574\n",
            "epoch 11 step 116 loss 1.669381856918335\n",
            "epoch 11 step 117 loss 1.614972472190857\n",
            "epoch 11 step 118 loss 1.6409451961517334\n",
            "epoch 11 step 119 loss 1.6478626728057861\n",
            "epoch 11 step 120 loss 1.662918210029602 test_accuracy 80.9000015258789 train_accuracy 86.5234375\n",
            "epoch 11 step 121 loss 1.6491672992706299\n",
            "epoch 11 step 122 loss 1.6592894792556763\n",
            "epoch 11 step 123 loss 1.7040603160858154\n",
            "epoch 11 step 124 loss 1.6371285915374756\n",
            "epoch 11 step 125 loss 1.6896599531173706\n",
            "epoch 11 step 126 loss 1.674526333808899\n",
            "epoch 11 step 127 loss 1.6723765134811401\n",
            "epoch 11 step 128 loss 1.651523470878601\n",
            "epoch 11 step 129 loss 1.6491204500198364\n",
            "epoch 11 step 130 loss 1.6608251333236694\n",
            "epoch 11 step 131 loss 1.6259114742279053\n",
            "epoch 11 step 132 loss 1.6541762351989746\n",
            "epoch 11 step 133 loss 1.6616876125335693\n",
            "epoch 11 step 134 loss 1.6487404108047485\n",
            "epoch 11 step 135 loss 1.654239296913147\n",
            "epoch 11 step 136 loss 1.6780554056167603\n",
            "epoch 11 step 137 loss 1.6610846519470215\n",
            "epoch 11 step 138 loss 1.642065167427063\n",
            "epoch 11 step 139 loss 1.6468589305877686\n",
            "epoch 11 step 140 loss 1.6379154920578003 test_accuracy 79.80000305175781 train_accuracy 84.765625\n",
            "epoch 11 step 141 loss 1.6495078802108765\n",
            "epoch 11 step 142 loss 1.648866057395935\n",
            "epoch 11 step 143 loss 1.6749598979949951\n",
            "epoch 11 step 144 loss 1.672582983970642\n",
            "epoch 11 step 145 loss 1.6739221811294556\n",
            "epoch 11 step 146 loss 1.6728273630142212\n",
            "epoch 11 step 147 loss 1.6534693241119385\n",
            "epoch 11 step 148 loss 1.6164630651474\n",
            "epoch 11 step 149 loss 1.6576173305511475\n",
            "epoch 11 step 150 loss 1.668927550315857\n",
            "epoch 11 step 151 loss 1.6670762300491333\n",
            "epoch 11 step 152 loss 1.6732360124588013\n",
            "epoch 11 step 153 loss 1.6241989135742188\n",
            "epoch 11 step 154 loss 1.6680546998977661\n",
            "epoch 11 step 155 loss 1.6492797136306763\n",
            "epoch 11 step 156 loss 1.6858432292938232\n",
            "epoch 11 step 157 loss 1.6425449848175049\n",
            "epoch 11 step 158 loss 1.6620010137557983\n",
            "epoch 11 step 159 loss 1.6790318489074707\n",
            "epoch 11 step 160 loss 1.6344393491744995 test_accuracy 81.20000457763672 train_accuracy 86.71875\n",
            "epoch 11 step 161 loss 1.6302930116653442\n",
            "epoch 11 step 162 loss 1.644110918045044\n",
            "epoch 11 step 163 loss 1.594524621963501\n",
            "epoch 11 step 164 loss 1.649763584136963\n",
            "epoch 11 step 165 loss 1.6597349643707275\n",
            "epoch 11 step 166 loss 1.633389949798584\n",
            "epoch 11 step 167 loss 1.6622109413146973\n",
            "epoch 11 step 168 loss 1.6449511051177979\n",
            "epoch 11 step 169 loss 1.6348798274993896\n",
            "epoch 11 step 170 loss 1.660398244857788\n",
            "epoch 11 step 171 loss 1.6518093347549438\n",
            "epoch 11 step 172 loss 1.6382802724838257\n",
            "epoch 11 step 173 loss 1.656229853630066\n",
            "epoch 11 step 174 loss 1.6651055812835693\n",
            "epoch 11 step 175 loss 1.6533833742141724\n",
            "epoch 11 step 176 loss 1.6289904117584229\n",
            "epoch 11 step 177 loss 1.6480311155319214\n",
            "epoch 11 step 178 loss 1.619323968887329\n",
            "epoch 11 step 179 loss 1.654688835144043\n",
            "epoch 11 step 180 loss 1.6429340839385986 test_accuracy 79.0 train_accuracy 83.984375\n",
            "epoch 11 step 181 loss 1.647108793258667\n",
            "epoch 11 step 182 loss 1.6562221050262451\n",
            "epoch 11 step 183 loss 1.6387618780136108\n",
            "epoch 11 step 184 loss 1.643202781677246\n",
            "epoch 11 step 185 loss 1.651658535003662\n",
            "epoch 11 step 186 loss 1.6709994077682495\n",
            "epoch 11 step 187 loss 1.6589388847351074\n",
            "epoch 11 step 188 loss 1.645256519317627\n",
            "epoch 11 step 189 loss 1.6369349956512451\n",
            "epoch 11 step 190 loss 1.651668906211853\n",
            "epoch 11 step 191 loss 1.6515083312988281\n",
            "epoch 11 step 192 loss 1.648605465888977\n",
            "epoch 11 step 193 loss 1.6646004915237427\n",
            "epoch 11 step 194 loss 1.6451610326766968\n",
            "epoch 11 step 195 loss 1.70450758934021\n",
            "epoch 12 step 0 loss 1.668709635734558 test_accuracy 79.10000610351562 train_accuracy 84.765625\n",
            "epoch 12 step 1 loss 1.6322312355041504\n",
            "epoch 12 step 2 loss 1.6502740383148193\n",
            "epoch 12 step 3 loss 1.6493327617645264\n",
            "epoch 12 step 4 loss 1.6588712930679321\n",
            "epoch 12 step 5 loss 1.645769476890564\n",
            "epoch 12 step 6 loss 1.6385480165481567\n",
            "epoch 12 step 7 loss 1.6386154890060425\n",
            "epoch 12 step 8 loss 1.643883466720581\n",
            "epoch 12 step 9 loss 1.6307746171951294\n",
            "epoch 12 step 10 loss 1.658623218536377\n",
            "epoch 12 step 11 loss 1.6501660346984863\n",
            "epoch 12 step 12 loss 1.675032138824463\n",
            "epoch 12 step 13 loss 1.6318165063858032\n",
            "epoch 12 step 14 loss 1.6520746946334839\n",
            "epoch 12 step 15 loss 1.6467499732971191\n",
            "epoch 12 step 16 loss 1.673958420753479\n",
            "epoch 12 step 17 loss 1.6684646606445312\n",
            "epoch 12 step 18 loss 1.6136054992675781\n",
            "epoch 12 step 19 loss 1.660130500793457\n",
            "epoch 12 step 20 loss 1.6439813375473022 test_accuracy 80.80000305175781 train_accuracy 86.9140625\n",
            "epoch 12 step 21 loss 1.6067531108856201\n",
            "epoch 12 step 22 loss 1.648457407951355\n",
            "epoch 12 step 23 loss 1.6497788429260254\n",
            "epoch 12 step 24 loss 1.6577627658843994\n",
            "epoch 12 step 25 loss 1.6536462306976318\n",
            "epoch 12 step 26 loss 1.653981328010559\n",
            "epoch 12 step 27 loss 1.6581588983535767\n",
            "epoch 12 step 28 loss 1.6641265153884888\n",
            "epoch 12 step 29 loss 1.6505802869796753\n",
            "epoch 12 step 30 loss 1.6290034055709839\n",
            "epoch 12 step 31 loss 1.6522194147109985\n",
            "epoch 12 step 32 loss 1.6368491649627686\n",
            "epoch 12 step 33 loss 1.646815299987793\n",
            "epoch 12 step 34 loss 1.6418606042861938\n",
            "epoch 12 step 35 loss 1.6405763626098633\n",
            "epoch 12 step 36 loss 1.6417368650436401\n",
            "epoch 12 step 37 loss 1.6498584747314453\n",
            "epoch 12 step 38 loss 1.6614595651626587\n",
            "epoch 12 step 39 loss 1.6521906852722168\n",
            "epoch 12 step 40 loss 1.672140121459961 test_accuracy 81.30000305175781 train_accuracy 83.984375\n",
            "epoch 12 step 41 loss 1.6486254930496216\n",
            "epoch 12 step 42 loss 1.6559994220733643\n",
            "epoch 12 step 43 loss 1.6377511024475098\n",
            "epoch 12 step 44 loss 1.6947054862976074\n",
            "epoch 12 step 45 loss 1.6567778587341309\n",
            "epoch 12 step 46 loss 1.6511024236679077\n",
            "epoch 12 step 47 loss 1.6612212657928467\n",
            "epoch 12 step 48 loss 1.6237386465072632\n",
            "epoch 12 step 49 loss 1.666157603263855\n",
            "epoch 12 step 50 loss 1.661881685256958\n",
            "epoch 12 step 51 loss 1.6679632663726807\n",
            "epoch 12 step 52 loss 1.6618505716323853\n",
            "epoch 12 step 53 loss 1.649025559425354\n",
            "epoch 12 step 54 loss 1.6540800333023071\n",
            "epoch 12 step 55 loss 1.6432474851608276\n",
            "epoch 12 step 56 loss 1.644713282585144\n",
            "epoch 12 step 57 loss 1.6627421379089355\n",
            "epoch 12 step 58 loss 1.6395761966705322\n",
            "epoch 12 step 59 loss 1.666601300239563\n",
            "epoch 12 step 60 loss 1.6201856136322021 test_accuracy 80.9000015258789 train_accuracy 85.3515625\n",
            "epoch 12 step 61 loss 1.6481016874313354\n",
            "epoch 12 step 62 loss 1.6445578336715698\n",
            "epoch 12 step 63 loss 1.6761988401412964\n",
            "epoch 12 step 64 loss 1.6578551530838013\n",
            "epoch 12 step 65 loss 1.6579400300979614\n",
            "epoch 12 step 66 loss 1.635040521621704\n",
            "epoch 12 step 67 loss 1.6531221866607666\n",
            "epoch 12 step 68 loss 1.6719657182693481\n",
            "epoch 12 step 69 loss 1.60517156124115\n",
            "epoch 12 step 70 loss 1.6193618774414062\n",
            "epoch 12 step 71 loss 1.624783992767334\n",
            "epoch 12 step 72 loss 1.6694687604904175\n",
            "epoch 12 step 73 loss 1.6493685245513916\n",
            "epoch 12 step 74 loss 1.646472454071045\n",
            "epoch 12 step 75 loss 1.6496919393539429\n",
            "epoch 12 step 76 loss 1.6492851972579956\n",
            "epoch 12 step 77 loss 1.6486937999725342\n",
            "epoch 12 step 78 loss 1.6640669107437134\n",
            "epoch 12 step 79 loss 1.621850609779358\n",
            "epoch 12 step 80 loss 1.658416986465454 test_accuracy 82.60000610351562 train_accuracy 87.109375\n",
            "epoch 12 step 81 loss 1.6211497783660889\n",
            "epoch 12 step 82 loss 1.6680322885513306\n",
            "epoch 12 step 83 loss 1.6553184986114502\n",
            "epoch 12 step 84 loss 1.6993979215621948\n",
            "epoch 12 step 85 loss 1.650462031364441\n",
            "epoch 12 step 86 loss 1.653790831565857\n",
            "epoch 12 step 87 loss 1.6507086753845215\n",
            "epoch 12 step 88 loss 1.6281051635742188\n",
            "epoch 12 step 89 loss 1.6533962488174438\n",
            "epoch 12 step 90 loss 1.6562716960906982\n",
            "epoch 12 step 91 loss 1.6311510801315308\n",
            "epoch 12 step 92 loss 1.6369743347167969\n",
            "epoch 12 step 93 loss 1.6625088453292847\n",
            "epoch 12 step 94 loss 1.6594231128692627\n",
            "epoch 12 step 95 loss 1.6531240940093994\n",
            "epoch 12 step 96 loss 1.6414704322814941\n",
            "epoch 12 step 97 loss 1.663464069366455\n",
            "epoch 12 step 98 loss 1.652428150177002\n",
            "epoch 12 step 99 loss 1.6620831489562988\n",
            "epoch 12 step 100 loss 1.6555027961730957 test_accuracy 78.9000015258789 train_accuracy 85.546875\n",
            "epoch 12 step 101 loss 1.6460188627243042\n",
            "epoch 12 step 102 loss 1.6332927942276\n",
            "epoch 12 step 103 loss 1.66110360622406\n",
            "epoch 12 step 104 loss 1.648561954498291\n",
            "epoch 12 step 105 loss 1.6186593770980835\n",
            "epoch 12 step 106 loss 1.666717529296875\n",
            "epoch 12 step 107 loss 1.6417814493179321\n",
            "epoch 12 step 108 loss 1.6322174072265625\n",
            "epoch 12 step 109 loss 1.6334491968154907\n",
            "epoch 12 step 110 loss 1.6658475399017334\n",
            "epoch 12 step 111 loss 1.6504225730895996\n",
            "epoch 12 step 112 loss 1.6445094347000122\n",
            "epoch 12 step 113 loss 1.6519720554351807\n",
            "epoch 12 step 114 loss 1.6546261310577393\n",
            "epoch 12 step 115 loss 1.6780213117599487\n",
            "epoch 12 step 116 loss 1.6561455726623535\n",
            "epoch 12 step 117 loss 1.6743834018707275\n",
            "epoch 12 step 118 loss 1.63542640209198\n",
            "epoch 12 step 119 loss 1.6450166702270508\n",
            "epoch 12 step 120 loss 1.6493229866027832 test_accuracy 81.30000305175781 train_accuracy 86.5234375\n",
            "epoch 12 step 121 loss 1.6387004852294922\n",
            "epoch 12 step 122 loss 1.6250097751617432\n",
            "epoch 12 step 123 loss 1.6733942031860352\n",
            "epoch 12 step 124 loss 1.6465229988098145\n",
            "epoch 12 step 125 loss 1.6253732442855835\n",
            "epoch 12 step 126 loss 1.637420415878296\n",
            "epoch 12 step 127 loss 1.6890432834625244\n",
            "epoch 12 step 128 loss 1.625308632850647\n",
            "epoch 12 step 129 loss 1.6488505601882935\n",
            "epoch 12 step 130 loss 1.6632136106491089\n",
            "epoch 12 step 131 loss 1.6585094928741455\n",
            "epoch 12 step 132 loss 1.6160203218460083\n",
            "epoch 12 step 133 loss 1.6614965200424194\n",
            "epoch 12 step 134 loss 1.6691367626190186\n",
            "epoch 12 step 135 loss 1.6339623928070068\n",
            "epoch 12 step 136 loss 1.6411601305007935\n",
            "epoch 12 step 137 loss 1.6810073852539062\n",
            "epoch 12 step 138 loss 1.6621525287628174\n",
            "epoch 12 step 139 loss 1.657239556312561\n",
            "epoch 12 step 140 loss 1.6175576448440552 test_accuracy 80.30000305175781 train_accuracy 82.2265625\n",
            "epoch 12 step 141 loss 1.6348823308944702\n",
            "epoch 12 step 142 loss 1.675103783607483\n",
            "epoch 12 step 143 loss 1.6416789293289185\n",
            "epoch 12 step 144 loss 1.6511250734329224\n",
            "epoch 12 step 145 loss 1.6598567962646484\n",
            "epoch 12 step 146 loss 1.6499847173690796\n",
            "epoch 12 step 147 loss 1.672741413116455\n",
            "epoch 12 step 148 loss 1.6397583484649658\n",
            "epoch 12 step 149 loss 1.6424055099487305\n",
            "epoch 12 step 150 loss 1.6699942350387573\n",
            "epoch 12 step 151 loss 1.6388903856277466\n",
            "epoch 12 step 152 loss 1.6963242292404175\n",
            "epoch 12 step 153 loss 1.6622203588485718\n",
            "epoch 12 step 154 loss 1.6614094972610474\n",
            "epoch 12 step 155 loss 1.6700127124786377\n",
            "epoch 12 step 156 loss 1.6680998802185059\n",
            "epoch 12 step 157 loss 1.6674994230270386\n",
            "epoch 12 step 158 loss 1.6823437213897705\n",
            "epoch 12 step 159 loss 1.666822910308838\n",
            "epoch 12 step 160 loss 1.6443430185317993 test_accuracy 79.10000610351562 train_accuracy 87.890625\n",
            "epoch 12 step 161 loss 1.6571002006530762\n",
            "epoch 12 step 162 loss 1.6510032415390015\n",
            "epoch 12 step 163 loss 1.6512340307235718\n",
            "epoch 12 step 164 loss 1.6443482637405396\n",
            "epoch 12 step 165 loss 1.6515473127365112\n",
            "epoch 12 step 166 loss 1.6871122121810913\n",
            "epoch 12 step 167 loss 1.6565260887145996\n",
            "epoch 12 step 168 loss 1.6388452053070068\n",
            "epoch 12 step 169 loss 1.6198393106460571\n",
            "epoch 12 step 170 loss 1.6418753862380981\n",
            "epoch 12 step 171 loss 1.656340479850769\n",
            "epoch 12 step 172 loss 1.6669158935546875\n",
            "epoch 12 step 173 loss 1.6284303665161133\n",
            "epoch 12 step 174 loss 1.6541872024536133\n",
            "epoch 12 step 175 loss 1.6215369701385498\n",
            "epoch 12 step 176 loss 1.660969614982605\n",
            "epoch 12 step 177 loss 1.6468181610107422\n",
            "epoch 12 step 178 loss 1.6411486864089966\n",
            "epoch 12 step 179 loss 1.6697776317596436\n",
            "epoch 12 step 180 loss 1.6660691499710083 test_accuracy 80.50000762939453 train_accuracy 88.28125\n",
            "epoch 12 step 181 loss 1.6544228792190552\n",
            "epoch 12 step 182 loss 1.6589908599853516\n",
            "epoch 12 step 183 loss 1.6366358995437622\n",
            "epoch 12 step 184 loss 1.6253042221069336\n",
            "epoch 12 step 185 loss 1.6602855920791626\n",
            "epoch 12 step 186 loss 1.6738545894622803\n",
            "epoch 12 step 187 loss 1.646592140197754\n",
            "epoch 12 step 188 loss 1.6487959623336792\n",
            "epoch 12 step 189 loss 1.6502832174301147\n",
            "epoch 12 step 190 loss 1.652178406715393\n",
            "epoch 12 step 191 loss 1.655563473701477\n",
            "epoch 12 step 192 loss 1.6345282793045044\n",
            "epoch 12 step 193 loss 1.6344133615493774\n",
            "epoch 12 step 194 loss 1.6306768655776978\n",
            "epoch 12 step 195 loss 1.656172752380371\n",
            "epoch 13 step 0 loss 1.6783305406570435 test_accuracy 79.80000305175781 train_accuracy 86.71875\n",
            "epoch 13 step 1 loss 1.6312952041625977\n",
            "epoch 13 step 2 loss 1.6480684280395508\n",
            "epoch 13 step 3 loss 1.6498900651931763\n",
            "epoch 13 step 4 loss 1.6565812826156616\n",
            "epoch 13 step 5 loss 1.6444501876831055\n",
            "epoch 13 step 6 loss 1.6774258613586426\n",
            "epoch 13 step 7 loss 1.6325615644454956\n",
            "epoch 13 step 8 loss 1.6790251731872559\n",
            "epoch 13 step 9 loss 1.6292299032211304\n",
            "epoch 13 step 10 loss 1.654289960861206\n",
            "epoch 13 step 11 loss 1.613734483718872\n",
            "epoch 13 step 12 loss 1.6449780464172363\n",
            "epoch 13 step 13 loss 1.6478466987609863\n",
            "epoch 13 step 14 loss 1.6348742246627808\n",
            "epoch 13 step 15 loss 1.641531229019165\n",
            "epoch 13 step 16 loss 1.6328319311141968\n",
            "epoch 13 step 17 loss 1.6350816488265991\n",
            "epoch 13 step 18 loss 1.6724152565002441\n",
            "epoch 13 step 19 loss 1.6306875944137573\n",
            "epoch 13 step 20 loss 1.6403075456619263 test_accuracy 79.30000305175781 train_accuracy 85.546875\n",
            "epoch 13 step 21 loss 1.6472532749176025\n",
            "epoch 13 step 22 loss 1.6447726488113403\n",
            "epoch 13 step 23 loss 1.6669502258300781\n",
            "epoch 13 step 24 loss 1.658523678779602\n",
            "epoch 13 step 25 loss 1.6353538036346436\n",
            "epoch 13 step 26 loss 1.6610815525054932\n",
            "epoch 13 step 27 loss 1.6548599004745483\n",
            "epoch 13 step 28 loss 1.6545875072479248\n",
            "epoch 13 step 29 loss 1.6461381912231445\n",
            "epoch 13 step 30 loss 1.6633878946304321\n",
            "epoch 13 step 31 loss 1.6706829071044922\n",
            "epoch 13 step 32 loss 1.6766208410263062\n",
            "epoch 13 step 33 loss 1.6496868133544922\n",
            "epoch 13 step 34 loss 1.6643205881118774\n",
            "epoch 13 step 35 loss 1.6826584339141846\n",
            "epoch 13 step 36 loss 1.6526408195495605\n",
            "epoch 13 step 37 loss 1.6442631483078003\n",
            "epoch 13 step 38 loss 1.6610991954803467\n",
            "epoch 13 step 39 loss 1.6309458017349243\n",
            "epoch 13 step 40 loss 1.6423789262771606 test_accuracy 80.10000610351562 train_accuracy 85.546875\n",
            "epoch 13 step 41 loss 1.645891785621643\n",
            "epoch 13 step 42 loss 1.669458270072937\n",
            "epoch 13 step 43 loss 1.6414759159088135\n",
            "epoch 13 step 44 loss 1.6303317546844482\n",
            "epoch 13 step 45 loss 1.6692320108413696\n",
            "epoch 13 step 46 loss 1.6643654108047485\n",
            "epoch 13 step 47 loss 1.6700007915496826\n",
            "epoch 13 step 48 loss 1.6494935750961304\n",
            "epoch 13 step 49 loss 1.6664209365844727\n",
            "epoch 13 step 50 loss 1.6447376012802124\n",
            "epoch 13 step 51 loss 1.638879418373108\n",
            "epoch 13 step 52 loss 1.6443054676055908\n",
            "epoch 13 step 53 loss 1.6595593690872192\n",
            "epoch 13 step 54 loss 1.6328450441360474\n",
            "epoch 13 step 55 loss 1.626024842262268\n",
            "epoch 13 step 56 loss 1.6259647607803345\n",
            "epoch 13 step 57 loss 1.6342568397521973\n",
            "epoch 13 step 58 loss 1.6494653224945068\n",
            "epoch 13 step 59 loss 1.666924238204956\n",
            "epoch 13 step 60 loss 1.6770204305648804 test_accuracy 81.60000610351562 train_accuracy 84.375\n",
            "epoch 13 step 61 loss 1.6679482460021973\n",
            "epoch 13 step 62 loss 1.6504894495010376\n",
            "epoch 13 step 63 loss 1.6434382200241089\n",
            "epoch 13 step 64 loss 1.6699509620666504\n",
            "epoch 13 step 65 loss 1.6531486511230469\n",
            "epoch 13 step 66 loss 1.6279520988464355\n",
            "epoch 13 step 67 loss 1.6385239362716675\n",
            "epoch 13 step 68 loss 1.653146743774414\n",
            "epoch 13 step 69 loss 1.602932095527649\n",
            "epoch 13 step 70 loss 1.6575199365615845\n",
            "epoch 13 step 71 loss 1.6478853225708008\n",
            "epoch 13 step 72 loss 1.6385618448257446\n",
            "epoch 13 step 73 loss 1.6653504371643066\n",
            "epoch 13 step 74 loss 1.6359431743621826\n",
            "epoch 13 step 75 loss 1.6399201154708862\n",
            "epoch 13 step 76 loss 1.6821706295013428\n",
            "epoch 13 step 77 loss 1.6657326221466064\n",
            "epoch 13 step 78 loss 1.664950966835022\n",
            "epoch 13 step 79 loss 1.6532219648361206\n",
            "epoch 13 step 80 loss 1.6423287391662598 test_accuracy 78.10000610351562 train_accuracy 86.5234375\n",
            "epoch 13 step 81 loss 1.6830366849899292\n",
            "epoch 13 step 82 loss 1.6629045009613037\n",
            "epoch 13 step 83 loss 1.6240465641021729\n",
            "epoch 13 step 84 loss 1.6548724174499512\n",
            "epoch 13 step 85 loss 1.6396845579147339\n",
            "epoch 13 step 86 loss 1.6368978023529053\n",
            "epoch 13 step 87 loss 1.6683285236358643\n",
            "epoch 13 step 88 loss 1.683525562286377\n",
            "epoch 13 step 89 loss 1.6696739196777344\n",
            "epoch 13 step 90 loss 1.660882592201233\n",
            "epoch 13 step 91 loss 1.6375700235366821\n",
            "epoch 13 step 92 loss 1.6748671531677246\n",
            "epoch 13 step 93 loss 1.6743463277816772\n",
            "epoch 13 step 94 loss 1.6544015407562256\n",
            "epoch 13 step 95 loss 1.6490280628204346\n",
            "epoch 13 step 96 loss 1.6445109844207764\n",
            "epoch 13 step 97 loss 1.6406067609786987\n",
            "epoch 13 step 98 loss 1.6457762718200684\n",
            "epoch 13 step 99 loss 1.6436340808868408\n",
            "epoch 13 step 100 loss 1.6585642099380493 test_accuracy 80.80000305175781 train_accuracy 86.5234375\n",
            "epoch 13 step 101 loss 1.6411731243133545\n",
            "epoch 13 step 102 loss 1.6444332599639893\n",
            "epoch 13 step 103 loss 1.6401915550231934\n",
            "epoch 13 step 104 loss 1.648298978805542\n",
            "epoch 13 step 105 loss 1.6555594205856323\n",
            "epoch 13 step 106 loss 1.6225730180740356\n",
            "epoch 13 step 107 loss 1.6445528268814087\n",
            "epoch 13 step 108 loss 1.6315481662750244\n",
            "epoch 13 step 109 loss 1.6724051237106323\n",
            "epoch 13 step 110 loss 1.6765271425247192\n",
            "epoch 13 step 111 loss 1.6693302392959595\n",
            "epoch 13 step 112 loss 1.6600450277328491\n",
            "epoch 13 step 113 loss 1.6262805461883545\n",
            "epoch 13 step 114 loss 1.6412091255187988\n",
            "epoch 13 step 115 loss 1.6488834619522095\n",
            "epoch 13 step 116 loss 1.661160945892334\n",
            "epoch 13 step 117 loss 1.6500111818313599\n",
            "epoch 13 step 118 loss 1.6473820209503174\n",
            "epoch 13 step 119 loss 1.6266934871673584\n",
            "epoch 13 step 120 loss 1.6568710803985596 test_accuracy 80.9000015258789 train_accuracy 89.2578125\n",
            "epoch 13 step 121 loss 1.6560893058776855\n",
            "epoch 13 step 122 loss 1.6228315830230713\n",
            "epoch 13 step 123 loss 1.665364384651184\n",
            "epoch 13 step 124 loss 1.6457653045654297\n",
            "epoch 13 step 125 loss 1.6523711681365967\n",
            "epoch 13 step 126 loss 1.6649531126022339\n",
            "epoch 13 step 127 loss 1.6750494241714478\n",
            "epoch 13 step 128 loss 1.6496597528457642\n",
            "epoch 13 step 129 loss 1.6311264038085938\n",
            "epoch 13 step 130 loss 1.6445794105529785\n",
            "epoch 13 step 131 loss 1.635575294494629\n",
            "epoch 13 step 132 loss 1.639532446861267\n",
            "epoch 13 step 133 loss 1.6631221771240234\n",
            "epoch 13 step 134 loss 1.6612179279327393\n",
            "epoch 13 step 135 loss 1.6418942213058472\n",
            "epoch 13 step 136 loss 1.644675612449646\n",
            "epoch 13 step 137 loss 1.6631147861480713\n",
            "epoch 13 step 138 loss 1.649526596069336\n",
            "epoch 13 step 139 loss 1.6736596822738647\n",
            "epoch 13 step 140 loss 1.6213819980621338 test_accuracy 81.10000610351562 train_accuracy 85.7421875\n",
            "epoch 13 step 141 loss 1.6569571495056152\n",
            "epoch 13 step 142 loss 1.6563060283660889\n",
            "epoch 13 step 143 loss 1.690062403678894\n",
            "epoch 13 step 144 loss 1.6340516805648804\n",
            "epoch 13 step 145 loss 1.6456947326660156\n",
            "epoch 13 step 146 loss 1.6348028182983398\n",
            "epoch 13 step 147 loss 1.6323522329330444\n",
            "epoch 13 step 148 loss 1.6478042602539062\n",
            "epoch 13 step 149 loss 1.6384280920028687\n",
            "epoch 13 step 150 loss 1.662693977355957\n",
            "epoch 13 step 151 loss 1.671883225440979\n",
            "epoch 13 step 152 loss 1.6704789400100708\n",
            "epoch 13 step 153 loss 1.6749142408370972\n",
            "epoch 13 step 154 loss 1.671027421951294\n",
            "epoch 13 step 155 loss 1.658826231956482\n",
            "epoch 13 step 156 loss 1.6631956100463867\n",
            "epoch 13 step 157 loss 1.6526418924331665\n",
            "epoch 13 step 158 loss 1.6344473361968994\n",
            "epoch 13 step 159 loss 1.6572233438491821\n",
            "epoch 13 step 160 loss 1.6869181394577026 test_accuracy 79.80000305175781 train_accuracy 86.5234375\n",
            "epoch 13 step 161 loss 1.649861454963684\n",
            "epoch 13 step 162 loss 1.6622865200042725\n",
            "epoch 13 step 163 loss 1.6541281938552856\n",
            "epoch 13 step 164 loss 1.6581566333770752\n",
            "epoch 13 step 165 loss 1.6536991596221924\n",
            "epoch 13 step 166 loss 1.6460347175598145\n",
            "epoch 13 step 167 loss 1.6819510459899902\n",
            "epoch 13 step 168 loss 1.6614933013916016\n",
            "epoch 13 step 169 loss 1.6211321353912354\n",
            "epoch 13 step 170 loss 1.6856780052185059\n",
            "epoch 13 step 171 loss 1.637562870979309\n",
            "epoch 13 step 172 loss 1.6456656455993652\n",
            "epoch 13 step 173 loss 1.6373246908187866\n",
            "epoch 13 step 174 loss 1.6310102939605713\n",
            "epoch 13 step 175 loss 1.6492043733596802\n",
            "epoch 13 step 176 loss 1.660746455192566\n",
            "epoch 13 step 177 loss 1.6481508016586304\n",
            "epoch 13 step 178 loss 1.6634151935577393\n",
            "epoch 13 step 179 loss 1.6260273456573486\n",
            "epoch 13 step 180 loss 1.677351713180542 test_accuracy 81.30000305175781 train_accuracy 86.5234375\n",
            "epoch 13 step 181 loss 1.6629642248153687\n",
            "epoch 13 step 182 loss 1.6459498405456543\n",
            "epoch 13 step 183 loss 1.6473934650421143\n",
            "epoch 13 step 184 loss 1.6313008069992065\n",
            "epoch 13 step 185 loss 1.6848498582839966\n",
            "epoch 13 step 186 loss 1.6722053289413452\n",
            "epoch 13 step 187 loss 1.6686599254608154\n",
            "epoch 13 step 188 loss 1.657607078552246\n",
            "epoch 13 step 189 loss 1.6634299755096436\n",
            "epoch 13 step 190 loss 1.6281685829162598\n",
            "epoch 13 step 191 loss 1.6342049837112427\n",
            "epoch 13 step 192 loss 1.6375232934951782\n",
            "epoch 13 step 193 loss 1.6457659006118774\n",
            "epoch 13 step 194 loss 1.667099118232727\n",
            "epoch 13 step 195 loss 1.6138331890106201\n",
            "epoch 14 step 0 loss 1.67385995388031 test_accuracy 83.60000610351562 train_accuracy 87.6953125\n",
            "epoch 14 step 1 loss 1.6654436588287354\n",
            "epoch 14 step 2 loss 1.6505608558654785\n",
            "epoch 14 step 3 loss 1.6495532989501953\n",
            "epoch 14 step 4 loss 1.6456259489059448\n",
            "epoch 14 step 5 loss 1.6581872701644897\n",
            "epoch 14 step 6 loss 1.651814341545105\n",
            "epoch 14 step 7 loss 1.6435115337371826\n",
            "epoch 14 step 8 loss 1.686508297920227\n",
            "epoch 14 step 9 loss 1.652718186378479\n",
            "epoch 14 step 10 loss 1.6315115690231323\n",
            "epoch 14 step 11 loss 1.634376049041748\n",
            "epoch 14 step 12 loss 1.6231544017791748\n",
            "epoch 14 step 13 loss 1.6772875785827637\n",
            "epoch 14 step 14 loss 1.6401411294937134\n",
            "epoch 14 step 15 loss 1.6439365148544312\n",
            "epoch 14 step 16 loss 1.6512161493301392\n",
            "epoch 14 step 17 loss 1.659521222114563\n",
            "epoch 14 step 18 loss 1.621175765991211\n",
            "epoch 14 step 19 loss 1.6567022800445557\n",
            "epoch 14 step 20 loss 1.631932258605957 test_accuracy 81.4000015258789 train_accuracy 84.5703125\n",
            "epoch 14 step 21 loss 1.6402876377105713\n",
            "epoch 14 step 22 loss 1.6533942222595215\n",
            "epoch 14 step 23 loss 1.6500177383422852\n",
            "epoch 14 step 24 loss 1.6574448347091675\n",
            "epoch 14 step 25 loss 1.6533342599868774\n",
            "epoch 14 step 26 loss 1.660222053527832\n",
            "epoch 14 step 27 loss 1.635398268699646\n",
            "epoch 14 step 28 loss 1.683187484741211\n",
            "epoch 14 step 29 loss 1.6212958097457886\n",
            "epoch 14 step 30 loss 1.6506654024124146\n",
            "epoch 14 step 31 loss 1.6723867654800415\n",
            "epoch 14 step 32 loss 1.6801059246063232\n",
            "epoch 14 step 33 loss 1.6650866270065308\n",
            "epoch 14 step 34 loss 1.6583032608032227\n",
            "epoch 14 step 35 loss 1.660861611366272\n",
            "epoch 14 step 36 loss 1.644047737121582\n",
            "epoch 14 step 37 loss 1.6440242528915405\n",
            "epoch 14 step 38 loss 1.6407631635665894\n",
            "epoch 14 step 39 loss 1.6144335269927979\n",
            "epoch 14 step 40 loss 1.6671361923217773 test_accuracy 79.60000610351562 train_accuracy 84.9609375\n",
            "epoch 14 step 41 loss 1.6316242218017578\n",
            "epoch 14 step 42 loss 1.6464556455612183\n",
            "epoch 14 step 43 loss 1.6681808233261108\n",
            "epoch 14 step 44 loss 1.6485539674758911\n",
            "epoch 14 step 45 loss 1.664269208908081\n",
            "epoch 14 step 46 loss 1.659062147140503\n",
            "epoch 14 step 47 loss 1.6388098001480103\n",
            "epoch 14 step 48 loss 1.662065029144287\n",
            "epoch 14 step 49 loss 1.6454579830169678\n",
            "epoch 14 step 50 loss 1.6248729228973389\n",
            "epoch 14 step 51 loss 1.6601386070251465\n",
            "epoch 14 step 52 loss 1.674240231513977\n",
            "epoch 14 step 53 loss 1.6749365329742432\n",
            "epoch 14 step 54 loss 1.6759107112884521\n",
            "epoch 14 step 55 loss 1.6548418998718262\n",
            "epoch 14 step 56 loss 1.6434742212295532\n",
            "epoch 14 step 57 loss 1.642748475074768\n",
            "epoch 14 step 58 loss 1.6663280725479126\n",
            "epoch 14 step 59 loss 1.6299957036972046\n",
            "epoch 14 step 60 loss 1.6499106884002686 test_accuracy 79.10000610351562 train_accuracy 83.203125\n",
            "epoch 14 step 61 loss 1.6469032764434814\n",
            "epoch 14 step 62 loss 1.6382752656936646\n",
            "epoch 14 step 63 loss 1.6392825841903687\n",
            "epoch 14 step 64 loss 1.6527155637741089\n",
            "epoch 14 step 65 loss 1.6610580682754517\n",
            "epoch 14 step 66 loss 1.67540442943573\n",
            "epoch 14 step 67 loss 1.654577374458313\n",
            "epoch 14 step 68 loss 1.6289979219436646\n",
            "epoch 14 step 69 loss 1.6320449113845825\n",
            "epoch 14 step 70 loss 1.6334160566329956\n",
            "epoch 14 step 71 loss 1.614286184310913\n",
            "epoch 14 step 72 loss 1.6373217105865479\n",
            "epoch 14 step 73 loss 1.6650207042694092\n",
            "epoch 14 step 74 loss 1.6630836725234985\n",
            "epoch 14 step 75 loss 1.656091332435608\n",
            "epoch 14 step 76 loss 1.6479029655456543\n",
            "epoch 14 step 77 loss 1.6313403844833374\n",
            "epoch 14 step 78 loss 1.681317925453186\n",
            "epoch 14 step 79 loss 1.643873691558838\n",
            "epoch 14 step 80 loss 1.6480070352554321 test_accuracy 80.0 train_accuracy 84.5703125\n",
            "epoch 14 step 81 loss 1.6373368501663208\n",
            "epoch 14 step 82 loss 1.6829501390457153\n",
            "epoch 14 step 83 loss 1.656325101852417\n",
            "epoch 14 step 84 loss 1.66715669631958\n",
            "epoch 14 step 85 loss 1.6190826892852783\n",
            "epoch 14 step 86 loss 1.6810507774353027\n",
            "epoch 14 step 87 loss 1.641693353652954\n",
            "epoch 14 step 88 loss 1.6716835498809814\n",
            "epoch 14 step 89 loss 1.648893117904663\n",
            "epoch 14 step 90 loss 1.6280522346496582\n",
            "epoch 14 step 91 loss 1.6443815231323242\n",
            "epoch 14 step 92 loss 1.6539803743362427\n",
            "epoch 14 step 93 loss 1.666741967201233\n",
            "epoch 14 step 94 loss 1.6415046453475952\n",
            "epoch 14 step 95 loss 1.6326698064804077\n",
            "epoch 14 step 96 loss 1.633954644203186\n",
            "epoch 14 step 97 loss 1.657286524772644\n",
            "epoch 14 step 98 loss 1.6458508968353271\n",
            "epoch 14 step 99 loss 1.660127878189087\n",
            "epoch 14 step 100 loss 1.6215722560882568 test_accuracy 81.20000457763672 train_accuracy 86.71875\n",
            "epoch 14 step 101 loss 1.6373343467712402\n",
            "epoch 14 step 102 loss 1.6589889526367188\n",
            "epoch 14 step 103 loss 1.6773027181625366\n",
            "epoch 14 step 104 loss 1.6817774772644043\n",
            "epoch 14 step 105 loss 1.6442890167236328\n",
            "epoch 14 step 106 loss 1.6738353967666626\n",
            "epoch 14 step 107 loss 1.6506478786468506\n",
            "epoch 14 step 108 loss 1.636552095413208\n",
            "epoch 14 step 109 loss 1.640895962715149\n",
            "epoch 14 step 110 loss 1.6441352367401123\n",
            "epoch 14 step 111 loss 1.627587914466858\n",
            "epoch 14 step 112 loss 1.6617095470428467\n",
            "epoch 14 step 113 loss 1.6368051767349243\n",
            "epoch 14 step 114 loss 1.6463252305984497\n",
            "epoch 14 step 115 loss 1.649696946144104\n",
            "epoch 14 step 116 loss 1.649216890335083\n",
            "epoch 14 step 117 loss 1.6252304315567017\n",
            "epoch 14 step 118 loss 1.6472558975219727\n",
            "epoch 14 step 119 loss 1.6574662923812866\n",
            "epoch 14 step 120 loss 1.641481876373291 test_accuracy 81.60000610351562 train_accuracy 85.9375\n",
            "epoch 14 step 121 loss 1.614912509918213\n",
            "epoch 14 step 122 loss 1.6863560676574707\n",
            "epoch 14 step 123 loss 1.6365747451782227\n",
            "epoch 14 step 124 loss 1.6634348630905151\n",
            "epoch 14 step 125 loss 1.6446387767791748\n",
            "epoch 14 step 126 loss 1.662678599357605\n",
            "epoch 14 step 127 loss 1.6781344413757324\n",
            "epoch 14 step 128 loss 1.657167673110962\n",
            "epoch 14 step 129 loss 1.6760268211364746\n",
            "epoch 14 step 130 loss 1.6447728872299194\n",
            "epoch 14 step 131 loss 1.6250433921813965\n",
            "epoch 14 step 132 loss 1.661742091178894\n",
            "epoch 14 step 133 loss 1.6421470642089844\n",
            "epoch 14 step 134 loss 1.66550612449646\n",
            "epoch 14 step 135 loss 1.6372323036193848\n",
            "epoch 14 step 136 loss 1.6653895378112793\n",
            "epoch 14 step 137 loss 1.670798420906067\n",
            "epoch 14 step 138 loss 1.6510515213012695\n",
            "epoch 14 step 139 loss 1.6300380229949951\n",
            "epoch 14 step 140 loss 1.6558966636657715 test_accuracy 80.9000015258789 train_accuracy 84.9609375\n",
            "epoch 14 step 141 loss 1.6471363306045532\n",
            "epoch 14 step 142 loss 1.6273146867752075\n",
            "epoch 14 step 143 loss 1.6309894323349\n",
            "epoch 14 step 144 loss 1.6423187255859375\n",
            "epoch 14 step 145 loss 1.6677404642105103\n",
            "epoch 14 step 146 loss 1.6645662784576416\n",
            "epoch 14 step 147 loss 1.666804313659668\n",
            "epoch 14 step 148 loss 1.6543467044830322\n",
            "epoch 14 step 149 loss 1.6823840141296387\n",
            "epoch 14 step 150 loss 1.6483949422836304\n",
            "epoch 14 step 151 loss 1.6620827913284302\n",
            "epoch 14 step 152 loss 1.6663751602172852\n",
            "epoch 14 step 153 loss 1.6342376470565796\n",
            "epoch 14 step 154 loss 1.6342761516571045\n",
            "epoch 14 step 155 loss 1.6481044292449951\n",
            "epoch 14 step 156 loss 1.6616545915603638\n",
            "epoch 14 step 157 loss 1.6412283182144165\n",
            "epoch 14 step 158 loss 1.6435972452163696\n",
            "epoch 14 step 159 loss 1.6380224227905273\n",
            "epoch 14 step 160 loss 1.64654541015625 test_accuracy 82.9000015258789 train_accuracy 82.2265625\n",
            "epoch 14 step 161 loss 1.6417585611343384\n",
            "epoch 14 step 162 loss 1.6385687589645386\n",
            "epoch 14 step 163 loss 1.638084888458252\n",
            "epoch 14 step 164 loss 1.6444625854492188\n",
            "epoch 14 step 165 loss 1.6583662033081055\n",
            "epoch 14 step 166 loss 1.61676824092865\n",
            "epoch 14 step 167 loss 1.6312278509140015\n",
            "epoch 14 step 168 loss 1.6431152820587158\n",
            "epoch 14 step 169 loss 1.6508631706237793\n",
            "epoch 14 step 170 loss 1.6643885374069214\n",
            "epoch 14 step 171 loss 1.634639024734497\n",
            "epoch 14 step 172 loss 1.640979528427124\n",
            "epoch 14 step 173 loss 1.6618728637695312\n",
            "epoch 14 step 174 loss 1.6689202785491943\n",
            "epoch 14 step 175 loss 1.647181749343872\n",
            "epoch 14 step 176 loss 1.6957693099975586\n",
            "epoch 14 step 177 loss 1.6560990810394287\n",
            "epoch 14 step 178 loss 1.6511121988296509\n",
            "epoch 14 step 179 loss 1.6320327520370483\n",
            "epoch 14 step 180 loss 1.6416651010513306 test_accuracy 82.50000762939453 train_accuracy 84.375\n",
            "epoch 14 step 181 loss 1.6501051187515259\n",
            "epoch 14 step 182 loss 1.6541246175765991\n",
            "epoch 14 step 183 loss 1.656369686126709\n",
            "epoch 14 step 184 loss 1.6566659212112427\n",
            "epoch 14 step 185 loss 1.6250243186950684\n",
            "epoch 14 step 186 loss 1.6348230838775635\n",
            "epoch 14 step 187 loss 1.706096887588501\n",
            "epoch 14 step 188 loss 1.6577801704406738\n",
            "epoch 14 step 189 loss 1.6424872875213623\n",
            "epoch 14 step 190 loss 1.6470615863800049\n",
            "epoch 14 step 191 loss 1.6284292936325073\n",
            "epoch 14 step 192 loss 1.6731442213058472\n",
            "epoch 14 step 193 loss 1.653695821762085\n",
            "epoch 14 step 194 loss 1.6482470035552979\n",
            "epoch 14 step 195 loss 1.679803490638733\n",
            "epoch 15 step 0 loss 1.653740406036377 test_accuracy 80.60000610351562 train_accuracy 86.5234375\n",
            "epoch 15 step 1 loss 1.6635422706604004\n",
            "epoch 15 step 2 loss 1.6547186374664307\n",
            "epoch 15 step 3 loss 1.6418280601501465\n",
            "epoch 15 step 4 loss 1.6817766427993774\n",
            "epoch 15 step 5 loss 1.6303373575210571\n",
            "epoch 15 step 6 loss 1.6644102334976196\n",
            "epoch 15 step 7 loss 1.6587015390396118\n",
            "epoch 15 step 8 loss 1.640750527381897\n",
            "epoch 15 step 9 loss 1.6477805376052856\n",
            "epoch 15 step 10 loss 1.6680574417114258\n",
            "epoch 15 step 11 loss 1.648267388343811\n",
            "epoch 15 step 12 loss 1.65901780128479\n",
            "epoch 15 step 13 loss 1.6486049890518188\n",
            "epoch 15 step 14 loss 1.6355174779891968\n",
            "epoch 15 step 15 loss 1.6660922765731812\n",
            "epoch 15 step 16 loss 1.6356682777404785\n",
            "epoch 15 step 17 loss 1.6096742153167725\n",
            "epoch 15 step 18 loss 1.641176700592041\n",
            "epoch 15 step 19 loss 1.6613174676895142\n",
            "epoch 15 step 20 loss 1.653840184211731 test_accuracy 81.30000305175781 train_accuracy 85.3515625\n",
            "epoch 15 step 21 loss 1.6379106044769287\n",
            "epoch 15 step 22 loss 1.6759803295135498\n",
            "epoch 15 step 23 loss 1.6450999975204468\n",
            "epoch 15 step 24 loss 1.639911413192749\n",
            "epoch 15 step 25 loss 1.66965651512146\n",
            "epoch 15 step 26 loss 1.6466325521469116\n",
            "epoch 15 step 27 loss 1.641130805015564\n",
            "epoch 15 step 28 loss 1.6570245027542114\n",
            "epoch 15 step 29 loss 1.648019552230835\n",
            "epoch 15 step 30 loss 1.6390525102615356\n",
            "epoch 15 step 31 loss 1.6498349905014038\n",
            "epoch 15 step 32 loss 1.650320053100586\n",
            "epoch 15 step 33 loss 1.6395478248596191\n",
            "epoch 15 step 34 loss 1.6244436502456665\n",
            "epoch 15 step 35 loss 1.6306936740875244\n",
            "epoch 15 step 36 loss 1.6442294120788574\n",
            "epoch 15 step 37 loss 1.666146993637085\n",
            "epoch 15 step 38 loss 1.6299585103988647\n",
            "epoch 15 step 39 loss 1.6311750411987305\n",
            "epoch 15 step 40 loss 1.6602247953414917 test_accuracy 80.0 train_accuracy 86.1328125\n",
            "epoch 15 step 41 loss 1.6379022598266602\n",
            "epoch 15 step 42 loss 1.6123031377792358\n",
            "epoch 15 step 43 loss 1.660036325454712\n",
            "epoch 15 step 44 loss 1.6423897743225098\n",
            "epoch 15 step 45 loss 1.682594656944275\n",
            "epoch 15 step 46 loss 1.6355297565460205\n",
            "epoch 15 step 47 loss 1.6510426998138428\n",
            "epoch 15 step 48 loss 1.658492088317871\n",
            "epoch 15 step 49 loss 1.6574482917785645\n",
            "epoch 15 step 50 loss 1.6625041961669922\n",
            "epoch 15 step 51 loss 1.6729296445846558\n",
            "epoch 15 step 52 loss 1.657557725906372\n",
            "epoch 15 step 53 loss 1.6405822038650513\n",
            "epoch 15 step 54 loss 1.6633418798446655\n",
            "epoch 15 step 55 loss 1.6270008087158203\n",
            "epoch 15 step 56 loss 1.6364799737930298\n",
            "epoch 15 step 57 loss 1.6499402523040771\n",
            "epoch 15 step 58 loss 1.66159188747406\n",
            "epoch 15 step 59 loss 1.6554858684539795\n",
            "epoch 15 step 60 loss 1.6799732446670532 test_accuracy 81.00000762939453 train_accuracy 82.8125\n",
            "epoch 15 step 61 loss 1.646721363067627\n",
            "epoch 15 step 62 loss 1.6442904472351074\n",
            "epoch 15 step 63 loss 1.6776320934295654\n",
            "epoch 15 step 64 loss 1.6592843532562256\n",
            "epoch 15 step 65 loss 1.6302398443222046\n",
            "epoch 15 step 66 loss 1.6666563749313354\n",
            "epoch 15 step 67 loss 1.621146559715271\n",
            "epoch 15 step 68 loss 1.6332502365112305\n",
            "epoch 15 step 69 loss 1.644511342048645\n",
            "epoch 15 step 70 loss 1.667625904083252\n",
            "epoch 15 step 71 loss 1.6611677408218384\n",
            "epoch 15 step 72 loss 1.6161643266677856\n",
            "epoch 15 step 73 loss 1.6788643598556519\n",
            "epoch 15 step 74 loss 1.636894702911377\n",
            "epoch 15 step 75 loss 1.6337039470672607\n",
            "epoch 15 step 76 loss 1.6268610954284668\n",
            "epoch 15 step 77 loss 1.6723370552062988\n",
            "epoch 15 step 78 loss 1.6478596925735474\n",
            "epoch 15 step 79 loss 1.6606277227401733\n",
            "epoch 15 step 80 loss 1.6872729063034058 test_accuracy 83.20000457763672 train_accuracy 89.0625\n",
            "epoch 15 step 81 loss 1.6348791122436523\n",
            "epoch 15 step 82 loss 1.6831822395324707\n",
            "epoch 15 step 83 loss 1.6517281532287598\n",
            "epoch 15 step 84 loss 1.6640429496765137\n",
            "epoch 15 step 85 loss 1.6466896533966064\n",
            "epoch 15 step 86 loss 1.6268728971481323\n",
            "epoch 15 step 87 loss 1.6385027170181274\n",
            "epoch 15 step 88 loss 1.6863698959350586\n",
            "epoch 15 step 89 loss 1.6421151161193848\n",
            "epoch 15 step 90 loss 1.6268473863601685\n",
            "epoch 15 step 91 loss 1.6581214666366577\n",
            "epoch 15 step 92 loss 1.6698561906814575\n",
            "epoch 15 step 93 loss 1.6357927322387695\n",
            "epoch 15 step 94 loss 1.6663559675216675\n",
            "epoch 15 step 95 loss 1.640124797821045\n",
            "epoch 15 step 96 loss 1.6674373149871826\n",
            "epoch 15 step 97 loss 1.6341404914855957\n",
            "epoch 15 step 98 loss 1.6410621404647827\n",
            "epoch 15 step 99 loss 1.6453368663787842\n",
            "epoch 15 step 100 loss 1.6604176759719849 test_accuracy 79.9000015258789 train_accuracy 84.765625\n",
            "epoch 15 step 101 loss 1.6689388751983643\n",
            "epoch 15 step 102 loss 1.6267900466918945\n",
            "epoch 15 step 103 loss 1.6669670343399048\n",
            "epoch 15 step 104 loss 1.6663525104522705\n",
            "epoch 15 step 105 loss 1.6694101095199585\n",
            "epoch 15 step 106 loss 1.6377190351486206\n",
            "epoch 15 step 107 loss 1.6465874910354614\n",
            "epoch 15 step 108 loss 1.618483066558838\n",
            "epoch 15 step 109 loss 1.6934354305267334\n",
            "epoch 15 step 110 loss 1.631993293762207\n",
            "epoch 15 step 111 loss 1.6231878995895386\n",
            "epoch 15 step 112 loss 1.6600596904754639\n",
            "epoch 15 step 113 loss 1.6527256965637207\n",
            "epoch 15 step 114 loss 1.6491522789001465\n",
            "epoch 15 step 115 loss 1.6238116025924683\n",
            "epoch 15 step 116 loss 1.6476651430130005\n",
            "epoch 15 step 117 loss 1.6556540727615356\n",
            "epoch 15 step 118 loss 1.6475656032562256\n",
            "epoch 15 step 119 loss 1.6545602083206177\n",
            "epoch 15 step 120 loss 1.6323984861373901 test_accuracy 80.9000015258789 train_accuracy 88.0859375\n",
            "epoch 15 step 121 loss 1.6600558757781982\n",
            "epoch 15 step 122 loss 1.6354124546051025\n",
            "epoch 15 step 123 loss 1.6569371223449707\n",
            "epoch 15 step 124 loss 1.6890645027160645\n",
            "epoch 15 step 125 loss 1.6612228155136108\n",
            "epoch 15 step 126 loss 1.6463971138000488\n",
            "epoch 15 step 127 loss 1.6299511194229126\n",
            "epoch 15 step 128 loss 1.6505028009414673\n",
            "epoch 15 step 129 loss 1.6571049690246582\n",
            "epoch 15 step 130 loss 1.6367064714431763\n",
            "epoch 15 step 131 loss 1.6330193281173706\n",
            "epoch 15 step 132 loss 1.632251262664795\n",
            "epoch 15 step 133 loss 1.6467176675796509\n",
            "epoch 15 step 134 loss 1.6483163833618164\n",
            "epoch 15 step 135 loss 1.6434611082077026\n",
            "epoch 15 step 136 loss 1.6428645849227905\n",
            "epoch 15 step 137 loss 1.684288740158081\n",
            "epoch 15 step 138 loss 1.6066908836364746\n",
            "epoch 15 step 139 loss 1.6606276035308838\n",
            "epoch 15 step 140 loss 1.618192195892334 test_accuracy 80.20000457763672 train_accuracy 86.9140625\n",
            "epoch 15 step 141 loss 1.660982370376587\n",
            "epoch 15 step 142 loss 1.6798678636550903\n",
            "epoch 15 step 143 loss 1.6706914901733398\n",
            "epoch 15 step 144 loss 1.6656795740127563\n",
            "epoch 15 step 145 loss 1.6503206491470337\n",
            "epoch 15 step 146 loss 1.6268779039382935\n",
            "epoch 15 step 147 loss 1.631658911705017\n",
            "epoch 15 step 148 loss 1.6462658643722534\n",
            "epoch 15 step 149 loss 1.6550474166870117\n",
            "epoch 15 step 150 loss 1.6438764333724976\n",
            "epoch 15 step 151 loss 1.6500320434570312\n",
            "epoch 15 step 152 loss 1.610173225402832\n",
            "epoch 15 step 153 loss 1.6752196550369263\n",
            "epoch 15 step 154 loss 1.6650387048721313\n",
            "epoch 15 step 155 loss 1.6359018087387085\n",
            "epoch 15 step 156 loss 1.638134241104126\n",
            "epoch 15 step 157 loss 1.6391910314559937\n",
            "epoch 15 step 158 loss 1.6328190565109253\n",
            "epoch 15 step 159 loss 1.6650950908660889\n",
            "epoch 15 step 160 loss 1.6449249982833862 test_accuracy 81.60000610351562 train_accuracy 84.1796875\n",
            "epoch 15 step 161 loss 1.64842689037323\n",
            "epoch 15 step 162 loss 1.6515246629714966\n",
            "epoch 15 step 163 loss 1.6477484703063965\n",
            "epoch 15 step 164 loss 1.6515400409698486\n",
            "epoch 15 step 165 loss 1.6480942964553833\n",
            "epoch 15 step 166 loss 1.6728615760803223\n",
            "epoch 15 step 167 loss 1.6613404750823975\n",
            "epoch 15 step 168 loss 1.654615879058838\n",
            "epoch 15 step 169 loss 1.6538336277008057\n",
            "epoch 15 step 170 loss 1.649587631225586\n",
            "epoch 15 step 171 loss 1.6596360206604004\n",
            "epoch 15 step 172 loss 1.6541697978973389\n",
            "epoch 15 step 173 loss 1.6279008388519287\n",
            "epoch 15 step 174 loss 1.6553008556365967\n",
            "epoch 15 step 175 loss 1.6586785316467285\n",
            "epoch 15 step 176 loss 1.641263723373413\n",
            "epoch 15 step 177 loss 1.6521226167678833\n",
            "epoch 15 step 178 loss 1.6724107265472412\n",
            "epoch 15 step 179 loss 1.6417040824890137\n",
            "epoch 15 step 180 loss 1.641653299331665 test_accuracy 81.10000610351562 train_accuracy 86.1328125\n",
            "epoch 15 step 181 loss 1.6438674926757812\n",
            "epoch 15 step 182 loss 1.6518052816390991\n",
            "epoch 15 step 183 loss 1.6373049020767212\n",
            "epoch 15 step 184 loss 1.6628458499908447\n",
            "epoch 15 step 185 loss 1.6571829319000244\n",
            "epoch 15 step 186 loss 1.6580114364624023\n",
            "epoch 15 step 187 loss 1.6760836839675903\n",
            "epoch 15 step 188 loss 1.6516594886779785\n",
            "epoch 15 step 189 loss 1.6345934867858887\n",
            "epoch 15 step 190 loss 1.657081961631775\n",
            "epoch 15 step 191 loss 1.6571903228759766\n",
            "epoch 15 step 192 loss 1.6585639715194702\n",
            "epoch 15 step 193 loss 1.6674540042877197\n",
            "epoch 15 step 194 loss 1.6291518211364746\n",
            "epoch 15 step 195 loss 1.6621240377426147\n",
            "epoch 16 step 0 loss 1.6634976863861084 test_accuracy 81.9000015258789 train_accuracy 84.765625\n",
            "epoch 16 step 1 loss 1.655781626701355\n",
            "epoch 16 step 2 loss 1.6451321840286255\n",
            "epoch 16 step 3 loss 1.6274470090866089\n",
            "epoch 16 step 4 loss 1.6387665271759033\n",
            "epoch 16 step 5 loss 1.6642134189605713\n",
            "epoch 16 step 6 loss 1.668441891670227\n",
            "epoch 16 step 7 loss 1.649753451347351\n",
            "epoch 16 step 8 loss 1.6657130718231201\n",
            "epoch 16 step 9 loss 1.6489523649215698\n",
            "epoch 16 step 10 loss 1.657349705696106\n",
            "epoch 16 step 11 loss 1.6240451335906982\n",
            "epoch 16 step 12 loss 1.6560442447662354\n",
            "epoch 16 step 13 loss 1.6462256908416748\n",
            "epoch 16 step 14 loss 1.6301313638687134\n",
            "epoch 16 step 15 loss 1.6700984239578247\n",
            "epoch 16 step 16 loss 1.6515331268310547\n",
            "epoch 16 step 17 loss 1.6588530540466309\n",
            "epoch 16 step 18 loss 1.6447051763534546\n",
            "epoch 16 step 19 loss 1.6345670223236084\n",
            "epoch 16 step 20 loss 1.6564927101135254 test_accuracy 79.4000015258789 train_accuracy 87.5\n",
            "epoch 16 step 21 loss 1.6711442470550537\n",
            "epoch 16 step 22 loss 1.664026141166687\n",
            "epoch 16 step 23 loss 1.6749237775802612\n",
            "epoch 16 step 24 loss 1.6386557817459106\n",
            "epoch 16 step 25 loss 1.682932734489441\n",
            "epoch 16 step 26 loss 1.662792682647705\n",
            "epoch 16 step 27 loss 1.6486769914627075\n",
            "epoch 16 step 28 loss 1.6167560815811157\n",
            "epoch 16 step 29 loss 1.664447546005249\n",
            "epoch 16 step 30 loss 1.6621489524841309\n",
            "epoch 16 step 31 loss 1.624184012413025\n",
            "epoch 16 step 32 loss 1.6708507537841797\n",
            "epoch 16 step 33 loss 1.6659389734268188\n",
            "epoch 16 step 34 loss 1.6458879709243774\n",
            "epoch 16 step 35 loss 1.6387830972671509\n",
            "epoch 16 step 36 loss 1.6453434228897095\n",
            "epoch 16 step 37 loss 1.640480637550354\n",
            "epoch 16 step 38 loss 1.629227638244629\n",
            "epoch 16 step 39 loss 1.6249313354492188\n",
            "epoch 16 step 40 loss 1.6402307748794556 test_accuracy 81.10000610351562 train_accuracy 84.765625\n",
            "epoch 16 step 41 loss 1.6558526754379272\n",
            "epoch 16 step 42 loss 1.6470468044281006\n",
            "epoch 16 step 43 loss 1.6219866275787354\n",
            "epoch 16 step 44 loss 1.6657345294952393\n",
            "epoch 16 step 45 loss 1.6400800943374634\n",
            "epoch 16 step 46 loss 1.6342408657073975\n",
            "epoch 16 step 47 loss 1.6450772285461426\n",
            "epoch 16 step 48 loss 1.6344139575958252\n",
            "epoch 16 step 49 loss 1.6410682201385498\n",
            "epoch 16 step 50 loss 1.6234354972839355\n",
            "epoch 16 step 51 loss 1.640078067779541\n",
            "epoch 16 step 52 loss 1.63688063621521\n",
            "epoch 16 step 53 loss 1.6457171440124512\n",
            "epoch 16 step 54 loss 1.6249744892120361\n",
            "epoch 16 step 55 loss 1.6637367010116577\n",
            "epoch 16 step 56 loss 1.6565982103347778\n",
            "epoch 16 step 57 loss 1.6720842123031616\n",
            "epoch 16 step 58 loss 1.644791603088379\n",
            "epoch 16 step 59 loss 1.6433911323547363\n",
            "epoch 16 step 60 loss 1.6292585134506226 test_accuracy 79.4000015258789 train_accuracy 85.9375\n",
            "epoch 16 step 61 loss 1.6535344123840332\n",
            "epoch 16 step 62 loss 1.6182838678359985\n",
            "epoch 16 step 63 loss 1.6613904237747192\n",
            "epoch 16 step 64 loss 1.6460868120193481\n",
            "epoch 16 step 65 loss 1.6266542673110962\n",
            "epoch 16 step 66 loss 1.6622636318206787\n",
            "epoch 16 step 67 loss 1.6674548387527466\n",
            "epoch 16 step 68 loss 1.640886664390564\n",
            "epoch 16 step 69 loss 1.6474372148513794\n",
            "epoch 16 step 70 loss 1.6595383882522583\n",
            "epoch 16 step 71 loss 1.641114592552185\n",
            "epoch 16 step 72 loss 1.6033961772918701\n",
            "epoch 16 step 73 loss 1.6494816541671753\n",
            "epoch 16 step 74 loss 1.6359021663665771\n",
            "epoch 16 step 75 loss 1.6678794622421265\n",
            "epoch 16 step 76 loss 1.63057279586792\n",
            "epoch 16 step 77 loss 1.6711524724960327\n",
            "epoch 16 step 78 loss 1.6436786651611328\n",
            "epoch 16 step 79 loss 1.6498208045959473\n",
            "epoch 16 step 80 loss 1.6540173292160034 test_accuracy 81.20000457763672 train_accuracy 87.109375\n",
            "epoch 16 step 81 loss 1.6691217422485352\n",
            "epoch 16 step 82 loss 1.6146095991134644\n",
            "epoch 16 step 83 loss 1.644540548324585\n",
            "epoch 16 step 84 loss 1.6449650526046753\n",
            "epoch 16 step 85 loss 1.653408169746399\n",
            "epoch 16 step 86 loss 1.6335407495498657\n",
            "epoch 16 step 87 loss 1.6527681350708008\n",
            "epoch 16 step 88 loss 1.6469311714172363\n",
            "epoch 16 step 89 loss 1.649991750717163\n",
            "epoch 16 step 90 loss 1.652223825454712\n",
            "epoch 16 step 91 loss 1.664470911026001\n",
            "epoch 16 step 92 loss 1.6337382793426514\n",
            "epoch 16 step 93 loss 1.6660035848617554\n",
            "epoch 16 step 94 loss 1.6570624113082886\n",
            "epoch 16 step 95 loss 1.651024580001831\n",
            "epoch 16 step 96 loss 1.6792761087417603\n",
            "epoch 16 step 97 loss 1.6576405763626099\n",
            "epoch 16 step 98 loss 1.6591925621032715\n",
            "epoch 16 step 99 loss 1.656536340713501\n",
            "epoch 16 step 100 loss 1.6570614576339722 test_accuracy 81.60000610351562 train_accuracy 87.890625\n",
            "epoch 16 step 101 loss 1.6685571670532227\n",
            "epoch 16 step 102 loss 1.638519287109375\n",
            "epoch 16 step 103 loss 1.6844711303710938\n",
            "epoch 16 step 104 loss 1.6558773517608643\n",
            "epoch 16 step 105 loss 1.6696842908859253\n",
            "epoch 16 step 106 loss 1.640153169631958\n",
            "epoch 16 step 107 loss 1.662681221961975\n",
            "epoch 16 step 108 loss 1.6653661727905273\n",
            "epoch 16 step 109 loss 1.638654351234436\n",
            "epoch 16 step 110 loss 1.6287274360656738\n",
            "epoch 16 step 111 loss 1.639884352684021\n",
            "epoch 16 step 112 loss 1.6411594152450562\n",
            "epoch 16 step 113 loss 1.6474298238754272\n",
            "epoch 16 step 114 loss 1.6326844692230225\n",
            "epoch 16 step 115 loss 1.6597704887390137\n",
            "epoch 16 step 116 loss 1.6345866918563843\n",
            "epoch 16 step 117 loss 1.6308543682098389\n",
            "epoch 16 step 118 loss 1.6637279987335205\n",
            "epoch 16 step 119 loss 1.6505873203277588\n",
            "epoch 16 step 120 loss 1.6336932182312012 test_accuracy 78.5 train_accuracy 86.5234375\n",
            "epoch 16 step 121 loss 1.6610016822814941\n",
            "epoch 16 step 122 loss 1.6371703147888184\n",
            "epoch 16 step 123 loss 1.6293156147003174\n",
            "epoch 16 step 124 loss 1.6311112642288208\n",
            "epoch 16 step 125 loss 1.6477183103561401\n",
            "epoch 16 step 126 loss 1.6643542051315308\n",
            "epoch 16 step 127 loss 1.6385111808776855\n",
            "epoch 16 step 128 loss 1.6323999166488647\n",
            "epoch 16 step 129 loss 1.6676268577575684\n",
            "epoch 16 step 130 loss 1.6488524675369263\n",
            "epoch 16 step 131 loss 1.6484086513519287\n",
            "epoch 16 step 132 loss 1.6279792785644531\n",
            "epoch 16 step 133 loss 1.634490728378296\n",
            "epoch 16 step 134 loss 1.637398362159729\n",
            "epoch 16 step 135 loss 1.6532303094863892\n",
            "epoch 16 step 136 loss 1.6339092254638672\n",
            "epoch 16 step 137 loss 1.6343382596969604\n",
            "epoch 16 step 138 loss 1.6379880905151367\n",
            "epoch 16 step 139 loss 1.6298187971115112\n",
            "epoch 16 step 140 loss 1.644096851348877 test_accuracy 78.70000457763672 train_accuracy 87.3046875\n",
            "epoch 16 step 141 loss 1.6288256645202637\n",
            "epoch 16 step 142 loss 1.6520042419433594\n",
            "epoch 16 step 143 loss 1.6564639806747437\n",
            "epoch 16 step 144 loss 1.659744143486023\n",
            "epoch 16 step 145 loss 1.6299575567245483\n",
            "epoch 16 step 146 loss 1.643285870552063\n",
            "epoch 16 step 147 loss 1.6418033838272095\n",
            "epoch 16 step 148 loss 1.6534701585769653\n",
            "epoch 16 step 149 loss 1.6567507982254028\n",
            "epoch 16 step 150 loss 1.655080795288086\n",
            "epoch 16 step 151 loss 1.6262048482894897\n",
            "epoch 16 step 152 loss 1.629309892654419\n",
            "epoch 16 step 153 loss 1.6428720951080322\n",
            "epoch 16 step 154 loss 1.6742658615112305\n",
            "epoch 16 step 155 loss 1.637239694595337\n",
            "epoch 16 step 156 loss 1.6218328475952148\n",
            "epoch 16 step 157 loss 1.6511448621749878\n",
            "epoch 16 step 158 loss 1.6439869403839111\n",
            "epoch 16 step 159 loss 1.6472043991088867\n",
            "epoch 16 step 160 loss 1.6548867225646973 test_accuracy 81.60000610351562 train_accuracy 87.6953125\n",
            "epoch 16 step 161 loss 1.657083511352539\n",
            "epoch 16 step 162 loss 1.6688761711120605\n",
            "epoch 16 step 163 loss 1.6441665887832642\n",
            "epoch 16 step 164 loss 1.639350414276123\n",
            "epoch 16 step 165 loss 1.6185375452041626\n",
            "epoch 16 step 166 loss 1.6662431955337524\n",
            "epoch 16 step 167 loss 1.6539678573608398\n",
            "epoch 16 step 168 loss 1.6558928489685059\n",
            "epoch 16 step 169 loss 1.6273828744888306\n",
            "epoch 16 step 170 loss 1.636420726776123\n",
            "epoch 16 step 171 loss 1.6152583360671997\n",
            "epoch 16 step 172 loss 1.6510794162750244\n",
            "epoch 16 step 173 loss 1.614611268043518\n",
            "epoch 16 step 174 loss 1.6622724533081055\n",
            "epoch 16 step 175 loss 1.669477939605713\n",
            "epoch 16 step 176 loss 1.6722687482833862\n",
            "epoch 16 step 177 loss 1.6697120666503906\n",
            "epoch 16 step 178 loss 1.6420282125473022\n",
            "epoch 16 step 179 loss 1.6257342100143433\n",
            "epoch 16 step 180 loss 1.6727790832519531 test_accuracy 82.70000457763672 train_accuracy 88.671875\n",
            "epoch 16 step 181 loss 1.6545767784118652\n",
            "epoch 16 step 182 loss 1.6613832712173462\n",
            "epoch 16 step 183 loss 1.6262058019638062\n",
            "epoch 16 step 184 loss 1.6518200635910034\n",
            "epoch 16 step 185 loss 1.6387611627578735\n",
            "epoch 16 step 186 loss 1.6336878538131714\n",
            "epoch 16 step 187 loss 1.6598231792449951\n",
            "epoch 16 step 188 loss 1.6635916233062744\n",
            "epoch 16 step 189 loss 1.638791799545288\n",
            "epoch 16 step 190 loss 1.6701569557189941\n",
            "epoch 16 step 191 loss 1.6789908409118652\n",
            "epoch 16 step 192 loss 1.65420663356781\n",
            "epoch 16 step 193 loss 1.667763113975525\n",
            "epoch 16 step 194 loss 1.628692388534546\n",
            "epoch 16 step 195 loss 1.6466712951660156\n",
            "epoch 17 step 0 loss 1.6536763906478882 test_accuracy 81.50000762939453 train_accuracy 83.3984375\n",
            "epoch 17 step 1 loss 1.6481561660766602\n",
            "epoch 17 step 2 loss 1.646457552909851\n",
            "epoch 17 step 3 loss 1.6847506761550903\n",
            "epoch 17 step 4 loss 1.6495802402496338\n",
            "epoch 17 step 5 loss 1.6751550436019897\n",
            "epoch 17 step 6 loss 1.6608154773712158\n",
            "epoch 17 step 7 loss 1.6096526384353638\n",
            "epoch 17 step 8 loss 1.6684725284576416\n",
            "epoch 17 step 9 loss 1.658255934715271\n",
            "epoch 17 step 10 loss 1.6409865617752075\n",
            "epoch 17 step 11 loss 1.658110499382019\n",
            "epoch 17 step 12 loss 1.6566914319992065\n",
            "epoch 17 step 13 loss 1.6733044385910034\n",
            "epoch 17 step 14 loss 1.6353652477264404\n",
            "epoch 17 step 15 loss 1.6438907384872437\n",
            "epoch 17 step 16 loss 1.6452478170394897\n",
            "epoch 17 step 17 loss 1.6503710746765137\n",
            "epoch 17 step 18 loss 1.6422861814498901\n",
            "epoch 17 step 19 loss 1.6673715114593506\n",
            "epoch 17 step 20 loss 1.632685661315918 test_accuracy 80.30000305175781 train_accuracy 84.1796875\n",
            "epoch 17 step 21 loss 1.6554440259933472\n",
            "epoch 17 step 22 loss 1.651528239250183\n",
            "epoch 17 step 23 loss 1.634277105331421\n",
            "epoch 17 step 24 loss 1.6687332391738892\n",
            "epoch 17 step 25 loss 1.6358014345169067\n",
            "epoch 17 step 26 loss 1.6423875093460083\n",
            "epoch 17 step 27 loss 1.633608102798462\n",
            "epoch 17 step 28 loss 1.6384069919586182\n",
            "epoch 17 step 29 loss 1.659813642501831\n",
            "epoch 17 step 30 loss 1.6311664581298828\n",
            "epoch 17 step 31 loss 1.6510308980941772\n",
            "epoch 17 step 32 loss 1.6376434564590454\n",
            "epoch 17 step 33 loss 1.6399856805801392\n",
            "epoch 17 step 34 loss 1.6515071392059326\n",
            "epoch 17 step 35 loss 1.6539793014526367\n",
            "epoch 17 step 36 loss 1.6425591707229614\n",
            "epoch 17 step 37 loss 1.6381845474243164\n",
            "epoch 17 step 38 loss 1.6376688480377197\n",
            "epoch 17 step 39 loss 1.6267644166946411\n",
            "epoch 17 step 40 loss 1.655409812927246 test_accuracy 80.0 train_accuracy 87.3046875\n",
            "epoch 17 step 41 loss 1.6528663635253906\n",
            "epoch 17 step 42 loss 1.6367915868759155\n",
            "epoch 17 step 43 loss 1.6293927431106567\n",
            "epoch 17 step 44 loss 1.6510192155838013\n",
            "epoch 17 step 45 loss 1.630513310432434\n",
            "epoch 17 step 46 loss 1.6631923913955688\n",
            "epoch 17 step 47 loss 1.6424732208251953\n",
            "epoch 17 step 48 loss 1.6411125659942627\n",
            "epoch 17 step 49 loss 1.6518293619155884\n",
            "epoch 17 step 50 loss 1.6252151727676392\n",
            "epoch 17 step 51 loss 1.6272560358047485\n",
            "epoch 17 step 52 loss 1.6739500761032104\n",
            "epoch 17 step 53 loss 1.6641496419906616\n",
            "epoch 17 step 54 loss 1.6368403434753418\n",
            "epoch 17 step 55 loss 1.6491949558258057\n",
            "epoch 17 step 56 loss 1.643404483795166\n",
            "epoch 17 step 57 loss 1.6435697078704834\n",
            "epoch 17 step 58 loss 1.6345516443252563\n",
            "epoch 17 step 59 loss 1.6354397535324097\n",
            "epoch 17 step 60 loss 1.6657781600952148 test_accuracy 80.60000610351562 train_accuracy 87.6953125\n",
            "epoch 17 step 61 loss 1.652361273765564\n",
            "epoch 17 step 62 loss 1.6686396598815918\n",
            "epoch 17 step 63 loss 1.6723902225494385\n",
            "epoch 17 step 64 loss 1.658820629119873\n",
            "epoch 17 step 65 loss 1.684705138206482\n",
            "epoch 17 step 66 loss 1.6550599336624146\n",
            "epoch 17 step 67 loss 1.6535050868988037\n",
            "epoch 17 step 68 loss 1.6463648080825806\n",
            "epoch 17 step 69 loss 1.6723837852478027\n",
            "epoch 17 step 70 loss 1.6513700485229492\n",
            "epoch 17 step 71 loss 1.6345977783203125\n",
            "epoch 17 step 72 loss 1.6571829319000244\n",
            "epoch 17 step 73 loss 1.6258560419082642\n",
            "epoch 17 step 74 loss 1.627060890197754\n",
            "epoch 17 step 75 loss 1.621275782585144\n",
            "epoch 17 step 76 loss 1.65684175491333\n",
            "epoch 17 step 77 loss 1.631649374961853\n",
            "epoch 17 step 78 loss 1.623216152191162\n",
            "epoch 17 step 79 loss 1.630575180053711\n",
            "epoch 17 step 80 loss 1.6379982233047485 test_accuracy 79.30000305175781 train_accuracy 86.5234375\n",
            "epoch 17 step 81 loss 1.6369203329086304\n",
            "epoch 17 step 82 loss 1.6560932397842407\n",
            "epoch 17 step 83 loss 1.6240057945251465\n",
            "epoch 17 step 84 loss 1.6510581970214844\n",
            "epoch 17 step 85 loss 1.662068247795105\n",
            "epoch 17 step 86 loss 1.6357789039611816\n",
            "epoch 17 step 87 loss 1.6550596952438354\n",
            "epoch 17 step 88 loss 1.66104257106781\n",
            "epoch 17 step 89 loss 1.6610610485076904\n",
            "epoch 17 step 90 loss 1.6381644010543823\n",
            "epoch 17 step 91 loss 1.6574623584747314\n",
            "epoch 17 step 92 loss 1.660070538520813\n",
            "epoch 17 step 93 loss 1.6656346321105957\n",
            "epoch 17 step 94 loss 1.6529325246810913\n",
            "epoch 17 step 95 loss 1.6550759077072144\n",
            "epoch 17 step 96 loss 1.6495410203933716\n",
            "epoch 17 step 97 loss 1.6606642007827759\n",
            "epoch 17 step 98 loss 1.6268119812011719\n",
            "epoch 17 step 99 loss 1.6602870225906372\n",
            "epoch 17 step 100 loss 1.6555290222167969 test_accuracy 83.60000610351562 train_accuracy 86.328125\n",
            "epoch 17 step 101 loss 1.65547513961792\n",
            "epoch 17 step 102 loss 1.6456081867218018\n",
            "epoch 17 step 103 loss 1.656290054321289\n",
            "epoch 17 step 104 loss 1.6500067710876465\n",
            "epoch 17 step 105 loss 1.6520411968231201\n",
            "epoch 17 step 106 loss 1.6279281377792358\n",
            "epoch 17 step 107 loss 1.6055456399917603\n",
            "epoch 17 step 108 loss 1.6624153852462769\n",
            "epoch 17 step 109 loss 1.664794683456421\n",
            "epoch 17 step 110 loss 1.6477528810501099\n",
            "epoch 17 step 111 loss 1.6567085981369019\n",
            "epoch 17 step 112 loss 1.6610195636749268\n",
            "epoch 17 step 113 loss 1.6362239122390747\n",
            "epoch 17 step 114 loss 1.6408772468566895\n",
            "epoch 17 step 115 loss 1.6159385442733765\n",
            "epoch 17 step 116 loss 1.6567527055740356\n",
            "epoch 17 step 117 loss 1.6546664237976074\n",
            "epoch 17 step 118 loss 1.6334755420684814\n",
            "epoch 17 step 119 loss 1.658030390739441\n",
            "epoch 17 step 120 loss 1.638122320175171 test_accuracy 81.50000762939453 train_accuracy 85.7421875\n",
            "epoch 17 step 121 loss 1.6807478666305542\n",
            "epoch 17 step 122 loss 1.6285216808319092\n",
            "epoch 17 step 123 loss 1.6279553174972534\n",
            "epoch 17 step 124 loss 1.6337374448776245\n",
            "epoch 17 step 125 loss 1.6754097938537598\n",
            "epoch 17 step 126 loss 1.6307085752487183\n",
            "epoch 17 step 127 loss 1.650890827178955\n",
            "epoch 17 step 128 loss 1.6580520868301392\n",
            "epoch 17 step 129 loss 1.6771892309188843\n",
            "epoch 17 step 130 loss 1.6385374069213867\n",
            "epoch 17 step 131 loss 1.6533068418502808\n",
            "epoch 17 step 132 loss 1.6736516952514648\n",
            "epoch 17 step 133 loss 1.6339114904403687\n",
            "epoch 17 step 134 loss 1.644409418106079\n",
            "epoch 17 step 135 loss 1.6354048252105713\n",
            "epoch 17 step 136 loss 1.619811773300171\n",
            "epoch 17 step 137 loss 1.6233106851577759\n",
            "epoch 17 step 138 loss 1.6497783660888672\n",
            "epoch 17 step 139 loss 1.6488456726074219\n",
            "epoch 17 step 140 loss 1.6242753267288208 test_accuracy 83.50000762939453 train_accuracy 88.28125\n",
            "epoch 17 step 141 loss 1.6690468788146973\n",
            "epoch 17 step 142 loss 1.644790768623352\n",
            "epoch 17 step 143 loss 1.6874041557312012\n",
            "epoch 17 step 144 loss 1.6448287963867188\n",
            "epoch 17 step 145 loss 1.6548597812652588\n",
            "epoch 17 step 146 loss 1.6467400789260864\n",
            "epoch 17 step 147 loss 1.612851858139038\n",
            "epoch 17 step 148 loss 1.6653761863708496\n",
            "epoch 17 step 149 loss 1.6164597272872925\n",
            "epoch 17 step 150 loss 1.6440179347991943\n",
            "epoch 17 step 151 loss 1.6469988822937012\n",
            "epoch 17 step 152 loss 1.651478886604309\n",
            "epoch 17 step 153 loss 1.6274371147155762\n",
            "epoch 17 step 154 loss 1.6632827520370483\n",
            "epoch 17 step 155 loss 1.6505240201950073\n",
            "epoch 17 step 156 loss 1.6145870685577393\n",
            "epoch 17 step 157 loss 1.646623969078064\n",
            "epoch 17 step 158 loss 1.6885567903518677\n",
            "epoch 17 step 159 loss 1.6539266109466553\n",
            "epoch 17 step 160 loss 1.6479060649871826 test_accuracy 79.0 train_accuracy 88.28125\n",
            "epoch 17 step 161 loss 1.6893210411071777\n",
            "epoch 17 step 162 loss 1.6539599895477295\n",
            "epoch 17 step 163 loss 1.672208309173584\n",
            "epoch 17 step 164 loss 1.6271519660949707\n",
            "epoch 17 step 165 loss 1.651016116142273\n",
            "epoch 17 step 166 loss 1.651538610458374\n",
            "epoch 17 step 167 loss 1.625879168510437\n",
            "epoch 17 step 168 loss 1.6464751958847046\n",
            "epoch 17 step 169 loss 1.6809346675872803\n",
            "epoch 17 step 170 loss 1.6411417722702026\n",
            "epoch 17 step 171 loss 1.666914463043213\n",
            "epoch 17 step 172 loss 1.657657265663147\n",
            "epoch 17 step 173 loss 1.6129215955734253\n",
            "epoch 17 step 174 loss 1.6666845083236694\n",
            "epoch 17 step 175 loss 1.6357429027557373\n",
            "epoch 17 step 176 loss 1.6474400758743286\n",
            "epoch 17 step 177 loss 1.6298915147781372\n",
            "epoch 17 step 178 loss 1.6432042121887207\n",
            "epoch 17 step 179 loss 1.6140451431274414\n",
            "epoch 17 step 180 loss 1.6444820165634155 test_accuracy 81.10000610351562 train_accuracy 85.3515625\n",
            "epoch 17 step 181 loss 1.662579894065857\n",
            "epoch 17 step 182 loss 1.6793756484985352\n",
            "epoch 17 step 183 loss 1.6321141719818115\n",
            "epoch 17 step 184 loss 1.6399331092834473\n",
            "epoch 17 step 185 loss 1.651795744895935\n",
            "epoch 17 step 186 loss 1.6697943210601807\n",
            "epoch 17 step 187 loss 1.6484860181808472\n",
            "epoch 17 step 188 loss 1.6416972875595093\n",
            "epoch 17 step 189 loss 1.6697123050689697\n",
            "epoch 17 step 190 loss 1.6741031408309937\n",
            "epoch 17 step 191 loss 1.651029109954834\n",
            "epoch 17 step 192 loss 1.6253414154052734\n",
            "epoch 17 step 193 loss 1.6539533138275146\n",
            "epoch 17 step 194 loss 1.6191812753677368\n",
            "epoch 17 step 195 loss 1.663320779800415\n",
            "epoch 18 step 0 loss 1.660556435585022 test_accuracy 80.70000457763672 train_accuracy 83.3984375\n",
            "epoch 18 step 1 loss 1.6411411762237549\n",
            "epoch 18 step 2 loss 1.6335495710372925\n",
            "epoch 18 step 3 loss 1.6430158615112305\n",
            "epoch 18 step 4 loss 1.6537120342254639\n",
            "epoch 18 step 5 loss 1.6550437211990356\n",
            "epoch 18 step 6 loss 1.63222074508667\n",
            "epoch 18 step 7 loss 1.6376222372055054\n",
            "epoch 18 step 8 loss 1.651894450187683\n",
            "epoch 18 step 9 loss 1.667344093322754\n",
            "epoch 18 step 10 loss 1.645429253578186\n",
            "epoch 18 step 11 loss 1.6597397327423096\n",
            "epoch 18 step 12 loss 1.6128861904144287\n",
            "epoch 18 step 13 loss 1.614998698234558\n",
            "epoch 18 step 14 loss 1.628994107246399\n",
            "epoch 18 step 15 loss 1.656720757484436\n",
            "epoch 18 step 16 loss 1.6696884632110596\n",
            "epoch 18 step 17 loss 1.649100422859192\n",
            "epoch 18 step 18 loss 1.647552490234375\n",
            "epoch 18 step 19 loss 1.6549267768859863\n",
            "epoch 18 step 20 loss 1.6340100765228271 test_accuracy 81.00000762939453 train_accuracy 85.546875\n",
            "epoch 18 step 21 loss 1.6396132707595825\n",
            "epoch 18 step 22 loss 1.6371006965637207\n",
            "epoch 18 step 23 loss 1.6053154468536377\n",
            "epoch 18 step 24 loss 1.6203140020370483\n",
            "epoch 18 step 25 loss 1.663040280342102\n",
            "epoch 18 step 26 loss 1.6571028232574463\n",
            "epoch 18 step 27 loss 1.663895845413208\n",
            "epoch 18 step 28 loss 1.6305174827575684\n",
            "epoch 18 step 29 loss 1.6444768905639648\n",
            "epoch 18 step 30 loss 1.651317834854126\n",
            "epoch 18 step 31 loss 1.6365618705749512\n",
            "epoch 18 step 32 loss 1.6492664813995361\n",
            "epoch 18 step 33 loss 1.6399624347686768\n",
            "epoch 18 step 34 loss 1.6476341485977173\n",
            "epoch 18 step 35 loss 1.6767836809158325\n",
            "epoch 18 step 36 loss 1.5983808040618896\n",
            "epoch 18 step 37 loss 1.6477335691452026\n",
            "epoch 18 step 38 loss 1.644258737564087\n",
            "epoch 18 step 39 loss 1.647136926651001\n",
            "epoch 18 step 40 loss 1.685786247253418 test_accuracy 80.50000762939453 train_accuracy 85.15625\n",
            "epoch 18 step 41 loss 1.6185119152069092\n",
            "epoch 18 step 42 loss 1.655942440032959\n",
            "epoch 18 step 43 loss 1.6480129957199097\n",
            "epoch 18 step 44 loss 1.6415951251983643\n",
            "epoch 18 step 45 loss 1.6410942077636719\n",
            "epoch 18 step 46 loss 1.6562516689300537\n",
            "epoch 18 step 47 loss 1.6559929847717285\n",
            "epoch 18 step 48 loss 1.661351203918457\n",
            "epoch 18 step 49 loss 1.6533726453781128\n",
            "epoch 18 step 50 loss 1.6748391389846802\n",
            "epoch 18 step 51 loss 1.6471009254455566\n",
            "epoch 18 step 52 loss 1.6417999267578125\n",
            "epoch 18 step 53 loss 1.6600900888442993\n",
            "epoch 18 step 54 loss 1.657808542251587\n",
            "epoch 18 step 55 loss 1.648504376411438\n",
            "epoch 18 step 56 loss 1.6418451070785522\n",
            "epoch 18 step 57 loss 1.6468106508255005\n",
            "epoch 18 step 58 loss 1.6732271909713745\n",
            "epoch 18 step 59 loss 1.6616027355194092\n",
            "epoch 18 step 60 loss 1.6521892547607422 test_accuracy 80.9000015258789 train_accuracy 85.546875\n",
            "epoch 18 step 61 loss 1.6335647106170654\n",
            "epoch 18 step 62 loss 1.6294233798980713\n",
            "epoch 18 step 63 loss 1.6728343963623047\n",
            "epoch 18 step 64 loss 1.6401031017303467\n",
            "epoch 18 step 65 loss 1.6849466562271118\n",
            "epoch 18 step 66 loss 1.6346410512924194\n",
            "epoch 18 step 67 loss 1.6310973167419434\n",
            "epoch 18 step 68 loss 1.66280996799469\n",
            "epoch 18 step 69 loss 1.6410733461380005\n",
            "epoch 18 step 70 loss 1.6397496461868286\n",
            "epoch 18 step 71 loss 1.6849392652511597\n",
            "epoch 18 step 72 loss 1.6616709232330322\n",
            "epoch 18 step 73 loss 1.6541483402252197\n",
            "epoch 18 step 74 loss 1.6792267560958862\n",
            "epoch 18 step 75 loss 1.6591237783432007\n",
            "epoch 18 step 76 loss 1.6472848653793335\n",
            "epoch 18 step 77 loss 1.6757899522781372\n",
            "epoch 18 step 78 loss 1.6852978467941284\n",
            "epoch 18 step 79 loss 1.6340217590332031\n",
            "epoch 18 step 80 loss 1.626711130142212 test_accuracy 79.4000015258789 train_accuracy 84.375\n",
            "epoch 18 step 81 loss 1.6701213121414185\n",
            "epoch 18 step 82 loss 1.6503403186798096\n",
            "epoch 18 step 83 loss 1.6414822340011597\n",
            "epoch 18 step 84 loss 1.6606816053390503\n",
            "epoch 18 step 85 loss 1.6625144481658936\n",
            "epoch 18 step 86 loss 1.638594150543213\n",
            "epoch 18 step 87 loss 1.661641240119934\n",
            "epoch 18 step 88 loss 1.6500343084335327\n",
            "epoch 18 step 89 loss 1.6699910163879395\n",
            "epoch 18 step 90 loss 1.6425259113311768\n",
            "epoch 18 step 91 loss 1.652617335319519\n",
            "epoch 18 step 92 loss 1.647718906402588\n",
            "epoch 18 step 93 loss 1.641310214996338\n",
            "epoch 18 step 94 loss 1.651716947555542\n",
            "epoch 18 step 95 loss 1.6502282619476318\n",
            "epoch 18 step 96 loss 1.6240383386611938\n",
            "epoch 18 step 97 loss 1.6440967321395874\n",
            "epoch 18 step 98 loss 1.6452523469924927\n",
            "epoch 18 step 99 loss 1.6703383922576904\n",
            "epoch 18 step 100 loss 1.670944333076477 test_accuracy 80.10000610351562 train_accuracy 86.9140625\n",
            "epoch 18 step 101 loss 1.6424832344055176\n",
            "epoch 18 step 102 loss 1.6288164854049683\n",
            "epoch 18 step 103 loss 1.6428943872451782\n",
            "epoch 18 step 104 loss 1.6399853229522705\n",
            "epoch 18 step 105 loss 1.6160131692886353\n",
            "epoch 18 step 106 loss 1.6292763948440552\n",
            "epoch 18 step 107 loss 1.6634787321090698\n",
            "epoch 18 step 108 loss 1.6601635217666626\n",
            "epoch 18 step 109 loss 1.6411750316619873\n",
            "epoch 18 step 110 loss 1.6512255668640137\n",
            "epoch 18 step 111 loss 1.6502183675765991\n",
            "epoch 18 step 112 loss 1.6741836071014404\n",
            "epoch 18 step 113 loss 1.6307507753372192\n",
            "epoch 18 step 114 loss 1.6291279792785645\n",
            "epoch 18 step 115 loss 1.6689995527267456\n",
            "epoch 18 step 116 loss 1.640679955482483\n",
            "epoch 18 step 117 loss 1.6349549293518066\n",
            "epoch 18 step 118 loss 1.6595323085784912\n",
            "epoch 18 step 119 loss 1.6521540880203247\n",
            "epoch 18 step 120 loss 1.6402987241744995 test_accuracy 81.9000015258789 train_accuracy 86.328125\n",
            "epoch 18 step 121 loss 1.6532691717147827\n",
            "epoch 18 step 122 loss 1.640761375427246\n",
            "epoch 18 step 123 loss 1.6206281185150146\n",
            "epoch 18 step 124 loss 1.6219288110733032\n",
            "epoch 18 step 125 loss 1.6608809232711792\n",
            "epoch 18 step 126 loss 1.668204426765442\n",
            "epoch 18 step 127 loss 1.6436551809310913\n",
            "epoch 18 step 128 loss 1.6364191770553589\n",
            "epoch 18 step 129 loss 1.6541146039962769\n",
            "epoch 18 step 130 loss 1.6555687189102173\n",
            "epoch 18 step 131 loss 1.64083731174469\n",
            "epoch 18 step 132 loss 1.6423165798187256\n",
            "epoch 18 step 133 loss 1.6503313779830933\n",
            "epoch 18 step 134 loss 1.6303666830062866\n",
            "epoch 18 step 135 loss 1.6149485111236572\n",
            "epoch 18 step 136 loss 1.656872272491455\n",
            "epoch 18 step 137 loss 1.6283609867095947\n",
            "epoch 18 step 138 loss 1.6190407276153564\n",
            "epoch 18 step 139 loss 1.656057357788086\n",
            "epoch 18 step 140 loss 1.6240010261535645 test_accuracy 80.70000457763672 train_accuracy 86.5234375\n",
            "epoch 18 step 141 loss 1.6386897563934326\n",
            "epoch 18 step 142 loss 1.6479746103286743\n",
            "epoch 18 step 143 loss 1.6648006439208984\n",
            "epoch 18 step 144 loss 1.6596976518630981\n",
            "epoch 18 step 145 loss 1.6433744430541992\n",
            "epoch 18 step 146 loss 1.6426628828048706\n",
            "epoch 18 step 147 loss 1.6174429655075073\n",
            "epoch 18 step 148 loss 1.6612164974212646\n",
            "epoch 18 step 149 loss 1.646944522857666\n",
            "epoch 18 step 150 loss 1.63457453250885\n",
            "epoch 18 step 151 loss 1.6703832149505615\n",
            "epoch 18 step 152 loss 1.6565989255905151\n",
            "epoch 18 step 153 loss 1.6390912532806396\n",
            "epoch 18 step 154 loss 1.6635680198669434\n",
            "epoch 18 step 155 loss 1.6585103273391724\n",
            "epoch 18 step 156 loss 1.6380577087402344\n",
            "epoch 18 step 157 loss 1.626314401626587\n",
            "epoch 18 step 158 loss 1.655067801475525\n",
            "epoch 18 step 159 loss 1.6492477655410767\n",
            "epoch 18 step 160 loss 1.6517250537872314 test_accuracy 79.30000305175781 train_accuracy 85.7421875\n",
            "epoch 18 step 161 loss 1.6531875133514404\n",
            "epoch 18 step 162 loss 1.6378562450408936\n",
            "epoch 18 step 163 loss 1.6594645977020264\n",
            "epoch 18 step 164 loss 1.6562334299087524\n",
            "epoch 18 step 165 loss 1.6608365774154663\n",
            "epoch 18 step 166 loss 1.6663738489151\n",
            "epoch 18 step 167 loss 1.650963544845581\n",
            "epoch 18 step 168 loss 1.6651262044906616\n",
            "epoch 18 step 169 loss 1.6349189281463623\n",
            "epoch 18 step 170 loss 1.6868494749069214\n",
            "epoch 18 step 171 loss 1.658130407333374\n",
            "epoch 18 step 172 loss 1.6189630031585693\n",
            "epoch 18 step 173 loss 1.6322702169418335\n",
            "epoch 18 step 174 loss 1.6212594509124756\n",
            "epoch 18 step 175 loss 1.6635020971298218\n",
            "epoch 18 step 176 loss 1.6326088905334473\n",
            "epoch 18 step 177 loss 1.6583932638168335\n",
            "epoch 18 step 178 loss 1.652172565460205\n",
            "epoch 18 step 179 loss 1.6347044706344604\n",
            "epoch 18 step 180 loss 1.667783498764038 test_accuracy 79.30000305175781 train_accuracy 86.5234375\n",
            "epoch 18 step 181 loss 1.6540489196777344\n",
            "epoch 18 step 182 loss 1.6523343324661255\n",
            "epoch 18 step 183 loss 1.6256519556045532\n",
            "epoch 18 step 184 loss 1.6446707248687744\n",
            "epoch 18 step 185 loss 1.6632542610168457\n",
            "epoch 18 step 186 loss 1.6306403875350952\n",
            "epoch 18 step 187 loss 1.6649137735366821\n",
            "epoch 18 step 188 loss 1.6328867673873901\n",
            "epoch 18 step 189 loss 1.6327974796295166\n",
            "epoch 18 step 190 loss 1.6501168012619019\n",
            "epoch 18 step 191 loss 1.679286241531372\n",
            "epoch 18 step 192 loss 1.6406744718551636\n",
            "epoch 18 step 193 loss 1.6652181148529053\n",
            "epoch 18 step 194 loss 1.6578361988067627\n",
            "epoch 18 step 195 loss 1.7015502452850342\n",
            "epoch 19 step 0 loss 1.653395175933838 test_accuracy 81.60000610351562 train_accuracy 83.3984375\n",
            "epoch 19 step 1 loss 1.626288890838623\n",
            "epoch 19 step 2 loss 1.6609549522399902\n",
            "epoch 19 step 3 loss 1.6341801881790161\n",
            "epoch 19 step 4 loss 1.6365406513214111\n",
            "epoch 19 step 5 loss 1.6318913698196411\n",
            "epoch 19 step 6 loss 1.652708888053894\n",
            "epoch 19 step 7 loss 1.679681658744812\n",
            "epoch 19 step 8 loss 1.6479557752609253\n",
            "epoch 19 step 9 loss 1.6221257448196411\n",
            "epoch 19 step 10 loss 1.6489734649658203\n",
            "epoch 19 step 11 loss 1.6528284549713135\n",
            "epoch 19 step 12 loss 1.6665462255477905\n",
            "epoch 19 step 13 loss 1.63705313205719\n",
            "epoch 19 step 14 loss 1.6444406509399414\n",
            "epoch 19 step 15 loss 1.6322908401489258\n",
            "epoch 19 step 16 loss 1.6536887884140015\n",
            "epoch 19 step 17 loss 1.6314918994903564\n",
            "epoch 19 step 18 loss 1.6256577968597412\n",
            "epoch 19 step 19 loss 1.6546372175216675\n",
            "epoch 19 step 20 loss 1.6513344049453735 test_accuracy 79.20000457763672 train_accuracy 88.0859375\n",
            "epoch 19 step 21 loss 1.6621055603027344\n",
            "epoch 19 step 22 loss 1.6492187976837158\n",
            "epoch 19 step 23 loss 1.6628929376602173\n",
            "epoch 19 step 24 loss 1.6487787961959839\n",
            "epoch 19 step 25 loss 1.633992075920105\n",
            "epoch 19 step 26 loss 1.635219931602478\n",
            "epoch 19 step 27 loss 1.6593211889266968\n",
            "epoch 19 step 28 loss 1.6450103521347046\n",
            "epoch 19 step 29 loss 1.6589796543121338\n",
            "epoch 19 step 30 loss 1.6414554119110107\n",
            "epoch 19 step 31 loss 1.6462198495864868\n",
            "epoch 19 step 32 loss 1.6441595554351807\n",
            "epoch 19 step 33 loss 1.6541519165039062\n",
            "epoch 19 step 34 loss 1.6304140090942383\n",
            "epoch 19 step 35 loss 1.655215859413147\n",
            "epoch 19 step 36 loss 1.6526460647583008\n",
            "epoch 19 step 37 loss 1.6327788829803467\n",
            "epoch 19 step 38 loss 1.6445685625076294\n",
            "epoch 19 step 39 loss 1.6079930067062378\n",
            "epoch 19 step 40 loss 1.6427394151687622 test_accuracy 78.60000610351562 train_accuracy 84.9609375\n",
            "epoch 19 step 41 loss 1.6520906686782837\n",
            "epoch 19 step 42 loss 1.657421588897705\n",
            "epoch 19 step 43 loss 1.654910922050476\n",
            "epoch 19 step 44 loss 1.6278454065322876\n",
            "epoch 19 step 45 loss 1.6408114433288574\n",
            "epoch 19 step 46 loss 1.6317377090454102\n",
            "epoch 19 step 47 loss 1.6360270977020264\n",
            "epoch 19 step 48 loss 1.627737045288086\n",
            "epoch 19 step 49 loss 1.643269658088684\n",
            "epoch 19 step 50 loss 1.677289605140686\n",
            "epoch 19 step 51 loss 1.6725109815597534\n",
            "epoch 19 step 52 loss 1.669023871421814\n",
            "epoch 19 step 53 loss 1.6454426050186157\n",
            "epoch 19 step 54 loss 1.6690880060195923\n",
            "epoch 19 step 55 loss 1.6394002437591553\n",
            "epoch 19 step 56 loss 1.6561095714569092\n",
            "epoch 19 step 57 loss 1.6615865230560303\n",
            "epoch 19 step 58 loss 1.610908031463623\n",
            "epoch 19 step 59 loss 1.628950595855713\n",
            "epoch 19 step 60 loss 1.6457386016845703 test_accuracy 80.4000015258789 train_accuracy 86.1328125\n",
            "epoch 19 step 61 loss 1.655496597290039\n",
            "epoch 19 step 62 loss 1.6601005792617798\n",
            "epoch 19 step 63 loss 1.632043719291687\n",
            "epoch 19 step 64 loss 1.65438711643219\n",
            "epoch 19 step 65 loss 1.6566073894500732\n",
            "epoch 19 step 66 loss 1.6525723934173584\n",
            "epoch 19 step 67 loss 1.6542885303497314\n",
            "epoch 19 step 68 loss 1.648160696029663\n",
            "epoch 19 step 69 loss 1.6675833463668823\n",
            "epoch 19 step 70 loss 1.6464866399765015\n",
            "epoch 19 step 71 loss 1.667701244354248\n",
            "epoch 19 step 72 loss 1.6357978582382202\n",
            "epoch 19 step 73 loss 1.6527681350708008\n",
            "epoch 19 step 74 loss 1.645650863647461\n",
            "epoch 19 step 75 loss 1.6292061805725098\n",
            "epoch 19 step 76 loss 1.6378765106201172\n",
            "epoch 19 step 77 loss 1.6490411758422852\n",
            "epoch 19 step 78 loss 1.6228333711624146\n",
            "epoch 19 step 79 loss 1.6335927248001099\n",
            "epoch 19 step 80 loss 1.6642987728118896 test_accuracy 81.60000610351562 train_accuracy 86.9140625\n",
            "epoch 19 step 81 loss 1.6359907388687134\n",
            "epoch 19 step 82 loss 1.6304848194122314\n",
            "epoch 19 step 83 loss 1.6853269338607788\n",
            "epoch 19 step 84 loss 1.6414694786071777\n",
            "epoch 19 step 85 loss 1.6536247730255127\n",
            "epoch 19 step 86 loss 1.636027455329895\n",
            "epoch 19 step 87 loss 1.642627239227295\n",
            "epoch 19 step 88 loss 1.6365569829940796\n",
            "epoch 19 step 89 loss 1.6697030067443848\n",
            "epoch 19 step 90 loss 1.6499457359313965\n",
            "epoch 19 step 91 loss 1.629492998123169\n",
            "epoch 19 step 92 loss 1.6690301895141602\n",
            "epoch 19 step 93 loss 1.6005222797393799\n",
            "epoch 19 step 94 loss 1.6213767528533936\n",
            "epoch 19 step 95 loss 1.646162986755371\n",
            "epoch 19 step 96 loss 1.6585438251495361\n",
            "epoch 19 step 97 loss 1.6412408351898193\n",
            "epoch 19 step 98 loss 1.6463149785995483\n",
            "epoch 19 step 99 loss 1.6604723930358887\n",
            "epoch 19 step 100 loss 1.6293531656265259 test_accuracy 81.70000457763672 train_accuracy 86.9140625\n",
            "epoch 19 step 101 loss 1.6639435291290283\n",
            "epoch 19 step 102 loss 1.6704968214035034\n",
            "epoch 19 step 103 loss 1.6579471826553345\n",
            "epoch 19 step 104 loss 1.6457778215408325\n",
            "epoch 19 step 105 loss 1.6686198711395264\n",
            "epoch 19 step 106 loss 1.6563327312469482\n",
            "epoch 19 step 107 loss 1.6628265380859375\n",
            "epoch 19 step 108 loss 1.6375513076782227\n",
            "epoch 19 step 109 loss 1.6205875873565674\n",
            "epoch 19 step 110 loss 1.629977822303772\n",
            "epoch 19 step 111 loss 1.6628811359405518\n",
            "epoch 19 step 112 loss 1.6549818515777588\n",
            "epoch 19 step 113 loss 1.6619478464126587\n",
            "epoch 19 step 114 loss 1.6351310014724731\n",
            "epoch 19 step 115 loss 1.6727104187011719\n",
            "epoch 19 step 116 loss 1.6534318923950195\n",
            "epoch 19 step 117 loss 1.6486645936965942\n",
            "epoch 19 step 118 loss 1.6580060720443726\n",
            "epoch 19 step 119 loss 1.6390870809555054\n",
            "epoch 19 step 120 loss 1.662628412246704 test_accuracy 81.60000610351562 train_accuracy 86.71875\n",
            "epoch 19 step 121 loss 1.655556321144104\n",
            "epoch 19 step 122 loss 1.671112298965454\n",
            "epoch 19 step 123 loss 1.6629117727279663\n",
            "epoch 19 step 124 loss 1.6443098783493042\n",
            "epoch 19 step 125 loss 1.625590443611145\n",
            "epoch 19 step 126 loss 1.6523189544677734\n",
            "epoch 19 step 127 loss 1.6248704195022583\n",
            "epoch 19 step 128 loss 1.6294852495193481\n",
            "epoch 19 step 129 loss 1.6463416814804077\n",
            "epoch 19 step 130 loss 1.6234451532363892\n",
            "epoch 19 step 131 loss 1.6469132900238037\n",
            "epoch 19 step 132 loss 1.6439679861068726\n",
            "epoch 19 step 133 loss 1.6300396919250488\n",
            "epoch 19 step 134 loss 1.6523222923278809\n",
            "epoch 19 step 135 loss 1.6140308380126953\n",
            "epoch 19 step 136 loss 1.6683939695358276\n",
            "epoch 19 step 137 loss 1.6263288259506226\n",
            "epoch 19 step 138 loss 1.6248034238815308\n",
            "epoch 19 step 139 loss 1.6599115133285522\n",
            "epoch 19 step 140 loss 1.6657270193099976 test_accuracy 80.80000305175781 train_accuracy 84.1796875\n",
            "epoch 19 step 141 loss 1.6491622924804688\n",
            "epoch 19 step 142 loss 1.6745232343673706\n",
            "epoch 19 step 143 loss 1.6567212343215942\n",
            "epoch 19 step 144 loss 1.6292486190795898\n",
            "epoch 19 step 145 loss 1.6312452554702759\n",
            "epoch 19 step 146 loss 1.6394671201705933\n",
            "epoch 19 step 147 loss 1.623549461364746\n",
            "epoch 19 step 148 loss 1.6471266746520996\n",
            "epoch 19 step 149 loss 1.652409315109253\n",
            "epoch 19 step 150 loss 1.6299185752868652\n",
            "epoch 19 step 151 loss 1.62411630153656\n",
            "epoch 19 step 152 loss 1.639219045639038\n",
            "epoch 19 step 153 loss 1.6920546293258667\n",
            "epoch 19 step 154 loss 1.6698439121246338\n",
            "epoch 19 step 155 loss 1.6423909664154053\n",
            "epoch 19 step 156 loss 1.6599448919296265\n",
            "epoch 19 step 157 loss 1.636291265487671\n",
            "epoch 19 step 158 loss 1.6502859592437744\n",
            "epoch 19 step 159 loss 1.6244055032730103\n",
            "epoch 19 step 160 loss 1.6615829467773438 test_accuracy 82.10000610351562 train_accuracy 88.671875\n",
            "epoch 19 step 161 loss 1.6612120866775513\n",
            "epoch 19 step 162 loss 1.6314878463745117\n",
            "epoch 19 step 163 loss 1.6525980234146118\n",
            "epoch 19 step 164 loss 1.6849408149719238\n",
            "epoch 19 step 165 loss 1.6496386528015137\n",
            "epoch 19 step 166 loss 1.6258901357650757\n",
            "epoch 19 step 167 loss 1.6377612352371216\n",
            "epoch 19 step 168 loss 1.6253387928009033\n",
            "epoch 19 step 169 loss 1.6414779424667358\n",
            "epoch 19 step 170 loss 1.6529285907745361\n",
            "epoch 19 step 171 loss 1.6256402730941772\n",
            "epoch 19 step 172 loss 1.6524732112884521\n",
            "epoch 19 step 173 loss 1.63277006149292\n",
            "epoch 19 step 174 loss 1.6022509336471558\n",
            "epoch 19 step 175 loss 1.6454617977142334\n",
            "epoch 19 step 176 loss 1.6432322263717651\n",
            "epoch 19 step 177 loss 1.6286753416061401\n",
            "epoch 19 step 178 loss 1.6526740789413452\n",
            "epoch 19 step 179 loss 1.6229889392852783\n",
            "epoch 19 step 180 loss 1.6314570903778076 test_accuracy 81.10000610351562 train_accuracy 90.0390625\n",
            "epoch 19 step 181 loss 1.650374174118042\n",
            "epoch 19 step 182 loss 1.6587327718734741\n",
            "epoch 19 step 183 loss 1.654830813407898\n",
            "epoch 19 step 184 loss 1.6511757373809814\n",
            "epoch 19 step 185 loss 1.669920802116394\n",
            "epoch 19 step 186 loss 1.6496630907058716\n",
            "epoch 19 step 187 loss 1.6424015760421753\n",
            "epoch 19 step 188 loss 1.672525405883789\n",
            "epoch 19 step 189 loss 1.648279070854187\n",
            "epoch 19 step 190 loss 1.6635080575942993\n",
            "epoch 19 step 191 loss 1.6402654647827148\n",
            "epoch 19 step 192 loss 1.655786156654358\n",
            "epoch 19 step 193 loss 1.6407907009124756\n",
            "epoch 19 step 194 loss 1.6154277324676514\n",
            "epoch 19 step 195 loss 1.6453006267547607\n",
            "epoch 20 step 0 loss 1.6247471570968628 test_accuracy 80.30000305175781 train_accuracy 86.71875\n",
            "epoch 20 step 1 loss 1.6479637622833252\n",
            "epoch 20 step 2 loss 1.6322461366653442\n",
            "epoch 20 step 3 loss 1.6046427488327026\n",
            "epoch 20 step 4 loss 1.6803356409072876\n",
            "epoch 20 step 5 loss 1.6480261087417603\n",
            "epoch 20 step 6 loss 1.6378724575042725\n",
            "epoch 20 step 7 loss 1.66061270236969\n",
            "epoch 20 step 8 loss 1.6077104806900024\n",
            "epoch 20 step 9 loss 1.643884301185608\n",
            "epoch 20 step 10 loss 1.635519027709961\n",
            "epoch 20 step 11 loss 1.6464735269546509\n",
            "epoch 20 step 12 loss 1.6776020526885986\n",
            "epoch 20 step 13 loss 1.6562505960464478\n",
            "epoch 20 step 14 loss 1.6342626810073853\n",
            "epoch 20 step 15 loss 1.6301331520080566\n",
            "epoch 20 step 16 loss 1.6618777513504028\n",
            "epoch 20 step 17 loss 1.6524161100387573\n",
            "epoch 20 step 18 loss 1.6413017511367798\n",
            "epoch 20 step 19 loss 1.6510088443756104\n",
            "epoch 20 step 20 loss 1.6388888359069824 test_accuracy 79.80000305175781 train_accuracy 84.765625\n",
            "epoch 20 step 21 loss 1.647246241569519\n",
            "epoch 20 step 22 loss 1.657119870185852\n",
            "epoch 20 step 23 loss 1.635869026184082\n",
            "epoch 20 step 24 loss 1.6276272535324097\n",
            "epoch 20 step 25 loss 1.6335541009902954\n",
            "epoch 20 step 26 loss 1.655747890472412\n",
            "epoch 20 step 27 loss 1.6605485677719116\n",
            "epoch 20 step 28 loss 1.6639583110809326\n",
            "epoch 20 step 29 loss 1.6584584712982178\n",
            "epoch 20 step 30 loss 1.6543519496917725\n",
            "epoch 20 step 31 loss 1.6467682123184204\n",
            "epoch 20 step 32 loss 1.6477464437484741\n",
            "epoch 20 step 33 loss 1.6628690958023071\n",
            "epoch 20 step 34 loss 1.631740689277649\n",
            "epoch 20 step 35 loss 1.6434999704360962\n",
            "epoch 20 step 36 loss 1.6513956785202026\n",
            "epoch 20 step 37 loss 1.6419310569763184\n",
            "epoch 20 step 38 loss 1.6554290056228638\n",
            "epoch 20 step 39 loss 1.6322219371795654\n",
            "epoch 20 step 40 loss 1.6213493347167969 test_accuracy 80.4000015258789 train_accuracy 87.3046875\n",
            "epoch 20 step 41 loss 1.6678807735443115\n",
            "epoch 20 step 42 loss 1.6661298274993896\n",
            "epoch 20 step 43 loss 1.6471484899520874\n",
            "epoch 20 step 44 loss 1.6526132822036743\n",
            "epoch 20 step 45 loss 1.654000163078308\n",
            "epoch 20 step 46 loss 1.6496094465255737\n",
            "epoch 20 step 47 loss 1.6610186100006104\n",
            "epoch 20 step 48 loss 1.65140700340271\n",
            "epoch 20 step 49 loss 1.6384676694869995\n",
            "epoch 20 step 50 loss 1.6186976432800293\n",
            "epoch 20 step 51 loss 1.6553430557250977\n",
            "epoch 20 step 52 loss 1.6186617612838745\n",
            "epoch 20 step 53 loss 1.6327334642410278\n",
            "epoch 20 step 54 loss 1.627297043800354\n",
            "epoch 20 step 55 loss 1.6503854990005493\n",
            "epoch 20 step 56 loss 1.6302425861358643\n",
            "epoch 20 step 57 loss 1.6700407266616821\n",
            "epoch 20 step 58 loss 1.6376465559005737\n",
            "epoch 20 step 59 loss 1.656830072402954\n",
            "epoch 20 step 60 loss 1.642286777496338 test_accuracy 83.4000015258789 train_accuracy 84.765625\n",
            "epoch 20 step 61 loss 1.6503300666809082\n",
            "epoch 20 step 62 loss 1.646310567855835\n",
            "epoch 20 step 63 loss 1.6607108116149902\n",
            "epoch 20 step 64 loss 1.6575658321380615\n",
            "epoch 20 step 65 loss 1.6444374322891235\n",
            "epoch 20 step 66 loss 1.6464388370513916\n",
            "epoch 20 step 67 loss 1.6548786163330078\n",
            "epoch 20 step 68 loss 1.628753900527954\n",
            "epoch 20 step 69 loss 1.6855324506759644\n",
            "epoch 20 step 70 loss 1.642843246459961\n",
            "epoch 20 step 71 loss 1.6558948755264282\n",
            "epoch 20 step 72 loss 1.676548719406128\n",
            "epoch 20 step 73 loss 1.6655901670455933\n",
            "epoch 20 step 74 loss 1.6279398202896118\n",
            "epoch 20 step 75 loss 1.646667242050171\n",
            "epoch 20 step 76 loss 1.63564133644104\n",
            "epoch 20 step 77 loss 1.637075662612915\n",
            "epoch 20 step 78 loss 1.6454825401306152\n",
            "epoch 20 step 79 loss 1.6273059844970703\n",
            "epoch 20 step 80 loss 1.6297907829284668 test_accuracy 80.70000457763672 train_accuracy 86.71875\n",
            "epoch 20 step 81 loss 1.6684075593948364\n",
            "epoch 20 step 82 loss 1.638264536857605\n",
            "epoch 20 step 83 loss 1.6482363939285278\n",
            "epoch 20 step 84 loss 1.633560299873352\n",
            "epoch 20 step 85 loss 1.6250118017196655\n",
            "epoch 20 step 86 loss 1.6377779245376587\n",
            "epoch 20 step 87 loss 1.659077763557434\n",
            "epoch 20 step 88 loss 1.631161093711853\n",
            "epoch 20 step 89 loss 1.6233525276184082\n",
            "epoch 20 step 90 loss 1.619407296180725\n",
            "epoch 20 step 91 loss 1.655720591545105\n",
            "epoch 20 step 92 loss 1.632612705230713\n",
            "epoch 20 step 93 loss 1.6527597904205322\n",
            "epoch 20 step 94 loss 1.6526775360107422\n",
            "epoch 20 step 95 loss 1.6672967672348022\n",
            "epoch 20 step 96 loss 1.6251704692840576\n",
            "epoch 20 step 97 loss 1.6489577293395996\n",
            "epoch 20 step 98 loss 1.6494897603988647\n",
            "epoch 20 step 99 loss 1.639111042022705\n",
            "epoch 20 step 100 loss 1.6388342380523682 test_accuracy 83.10000610351562 train_accuracy 85.7421875\n",
            "epoch 20 step 101 loss 1.6522644758224487\n",
            "epoch 20 step 102 loss 1.6742072105407715\n",
            "epoch 20 step 103 loss 1.6553912162780762\n",
            "epoch 20 step 104 loss 1.6347559690475464\n",
            "epoch 20 step 105 loss 1.6466503143310547\n",
            "epoch 20 step 106 loss 1.64999258518219\n",
            "epoch 20 step 107 loss 1.6237730979919434\n",
            "epoch 20 step 108 loss 1.6452769041061401\n",
            "epoch 20 step 109 loss 1.6464556455612183\n",
            "epoch 20 step 110 loss 1.6454460620880127\n",
            "epoch 20 step 111 loss 1.667291283607483\n",
            "epoch 20 step 112 loss 1.6416720151901245\n",
            "epoch 20 step 113 loss 1.6380418539047241\n",
            "epoch 20 step 114 loss 1.6535305976867676\n",
            "epoch 20 step 115 loss 1.6633038520812988\n",
            "epoch 20 step 116 loss 1.6558550596237183\n",
            "epoch 20 step 117 loss 1.6677584648132324\n",
            "epoch 20 step 118 loss 1.6703046560287476\n",
            "epoch 20 step 119 loss 1.6526727676391602\n",
            "epoch 20 step 120 loss 1.629227638244629 test_accuracy 79.5 train_accuracy 85.3515625\n",
            "epoch 20 step 121 loss 1.635439157485962\n",
            "epoch 20 step 122 loss 1.628504991531372\n",
            "epoch 20 step 123 loss 1.6346616744995117\n",
            "epoch 20 step 124 loss 1.6601077318191528\n",
            "epoch 20 step 125 loss 1.6592779159545898\n",
            "epoch 20 step 126 loss 1.6470825672149658\n",
            "epoch 20 step 127 loss 1.620115876197815\n",
            "epoch 20 step 128 loss 1.6480340957641602\n",
            "epoch 20 step 129 loss 1.658928632736206\n",
            "epoch 20 step 130 loss 1.63201904296875\n",
            "epoch 20 step 131 loss 1.6567518711090088\n",
            "epoch 20 step 132 loss 1.6360433101654053\n",
            "epoch 20 step 133 loss 1.667268991470337\n",
            "epoch 20 step 134 loss 1.647260308265686\n",
            "epoch 20 step 135 loss 1.6462079286575317\n",
            "epoch 20 step 136 loss 1.6498173475265503\n",
            "epoch 20 step 137 loss 1.6375536918640137\n",
            "epoch 20 step 138 loss 1.6457628011703491\n",
            "epoch 20 step 139 loss 1.6378575563430786\n",
            "epoch 20 step 140 loss 1.6550935506820679 test_accuracy 79.9000015258789 train_accuracy 85.546875\n",
            "epoch 20 step 141 loss 1.623990535736084\n",
            "epoch 20 step 142 loss 1.6187691688537598\n",
            "epoch 20 step 143 loss 1.6404037475585938\n",
            "epoch 20 step 144 loss 1.649855375289917\n",
            "epoch 20 step 145 loss 1.6310685873031616\n",
            "epoch 20 step 146 loss 1.648490309715271\n",
            "epoch 20 step 147 loss 1.6122108697891235\n",
            "epoch 20 step 148 loss 1.627631664276123\n",
            "epoch 20 step 149 loss 1.6735156774520874\n",
            "epoch 20 step 150 loss 1.6489806175231934\n",
            "epoch 20 step 151 loss 1.6350053548812866\n",
            "epoch 20 step 152 loss 1.6465588808059692\n",
            "epoch 20 step 153 loss 1.6601340770721436\n",
            "epoch 20 step 154 loss 1.6322790384292603\n",
            "epoch 20 step 155 loss 1.6410835981369019\n",
            "epoch 20 step 156 loss 1.6398605108261108\n",
            "epoch 20 step 157 loss 1.6356310844421387\n",
            "epoch 20 step 158 loss 1.6406298875808716\n",
            "epoch 20 step 159 loss 1.6391069889068604\n",
            "epoch 20 step 160 loss 1.6424736976623535 test_accuracy 80.70000457763672 train_accuracy 84.9609375\n",
            "epoch 20 step 161 loss 1.6400244235992432\n",
            "epoch 20 step 162 loss 1.6395655870437622\n",
            "epoch 20 step 163 loss 1.6664576530456543\n",
            "epoch 20 step 164 loss 1.6243090629577637\n",
            "epoch 20 step 165 loss 1.6665475368499756\n",
            "epoch 20 step 166 loss 1.6533358097076416\n",
            "epoch 20 step 167 loss 1.6135377883911133\n",
            "epoch 20 step 168 loss 1.6595799922943115\n",
            "epoch 20 step 169 loss 1.6393073797225952\n",
            "epoch 20 step 170 loss 1.6254503726959229\n",
            "epoch 20 step 171 loss 1.625349521636963\n",
            "epoch 20 step 172 loss 1.6382758617401123\n",
            "epoch 20 step 173 loss 1.655001163482666\n",
            "epoch 20 step 174 loss 1.6428018808364868\n",
            "epoch 20 step 175 loss 1.6695897579193115\n",
            "epoch 20 step 176 loss 1.6401293277740479\n",
            "epoch 20 step 177 loss 1.6702057123184204\n",
            "epoch 20 step 178 loss 1.642449140548706\n",
            "epoch 20 step 179 loss 1.6495544910430908\n",
            "epoch 20 step 180 loss 1.6515694856643677 test_accuracy 81.00000762939453 train_accuracy 87.3046875\n",
            "epoch 20 step 181 loss 1.6576778888702393\n",
            "epoch 20 step 182 loss 1.6410232782363892\n",
            "epoch 20 step 183 loss 1.6305245161056519\n",
            "epoch 20 step 184 loss 1.6328665018081665\n",
            "epoch 20 step 185 loss 1.610915184020996\n",
            "epoch 20 step 186 loss 1.6390817165374756\n",
            "epoch 20 step 187 loss 1.6622260808944702\n",
            "epoch 20 step 188 loss 1.637642741203308\n",
            "epoch 20 step 189 loss 1.6327975988388062\n",
            "epoch 20 step 190 loss 1.647132396697998\n",
            "epoch 20 step 191 loss 1.656542181968689\n",
            "epoch 20 step 192 loss 1.6548129320144653\n",
            "epoch 20 step 193 loss 1.6307339668273926\n",
            "epoch 20 step 194 loss 1.6180634498596191\n",
            "epoch 20 step 195 loss 1.6285988092422485\n",
            "epoch 21 step 0 loss 1.6414016485214233 test_accuracy 82.10000610351562 train_accuracy 85.9375\n",
            "epoch 21 step 1 loss 1.6220974922180176\n",
            "epoch 21 step 2 loss 1.6454579830169678\n",
            "epoch 21 step 3 loss 1.6439870595932007\n",
            "epoch 21 step 4 loss 1.6267787218093872\n",
            "epoch 21 step 5 loss 1.62846040725708\n",
            "epoch 21 step 6 loss 1.6508138179779053\n",
            "epoch 21 step 7 loss 1.636516809463501\n",
            "epoch 21 step 8 loss 1.6275426149368286\n",
            "epoch 21 step 9 loss 1.6420154571533203\n",
            "epoch 21 step 10 loss 1.6644681692123413\n",
            "epoch 21 step 11 loss 1.6675045490264893\n",
            "epoch 21 step 12 loss 1.6302125453948975\n",
            "epoch 21 step 13 loss 1.6571365594863892\n",
            "epoch 21 step 14 loss 1.6280139684677124\n",
            "epoch 21 step 15 loss 1.6500985622406006\n",
            "epoch 21 step 16 loss 1.6395418643951416\n",
            "epoch 21 step 17 loss 1.6497273445129395\n",
            "epoch 21 step 18 loss 1.6336894035339355\n",
            "epoch 21 step 19 loss 1.6345398426055908\n",
            "epoch 21 step 20 loss 1.6614221334457397 test_accuracy 82.10000610351562 train_accuracy 86.71875\n",
            "epoch 21 step 21 loss 1.6320449113845825\n",
            "epoch 21 step 22 loss 1.6497200727462769\n",
            "epoch 21 step 23 loss 1.6411030292510986\n",
            "epoch 21 step 24 loss 1.6769453287124634\n",
            "epoch 21 step 25 loss 1.6331557035446167\n",
            "epoch 21 step 26 loss 1.6238512992858887\n",
            "epoch 21 step 27 loss 1.6462509632110596\n",
            "epoch 21 step 28 loss 1.6637773513793945\n",
            "epoch 21 step 29 loss 1.6235462427139282\n",
            "epoch 21 step 30 loss 1.6327526569366455\n",
            "epoch 21 step 31 loss 1.6659245491027832\n",
            "epoch 21 step 32 loss 1.6430307626724243\n",
            "epoch 21 step 33 loss 1.6569384336471558\n",
            "epoch 21 step 34 loss 1.6485722064971924\n",
            "epoch 21 step 35 loss 1.6418133974075317\n",
            "epoch 21 step 36 loss 1.650699257850647\n",
            "epoch 21 step 37 loss 1.6639313697814941\n",
            "epoch 21 step 38 loss 1.6493663787841797\n",
            "epoch 21 step 39 loss 1.673668622970581\n",
            "epoch 21 step 40 loss 1.641979694366455 test_accuracy 79.80000305175781 train_accuracy 88.8671875\n",
            "epoch 21 step 41 loss 1.6633862257003784\n",
            "epoch 21 step 42 loss 1.6462726593017578\n",
            "epoch 21 step 43 loss 1.634931206703186\n",
            "epoch 21 step 44 loss 1.6387808322906494\n",
            "epoch 21 step 45 loss 1.624507188796997\n",
            "epoch 21 step 46 loss 1.6346198320388794\n",
            "epoch 21 step 47 loss 1.6589632034301758\n",
            "epoch 21 step 48 loss 1.6304497718811035\n",
            "epoch 21 step 49 loss 1.6218996047973633\n",
            "epoch 21 step 50 loss 1.653382420539856\n",
            "epoch 21 step 51 loss 1.639146327972412\n",
            "epoch 21 step 52 loss 1.6553255319595337\n",
            "epoch 21 step 53 loss 1.6414929628372192\n",
            "epoch 21 step 54 loss 1.6247962713241577\n",
            "epoch 21 step 55 loss 1.6517541408538818\n",
            "epoch 21 step 56 loss 1.6717987060546875\n",
            "epoch 21 step 57 loss 1.6384425163269043\n",
            "epoch 21 step 58 loss 1.6294714212417603\n",
            "epoch 21 step 59 loss 1.664181113243103\n",
            "epoch 21 step 60 loss 1.6303309202194214 test_accuracy 80.30000305175781 train_accuracy 87.6953125\n",
            "epoch 21 step 61 loss 1.604871153831482\n",
            "epoch 21 step 62 loss 1.6474233865737915\n",
            "epoch 21 step 63 loss 1.6420942544937134\n",
            "epoch 21 step 64 loss 1.641165018081665\n",
            "epoch 21 step 65 loss 1.6364798545837402\n",
            "epoch 21 step 66 loss 1.6516989469528198\n",
            "epoch 21 step 67 loss 1.6620358228683472\n",
            "epoch 21 step 68 loss 1.6305712461471558\n",
            "epoch 21 step 69 loss 1.656694769859314\n",
            "epoch 21 step 70 loss 1.6725674867630005\n",
            "epoch 21 step 71 loss 1.6179869174957275\n",
            "epoch 21 step 72 loss 1.6111502647399902\n",
            "epoch 21 step 73 loss 1.6449488401412964\n",
            "epoch 21 step 74 loss 1.6434615850448608\n",
            "epoch 21 step 75 loss 1.6050933599472046\n",
            "epoch 21 step 76 loss 1.6554001569747925\n",
            "epoch 21 step 77 loss 1.625652551651001\n",
            "epoch 21 step 78 loss 1.64047110080719\n",
            "epoch 21 step 79 loss 1.6433101892471313\n",
            "epoch 21 step 80 loss 1.6271382570266724 test_accuracy 78.80000305175781 train_accuracy 85.9375\n",
            "epoch 21 step 81 loss 1.6372450590133667\n",
            "epoch 21 step 82 loss 1.6474252939224243\n",
            "epoch 21 step 83 loss 1.6216132640838623\n",
            "epoch 21 step 84 loss 1.6414742469787598\n",
            "epoch 21 step 85 loss 1.6563324928283691\n",
            "epoch 21 step 86 loss 1.6651287078857422\n",
            "epoch 21 step 87 loss 1.643898844718933\n",
            "epoch 21 step 88 loss 1.6627053022384644\n",
            "epoch 21 step 89 loss 1.6386587619781494\n",
            "epoch 21 step 90 loss 1.6545230150222778\n",
            "epoch 21 step 91 loss 1.6384892463684082\n",
            "epoch 21 step 92 loss 1.6652939319610596\n",
            "epoch 21 step 93 loss 1.6508004665374756\n",
            "epoch 21 step 94 loss 1.6331754922866821\n",
            "epoch 21 step 95 loss 1.6228091716766357\n",
            "epoch 21 step 96 loss 1.6498384475708008\n",
            "epoch 21 step 97 loss 1.640964150428772\n",
            "epoch 21 step 98 loss 1.6473991870880127\n",
            "epoch 21 step 99 loss 1.635542869567871\n",
            "epoch 21 step 100 loss 1.6648783683776855 test_accuracy 80.0 train_accuracy 83.3984375\n",
            "epoch 21 step 101 loss 1.6331641674041748\n",
            "epoch 21 step 102 loss 1.643696665763855\n",
            "epoch 21 step 103 loss 1.6424155235290527\n",
            "epoch 21 step 104 loss 1.6592020988464355\n",
            "epoch 21 step 105 loss 1.622498631477356\n",
            "epoch 21 step 106 loss 1.6490665674209595\n",
            "epoch 21 step 107 loss 1.6330492496490479\n",
            "epoch 21 step 108 loss 1.6626832485198975\n",
            "epoch 21 step 109 loss 1.6363062858581543\n",
            "epoch 21 step 110 loss 1.629517674446106\n",
            "epoch 21 step 111 loss 1.6219797134399414\n",
            "epoch 21 step 112 loss 1.6464028358459473\n",
            "epoch 21 step 113 loss 1.631213903427124\n",
            "epoch 21 step 114 loss 1.6560213565826416\n",
            "epoch 21 step 115 loss 1.6466573476791382\n",
            "epoch 21 step 116 loss 1.6547728776931763\n",
            "epoch 21 step 117 loss 1.6555827856063843\n",
            "epoch 21 step 118 loss 1.665659785270691\n",
            "epoch 21 step 119 loss 1.6637563705444336\n",
            "epoch 21 step 120 loss 1.6359246969223022 test_accuracy 80.50000762939453 train_accuracy 85.15625\n",
            "epoch 21 step 121 loss 1.6477909088134766\n",
            "epoch 21 step 122 loss 1.6455309391021729\n",
            "epoch 21 step 123 loss 1.6654205322265625\n",
            "epoch 21 step 124 loss 1.660111665725708\n",
            "epoch 21 step 125 loss 1.6454864740371704\n",
            "epoch 21 step 126 loss 1.6510701179504395\n",
            "epoch 21 step 127 loss 1.619231104850769\n",
            "epoch 21 step 128 loss 1.6594964265823364\n",
            "epoch 21 step 129 loss 1.66619873046875\n",
            "epoch 21 step 130 loss 1.6661133766174316\n",
            "epoch 21 step 131 loss 1.6518384218215942\n",
            "epoch 21 step 132 loss 1.6401530504226685\n",
            "epoch 21 step 133 loss 1.6154000759124756\n",
            "epoch 21 step 134 loss 1.6817532777786255\n",
            "epoch 21 step 135 loss 1.6395342350006104\n",
            "epoch 21 step 136 loss 1.613765835762024\n",
            "epoch 21 step 137 loss 1.6336392164230347\n",
            "epoch 21 step 138 loss 1.6352683305740356\n",
            "epoch 21 step 139 loss 1.6243796348571777\n",
            "epoch 21 step 140 loss 1.648488163948059 test_accuracy 82.50000762939453 train_accuracy 87.6953125\n",
            "epoch 21 step 141 loss 1.6282448768615723\n",
            "epoch 21 step 142 loss 1.6198680400848389\n",
            "epoch 21 step 143 loss 1.6779128313064575\n",
            "epoch 21 step 144 loss 1.6831213235855103\n",
            "epoch 21 step 145 loss 1.6419240236282349\n",
            "epoch 21 step 146 loss 1.6444194316864014\n",
            "epoch 21 step 147 loss 1.6574054956436157\n",
            "epoch 21 step 148 loss 1.6176472902297974\n",
            "epoch 21 step 149 loss 1.6579548120498657\n",
            "epoch 21 step 150 loss 1.6530811786651611\n",
            "epoch 21 step 151 loss 1.6187268495559692\n",
            "epoch 21 step 152 loss 1.6458353996276855\n",
            "epoch 21 step 153 loss 1.642166018486023\n",
            "epoch 21 step 154 loss 1.6658695936203003\n",
            "epoch 21 step 155 loss 1.6462260484695435\n",
            "epoch 21 step 156 loss 1.649951457977295\n",
            "epoch 21 step 157 loss 1.6296350955963135\n",
            "epoch 21 step 158 loss 1.6291584968566895\n",
            "epoch 21 step 159 loss 1.658663272857666\n",
            "epoch 21 step 160 loss 1.64592444896698 test_accuracy 82.50000762939453 train_accuracy 85.9375\n",
            "epoch 21 step 161 loss 1.6356369256973267\n",
            "epoch 21 step 162 loss 1.6538501977920532\n",
            "epoch 21 step 163 loss 1.6381189823150635\n",
            "epoch 21 step 164 loss 1.6627851724624634\n",
            "epoch 21 step 165 loss 1.641745686531067\n",
            "epoch 21 step 166 loss 1.6194987297058105\n",
            "epoch 21 step 167 loss 1.6324511766433716\n",
            "epoch 21 step 168 loss 1.6573717594146729\n",
            "epoch 21 step 169 loss 1.6342387199401855\n",
            "epoch 21 step 170 loss 1.6405763626098633\n",
            "epoch 21 step 171 loss 1.6656948328018188\n",
            "epoch 21 step 172 loss 1.6537564992904663\n",
            "epoch 21 step 173 loss 1.6538794040679932\n",
            "epoch 21 step 174 loss 1.625205636024475\n",
            "epoch 21 step 175 loss 1.665653109550476\n",
            "epoch 21 step 176 loss 1.663908839225769\n",
            "epoch 21 step 177 loss 1.6330184936523438\n",
            "epoch 21 step 178 loss 1.6520466804504395\n",
            "epoch 21 step 179 loss 1.640884280204773\n",
            "epoch 21 step 180 loss 1.6280698776245117 test_accuracy 81.4000015258789 train_accuracy 86.328125\n",
            "epoch 21 step 181 loss 1.6699572801589966\n",
            "epoch 21 step 182 loss 1.6596403121948242\n",
            "epoch 21 step 183 loss 1.620942234992981\n",
            "epoch 21 step 184 loss 1.615429401397705\n",
            "epoch 21 step 185 loss 1.646924376487732\n",
            "epoch 21 step 186 loss 1.654818058013916\n",
            "epoch 21 step 187 loss 1.6376912593841553\n",
            "epoch 21 step 188 loss 1.6245994567871094\n",
            "epoch 21 step 189 loss 1.6542654037475586\n",
            "epoch 21 step 190 loss 1.6392662525177002\n",
            "epoch 21 step 191 loss 1.6523927450180054\n",
            "epoch 21 step 192 loss 1.6472182273864746\n",
            "epoch 21 step 193 loss 1.636256456375122\n",
            "epoch 21 step 194 loss 1.6196283102035522\n",
            "epoch 21 step 195 loss 1.6098016500473022\n",
            "epoch 22 step 0 loss 1.6427754163742065 test_accuracy 80.30000305175781 train_accuracy 89.6484375\n",
            "epoch 22 step 1 loss 1.6443605422973633\n",
            "epoch 22 step 2 loss 1.643935203552246\n",
            "epoch 22 step 3 loss 1.6383479833602905\n",
            "epoch 22 step 4 loss 1.658408761024475\n",
            "epoch 22 step 5 loss 1.5920723676681519\n",
            "epoch 22 step 6 loss 1.6031568050384521\n",
            "epoch 22 step 7 loss 1.6555286645889282\n",
            "epoch 22 step 8 loss 1.6114288568496704\n",
            "epoch 22 step 9 loss 1.6315158605575562\n",
            "epoch 22 step 10 loss 1.6205512285232544\n",
            "epoch 22 step 11 loss 1.643060326576233\n",
            "epoch 22 step 12 loss 1.6589624881744385\n",
            "epoch 22 step 13 loss 1.6468784809112549\n",
            "epoch 22 step 14 loss 1.6426339149475098\n",
            "epoch 22 step 15 loss 1.6546845436096191\n",
            "epoch 22 step 16 loss 1.6533664464950562\n",
            "epoch 22 step 17 loss 1.638289213180542\n",
            "epoch 22 step 18 loss 1.6450278759002686\n",
            "epoch 22 step 19 loss 1.617142915725708\n",
            "epoch 22 step 20 loss 1.643343448638916 test_accuracy 79.4000015258789 train_accuracy 85.15625\n",
            "epoch 22 step 21 loss 1.629535436630249\n",
            "epoch 22 step 22 loss 1.6756306886672974\n",
            "epoch 22 step 23 loss 1.6373635530471802\n",
            "epoch 22 step 24 loss 1.6538054943084717\n",
            "epoch 22 step 25 loss 1.6518858671188354\n",
            "epoch 22 step 26 loss 1.6165690422058105\n",
            "epoch 22 step 27 loss 1.6615405082702637\n",
            "epoch 22 step 28 loss 1.6258141994476318\n",
            "epoch 22 step 29 loss 1.6689668893814087\n",
            "epoch 22 step 30 loss 1.6425292491912842\n",
            "epoch 22 step 31 loss 1.645664930343628\n",
            "epoch 22 step 32 loss 1.6383795738220215\n",
            "epoch 22 step 33 loss 1.645127296447754\n",
            "epoch 22 step 34 loss 1.6175498962402344\n",
            "epoch 22 step 35 loss 1.628503441810608\n",
            "epoch 22 step 36 loss 1.6584886312484741\n",
            "epoch 22 step 37 loss 1.6771870851516724\n",
            "epoch 22 step 38 loss 1.6639330387115479\n",
            "epoch 22 step 39 loss 1.6307575702667236\n",
            "epoch 22 step 40 loss 1.658294439315796 test_accuracy 80.30000305175781 train_accuracy 86.1328125\n",
            "epoch 22 step 41 loss 1.6355608701705933\n",
            "epoch 22 step 42 loss 1.6194894313812256\n",
            "epoch 22 step 43 loss 1.643930196762085\n",
            "epoch 22 step 44 loss 1.6289840936660767\n",
            "epoch 22 step 45 loss 1.640373945236206\n",
            "epoch 22 step 46 loss 1.629638910293579\n",
            "epoch 22 step 47 loss 1.6461622714996338\n",
            "epoch 22 step 48 loss 1.6523224115371704\n",
            "epoch 22 step 49 loss 1.6658741235733032\n",
            "epoch 22 step 50 loss 1.6428028345108032\n",
            "epoch 22 step 51 loss 1.6445927619934082\n",
            "epoch 22 step 52 loss 1.648249626159668\n",
            "epoch 22 step 53 loss 1.6468265056610107\n",
            "epoch 22 step 54 loss 1.6340535879135132\n",
            "epoch 22 step 55 loss 1.637884497642517\n",
            "epoch 22 step 56 loss 1.6441277265548706\n",
            "epoch 22 step 57 loss 1.6661937236785889\n",
            "epoch 22 step 58 loss 1.6417475938796997\n",
            "epoch 22 step 59 loss 1.6626720428466797\n",
            "epoch 22 step 60 loss 1.645674228668213 test_accuracy 82.4000015258789 train_accuracy 85.15625\n",
            "epoch 22 step 61 loss 1.612738847732544\n",
            "epoch 22 step 62 loss 1.6492072343826294\n",
            "epoch 22 step 63 loss 1.6339465379714966\n",
            "epoch 22 step 64 loss 1.6683311462402344\n",
            "epoch 22 step 65 loss 1.6335411071777344\n",
            "epoch 22 step 66 loss 1.64544677734375\n",
            "epoch 22 step 67 loss 1.639954686164856\n",
            "epoch 22 step 68 loss 1.63148832321167\n",
            "epoch 22 step 69 loss 1.6541345119476318\n",
            "epoch 22 step 70 loss 1.6332980394363403\n",
            "epoch 22 step 71 loss 1.6272398233413696\n",
            "epoch 22 step 72 loss 1.6453582048416138\n",
            "epoch 22 step 73 loss 1.644744873046875\n",
            "epoch 22 step 74 loss 1.6289888620376587\n",
            "epoch 22 step 75 loss 1.655260443687439\n",
            "epoch 22 step 76 loss 1.6148550510406494\n",
            "epoch 22 step 77 loss 1.6260783672332764\n",
            "epoch 22 step 78 loss 1.6318804025650024\n",
            "epoch 22 step 79 loss 1.6490447521209717\n",
            "epoch 22 step 80 loss 1.643440842628479 test_accuracy 80.70000457763672 train_accuracy 85.9375\n",
            "epoch 22 step 81 loss 1.656104564666748\n",
            "epoch 22 step 82 loss 1.6459413766860962\n",
            "epoch 22 step 83 loss 1.6246914863586426\n",
            "epoch 22 step 84 loss 1.6436787843704224\n",
            "epoch 22 step 85 loss 1.6275177001953125\n",
            "epoch 22 step 86 loss 1.6592042446136475\n",
            "epoch 22 step 87 loss 1.6533787250518799\n",
            "epoch 22 step 88 loss 1.649795413017273\n",
            "epoch 22 step 89 loss 1.6466469764709473\n",
            "epoch 22 step 90 loss 1.647756576538086\n",
            "epoch 22 step 91 loss 1.642473816871643\n",
            "epoch 22 step 92 loss 1.6499805450439453\n",
            "epoch 22 step 93 loss 1.6284284591674805\n",
            "epoch 22 step 94 loss 1.6288163661956787\n",
            "epoch 22 step 95 loss 1.645223617553711\n",
            "epoch 22 step 96 loss 1.6436400413513184\n",
            "epoch 22 step 97 loss 1.6426799297332764\n",
            "epoch 22 step 98 loss 1.6503779888153076\n",
            "epoch 22 step 99 loss 1.6351449489593506\n",
            "epoch 22 step 100 loss 1.6267573833465576 test_accuracy 80.80000305175781 train_accuracy 88.28125\n",
            "epoch 22 step 101 loss 1.605815052986145\n",
            "epoch 22 step 102 loss 1.6658189296722412\n",
            "epoch 22 step 103 loss 1.6366245746612549\n",
            "epoch 22 step 104 loss 1.664899468421936\n",
            "epoch 22 step 105 loss 1.6192742586135864\n",
            "epoch 22 step 106 loss 1.6377376317977905\n",
            "epoch 22 step 107 loss 1.6422719955444336\n",
            "epoch 22 step 108 loss 1.6569117307662964\n",
            "epoch 22 step 109 loss 1.67622971534729\n",
            "epoch 22 step 110 loss 1.6264392137527466\n",
            "epoch 22 step 111 loss 1.6423131227493286\n",
            "epoch 22 step 112 loss 1.6801159381866455\n",
            "epoch 22 step 113 loss 1.6324313879013062\n",
            "epoch 22 step 114 loss 1.6625001430511475\n",
            "epoch 22 step 115 loss 1.6597421169281006\n",
            "epoch 22 step 116 loss 1.6552594900131226\n",
            "epoch 22 step 117 loss 1.5890613794326782\n",
            "epoch 22 step 118 loss 1.6561810970306396\n",
            "epoch 22 step 119 loss 1.6340794563293457\n",
            "epoch 22 step 120 loss 1.6408462524414062 test_accuracy 81.20000457763672 train_accuracy 85.9375\n",
            "epoch 22 step 121 loss 1.6450163125991821\n",
            "epoch 22 step 122 loss 1.6680045127868652\n",
            "epoch 22 step 123 loss 1.6551669836044312\n",
            "epoch 22 step 124 loss 1.5998166799545288\n",
            "epoch 22 step 125 loss 1.6190319061279297\n",
            "epoch 22 step 126 loss 1.6666394472122192\n",
            "epoch 22 step 127 loss 1.6621356010437012\n",
            "epoch 22 step 128 loss 1.6513363122940063\n",
            "epoch 22 step 129 loss 1.636292576789856\n",
            "epoch 22 step 130 loss 1.6268837451934814\n",
            "epoch 22 step 131 loss 1.6474049091339111\n",
            "epoch 22 step 132 loss 1.6698368787765503\n",
            "epoch 22 step 133 loss 1.6301077604293823\n",
            "epoch 22 step 134 loss 1.6109042167663574\n",
            "epoch 22 step 135 loss 1.6340091228485107\n",
            "epoch 22 step 136 loss 1.6686683893203735\n",
            "epoch 22 step 137 loss 1.664296269416809\n",
            "epoch 22 step 138 loss 1.6786788702011108\n",
            "epoch 22 step 139 loss 1.6235682964324951\n",
            "epoch 22 step 140 loss 1.6557695865631104 test_accuracy 82.10000610351562 train_accuracy 84.765625\n",
            "epoch 22 step 141 loss 1.641336441040039\n",
            "epoch 22 step 142 loss 1.6200902462005615\n",
            "epoch 22 step 143 loss 1.6296491622924805\n",
            "epoch 22 step 144 loss 1.6352952718734741\n",
            "epoch 22 step 145 loss 1.636155366897583\n",
            "epoch 22 step 146 loss 1.6644477844238281\n",
            "epoch 22 step 147 loss 1.6293799877166748\n",
            "epoch 22 step 148 loss 1.6403383016586304\n",
            "epoch 22 step 149 loss 1.6434309482574463\n",
            "epoch 22 step 150 loss 1.6818798780441284\n",
            "epoch 22 step 151 loss 1.6670278310775757\n",
            "epoch 22 step 152 loss 1.6255362033843994\n",
            "epoch 22 step 153 loss 1.6452362537384033\n",
            "epoch 22 step 154 loss 1.6288373470306396\n",
            "epoch 22 step 155 loss 1.6376397609710693\n",
            "epoch 22 step 156 loss 1.65489661693573\n",
            "epoch 22 step 157 loss 1.6682840585708618\n",
            "epoch 22 step 158 loss 1.6350244283676147\n",
            "epoch 22 step 159 loss 1.6695678234100342\n",
            "epoch 22 step 160 loss 1.6646883487701416 test_accuracy 81.30000305175781 train_accuracy 89.2578125\n",
            "epoch 22 step 161 loss 1.6477150917053223\n",
            "epoch 22 step 162 loss 1.6517815589904785\n",
            "epoch 22 step 163 loss 1.658230185508728\n",
            "epoch 22 step 164 loss 1.6402220726013184\n",
            "epoch 22 step 165 loss 1.6335681676864624\n",
            "epoch 22 step 166 loss 1.6374642848968506\n",
            "epoch 22 step 167 loss 1.6427195072174072\n",
            "epoch 22 step 168 loss 1.648161768913269\n",
            "epoch 22 step 169 loss 1.64306640625\n",
            "epoch 22 step 170 loss 1.6572494506835938\n",
            "epoch 22 step 171 loss 1.6556334495544434\n",
            "epoch 22 step 172 loss 1.6419847011566162\n",
            "epoch 22 step 173 loss 1.651925802230835\n",
            "epoch 22 step 174 loss 1.6238888502120972\n",
            "epoch 22 step 175 loss 1.644371509552002\n",
            "epoch 22 step 176 loss 1.6515076160430908\n",
            "epoch 22 step 177 loss 1.6327810287475586\n",
            "epoch 22 step 178 loss 1.631044864654541\n",
            "epoch 22 step 179 loss 1.6207224130630493\n",
            "epoch 22 step 180 loss 1.6387155055999756 test_accuracy 79.5 train_accuracy 84.765625\n",
            "epoch 22 step 181 loss 1.6169207096099854\n",
            "epoch 22 step 182 loss 1.6661244630813599\n",
            "epoch 22 step 183 loss 1.6698569059371948\n",
            "epoch 22 step 184 loss 1.6405717134475708\n",
            "epoch 22 step 185 loss 1.659627079963684\n",
            "epoch 22 step 186 loss 1.6751855611801147\n",
            "epoch 22 step 187 loss 1.631804347038269\n",
            "epoch 22 step 188 loss 1.649472951889038\n",
            "epoch 22 step 189 loss 1.6353123188018799\n",
            "epoch 22 step 190 loss 1.6280614137649536\n",
            "epoch 22 step 191 loss 1.6534758806228638\n",
            "epoch 22 step 192 loss 1.6320737600326538\n",
            "epoch 22 step 193 loss 1.6237746477127075\n",
            "epoch 22 step 194 loss 1.6596051454544067\n",
            "epoch 22 step 195 loss 1.5958082675933838\n",
            "epoch 23 step 0 loss 1.6603381633758545 test_accuracy 79.30000305175781 train_accuracy 85.15625\n",
            "epoch 23 step 1 loss 1.6351367235183716\n",
            "epoch 23 step 2 loss 1.6339366436004639\n",
            "epoch 23 step 3 loss 1.6532082557678223\n",
            "epoch 23 step 4 loss 1.6610385179519653\n",
            "epoch 23 step 5 loss 1.6394718885421753\n",
            "epoch 23 step 6 loss 1.6728978157043457\n",
            "epoch 23 step 7 loss 1.6705819368362427\n",
            "epoch 23 step 8 loss 1.6199971437454224\n",
            "epoch 23 step 9 loss 1.6429201364517212\n",
            "epoch 23 step 10 loss 1.6549166440963745\n",
            "epoch 23 step 11 loss 1.637716293334961\n",
            "epoch 23 step 12 loss 1.6633179187774658\n",
            "epoch 23 step 13 loss 1.6271775960922241\n",
            "epoch 23 step 14 loss 1.6592624187469482\n",
            "epoch 23 step 15 loss 1.6262867450714111\n",
            "epoch 23 step 16 loss 1.6171462535858154\n",
            "epoch 23 step 17 loss 1.636519193649292\n",
            "epoch 23 step 18 loss 1.6071162223815918\n",
            "epoch 23 step 19 loss 1.6396596431732178\n",
            "epoch 23 step 20 loss 1.6335740089416504 test_accuracy 81.70000457763672 train_accuracy 85.9375\n",
            "epoch 23 step 21 loss 1.6332935094833374\n",
            "epoch 23 step 22 loss 1.6555248498916626\n",
            "epoch 23 step 23 loss 1.6399931907653809\n",
            "epoch 23 step 24 loss 1.6155784130096436\n",
            "epoch 23 step 25 loss 1.6512303352355957\n",
            "epoch 23 step 26 loss 1.6438968181610107\n",
            "epoch 23 step 27 loss 1.6464049816131592\n",
            "epoch 23 step 28 loss 1.6474285125732422\n",
            "epoch 23 step 29 loss 1.6393485069274902\n",
            "epoch 23 step 30 loss 1.6315438747406006\n",
            "epoch 23 step 31 loss 1.6426753997802734\n",
            "epoch 23 step 32 loss 1.6427099704742432\n",
            "epoch 23 step 33 loss 1.6461716890335083\n",
            "epoch 23 step 34 loss 1.6335421800613403\n",
            "epoch 23 step 35 loss 1.6503901481628418\n",
            "epoch 23 step 36 loss 1.6380267143249512\n",
            "epoch 23 step 37 loss 1.6164124011993408\n",
            "epoch 23 step 38 loss 1.6175055503845215\n",
            "epoch 23 step 39 loss 1.6384037733078003\n",
            "epoch 23 step 40 loss 1.648722529411316 test_accuracy 82.50000762939453 train_accuracy 86.5234375\n",
            "epoch 23 step 41 loss 1.6330697536468506\n",
            "epoch 23 step 42 loss 1.6375774145126343\n",
            "epoch 23 step 43 loss 1.6303980350494385\n",
            "epoch 23 step 44 loss 1.664448618888855\n",
            "epoch 23 step 45 loss 1.6479051113128662\n",
            "epoch 23 step 46 loss 1.6511493921279907\n",
            "epoch 23 step 47 loss 1.6439485549926758\n",
            "epoch 23 step 48 loss 1.6480157375335693\n",
            "epoch 23 step 49 loss 1.659731388092041\n",
            "epoch 23 step 50 loss 1.6596386432647705\n",
            "epoch 23 step 51 loss 1.6263177394866943\n",
            "epoch 23 step 52 loss 1.6208571195602417\n",
            "epoch 23 step 53 loss 1.6529560089111328\n",
            "epoch 23 step 54 loss 1.6516468524932861\n",
            "epoch 23 step 55 loss 1.6437880992889404\n",
            "epoch 23 step 56 loss 1.6251226663589478\n",
            "epoch 23 step 57 loss 1.6441746950149536\n",
            "epoch 23 step 58 loss 1.6453129053115845\n",
            "epoch 23 step 59 loss 1.6040202379226685\n",
            "epoch 23 step 60 loss 1.6542131900787354 test_accuracy 81.10000610351562 train_accuracy 85.9375\n",
            "epoch 23 step 61 loss 1.6471236944198608\n",
            "epoch 23 step 62 loss 1.6634820699691772\n",
            "epoch 23 step 63 loss 1.6227216720581055\n",
            "epoch 23 step 64 loss 1.6500447988510132\n",
            "epoch 23 step 65 loss 1.6594024896621704\n",
            "epoch 23 step 66 loss 1.6420973539352417\n",
            "epoch 23 step 67 loss 1.6306469440460205\n",
            "epoch 23 step 68 loss 1.6506760120391846\n",
            "epoch 23 step 69 loss 1.6340856552124023\n",
            "epoch 23 step 70 loss 1.6594487428665161\n",
            "epoch 23 step 71 loss 1.6506083011627197\n",
            "epoch 23 step 72 loss 1.6447627544403076\n",
            "epoch 23 step 73 loss 1.6320242881774902\n",
            "epoch 23 step 74 loss 1.6246768236160278\n",
            "epoch 23 step 75 loss 1.634351372718811\n",
            "epoch 23 step 76 loss 1.6152489185333252\n",
            "epoch 23 step 77 loss 1.643227458000183\n",
            "epoch 23 step 78 loss 1.644950032234192\n",
            "epoch 23 step 79 loss 1.6432793140411377\n",
            "epoch 23 step 80 loss 1.621687412261963 test_accuracy 83.60000610351562 train_accuracy 88.671875\n",
            "epoch 23 step 81 loss 1.6531462669372559\n",
            "epoch 23 step 82 loss 1.6489479541778564\n",
            "epoch 23 step 83 loss 1.654166340827942\n",
            "epoch 23 step 84 loss 1.6766376495361328\n",
            "epoch 23 step 85 loss 1.6453899145126343\n",
            "epoch 23 step 86 loss 1.627954125404358\n",
            "epoch 23 step 87 loss 1.655362844467163\n",
            "epoch 23 step 88 loss 1.644010066986084\n",
            "epoch 23 step 89 loss 1.6235637664794922\n",
            "epoch 23 step 90 loss 1.6221708059310913\n",
            "epoch 23 step 91 loss 1.627121925354004\n",
            "epoch 23 step 92 loss 1.6150261163711548\n",
            "epoch 23 step 93 loss 1.6469680070877075\n",
            "epoch 23 step 94 loss 1.6481916904449463\n",
            "epoch 23 step 95 loss 1.6490882635116577\n",
            "epoch 23 step 96 loss 1.633107304573059\n",
            "epoch 23 step 97 loss 1.6331254243850708\n",
            "epoch 23 step 98 loss 1.6678295135498047\n",
            "epoch 23 step 99 loss 1.6344852447509766\n",
            "epoch 23 step 100 loss 1.6362905502319336 test_accuracy 82.9000015258789 train_accuracy 86.71875\n",
            "epoch 23 step 101 loss 1.6336627006530762\n",
            "epoch 23 step 102 loss 1.6688534021377563\n",
            "epoch 23 step 103 loss 1.6542868614196777\n",
            "epoch 23 step 104 loss 1.6604660749435425\n",
            "epoch 23 step 105 loss 1.6228721141815186\n",
            "epoch 23 step 106 loss 1.6506091356277466\n",
            "epoch 23 step 107 loss 1.6246631145477295\n",
            "epoch 23 step 108 loss 1.6453315019607544\n",
            "epoch 23 step 109 loss 1.6355111598968506\n",
            "epoch 23 step 110 loss 1.6626219749450684\n",
            "epoch 23 step 111 loss 1.6650477647781372\n",
            "epoch 23 step 112 loss 1.6626862287521362\n",
            "epoch 23 step 113 loss 1.6577622890472412\n",
            "epoch 23 step 114 loss 1.660591959953308\n",
            "epoch 23 step 115 loss 1.642059326171875\n",
            "epoch 23 step 116 loss 1.6417678594589233\n",
            "epoch 23 step 117 loss 1.6334134340286255\n",
            "epoch 23 step 118 loss 1.6396353244781494\n",
            "epoch 23 step 119 loss 1.6602061986923218\n",
            "epoch 23 step 120 loss 1.6567811965942383 test_accuracy 78.5 train_accuracy 87.109375\n",
            "epoch 23 step 121 loss 1.6482619047164917\n",
            "epoch 23 step 122 loss 1.6591304540634155\n",
            "epoch 23 step 123 loss 1.655027151107788\n",
            "epoch 23 step 124 loss 1.6555306911468506\n",
            "epoch 23 step 125 loss 1.6662428379058838\n",
            "epoch 23 step 126 loss 1.6309677362442017\n",
            "epoch 23 step 127 loss 1.6459918022155762\n",
            "epoch 23 step 128 loss 1.6763378381729126\n",
            "epoch 23 step 129 loss 1.6408028602600098\n",
            "epoch 23 step 130 loss 1.6391891241073608\n",
            "epoch 23 step 131 loss 1.660213589668274\n",
            "epoch 23 step 132 loss 1.6511740684509277\n",
            "epoch 23 step 133 loss 1.651937484741211\n",
            "epoch 23 step 134 loss 1.6440361738204956\n",
            "epoch 23 step 135 loss 1.6479989290237427\n",
            "epoch 23 step 136 loss 1.6310293674468994\n",
            "epoch 23 step 137 loss 1.6629822254180908\n",
            "epoch 23 step 138 loss 1.6496453285217285\n",
            "epoch 23 step 139 loss 1.6438617706298828\n",
            "epoch 23 step 140 loss 1.6263107061386108 test_accuracy 82.4000015258789 train_accuracy 84.375\n",
            "epoch 23 step 141 loss 1.6228182315826416\n",
            "epoch 23 step 142 loss 1.646417260169983\n",
            "epoch 23 step 143 loss 1.6552965641021729\n",
            "epoch 23 step 144 loss 1.6384695768356323\n",
            "epoch 23 step 145 loss 1.626692771911621\n",
            "epoch 23 step 146 loss 1.6296589374542236\n",
            "epoch 23 step 147 loss 1.6260192394256592\n",
            "epoch 23 step 148 loss 1.6217809915542603\n",
            "epoch 23 step 149 loss 1.6291640996932983\n",
            "epoch 23 step 150 loss 1.6629664897918701\n",
            "epoch 23 step 151 loss 1.6511796712875366\n",
            "epoch 23 step 152 loss 1.6198184490203857\n",
            "epoch 23 step 153 loss 1.61628258228302\n",
            "epoch 23 step 154 loss 1.6914024353027344\n",
            "epoch 23 step 155 loss 1.6346511840820312\n",
            "epoch 23 step 156 loss 1.6514445543289185\n",
            "epoch 23 step 157 loss 1.6581721305847168\n",
            "epoch 23 step 158 loss 1.629124402999878\n",
            "epoch 23 step 159 loss 1.6548569202423096\n",
            "epoch 23 step 160 loss 1.6365119218826294 test_accuracy 79.9000015258789 train_accuracy 87.109375\n",
            "epoch 23 step 161 loss 1.6457468271255493\n",
            "epoch 23 step 162 loss 1.646347165107727\n",
            "epoch 23 step 163 loss 1.6419720649719238\n",
            "epoch 23 step 164 loss 1.638037919998169\n",
            "epoch 23 step 165 loss 1.6515228748321533\n",
            "epoch 23 step 166 loss 1.6353510618209839\n",
            "epoch 23 step 167 loss 1.6364682912826538\n",
            "epoch 23 step 168 loss 1.6271244287490845\n",
            "epoch 23 step 169 loss 1.6293504238128662\n",
            "epoch 23 step 170 loss 1.619315266609192\n",
            "epoch 23 step 171 loss 1.6307345628738403\n",
            "epoch 23 step 172 loss 1.6402790546417236\n",
            "epoch 23 step 173 loss 1.6705632209777832\n",
            "epoch 23 step 174 loss 1.6470147371292114\n",
            "epoch 23 step 175 loss 1.6399657726287842\n",
            "epoch 23 step 176 loss 1.6064822673797607\n",
            "epoch 23 step 177 loss 1.6314319372177124\n",
            "epoch 23 step 178 loss 1.6567646265029907\n",
            "epoch 23 step 179 loss 1.6388075351715088\n",
            "epoch 23 step 180 loss 1.6337155103683472 test_accuracy 82.00000762939453 train_accuracy 86.71875\n",
            "epoch 23 step 181 loss 1.652055263519287\n",
            "epoch 23 step 182 loss 1.646358609199524\n",
            "epoch 23 step 183 loss 1.6305809020996094\n",
            "epoch 23 step 184 loss 1.6657226085662842\n",
            "epoch 23 step 185 loss 1.640062928199768\n",
            "epoch 23 step 186 loss 1.6581108570098877\n",
            "epoch 23 step 187 loss 1.6380300521850586\n",
            "epoch 23 step 188 loss 1.6360496282577515\n",
            "epoch 23 step 189 loss 1.6584874391555786\n",
            "epoch 23 step 190 loss 1.6349600553512573\n",
            "epoch 23 step 191 loss 1.6684761047363281\n",
            "epoch 23 step 192 loss 1.6534254550933838\n",
            "epoch 23 step 193 loss 1.6317037343978882\n",
            "epoch 23 step 194 loss 1.66400146484375\n",
            "epoch 23 step 195 loss 1.6609147787094116\n",
            "epoch 24 step 0 loss 1.6328595876693726 test_accuracy 80.4000015258789 train_accuracy 86.5234375\n",
            "epoch 24 step 1 loss 1.6420637369155884\n",
            "epoch 24 step 2 loss 1.631535291671753\n",
            "epoch 24 step 3 loss 1.642329216003418\n",
            "epoch 24 step 4 loss 1.6624308824539185\n",
            "epoch 24 step 5 loss 1.653523564338684\n",
            "epoch 24 step 6 loss 1.636473536491394\n",
            "epoch 24 step 7 loss 1.657729983329773\n",
            "epoch 24 step 8 loss 1.6534940004348755\n",
            "epoch 24 step 9 loss 1.664819598197937\n",
            "epoch 24 step 10 loss 1.6515949964523315\n",
            "epoch 24 step 11 loss 1.6253914833068848\n",
            "epoch 24 step 12 loss 1.653612732887268\n",
            "epoch 24 step 13 loss 1.623737096786499\n",
            "epoch 24 step 14 loss 1.6298521757125854\n",
            "epoch 24 step 15 loss 1.6333712339401245\n",
            "epoch 24 step 16 loss 1.6268633604049683\n",
            "epoch 24 step 17 loss 1.6210567951202393\n",
            "epoch 24 step 18 loss 1.6462604999542236\n",
            "epoch 24 step 19 loss 1.6494284868240356\n",
            "epoch 24 step 20 loss 1.6408095359802246 test_accuracy 80.9000015258789 train_accuracy 86.1328125\n",
            "epoch 24 step 21 loss 1.6535829305648804\n",
            "epoch 24 step 22 loss 1.6360114812850952\n",
            "epoch 24 step 23 loss 1.6450189352035522\n",
            "epoch 24 step 24 loss 1.662827968597412\n",
            "epoch 24 step 25 loss 1.6474493741989136\n",
            "epoch 24 step 26 loss 1.6344503164291382\n",
            "epoch 24 step 27 loss 1.6652201414108276\n",
            "epoch 24 step 28 loss 1.6395355463027954\n",
            "epoch 24 step 29 loss 1.6617236137390137\n",
            "epoch 24 step 30 loss 1.6252368688583374\n",
            "epoch 24 step 31 loss 1.6510579586029053\n",
            "epoch 24 step 32 loss 1.625911831855774\n",
            "epoch 24 step 33 loss 1.6638379096984863\n",
            "epoch 24 step 34 loss 1.6483309268951416\n",
            "epoch 24 step 35 loss 1.6300681829452515\n",
            "epoch 24 step 36 loss 1.6291160583496094\n",
            "epoch 24 step 37 loss 1.6457616090774536\n",
            "epoch 24 step 38 loss 1.6304395198822021\n",
            "epoch 24 step 39 loss 1.6373411417007446\n",
            "epoch 24 step 40 loss 1.640997290611267 test_accuracy 80.50000762939453 train_accuracy 86.328125\n",
            "epoch 24 step 41 loss 1.6438337564468384\n",
            "epoch 24 step 42 loss 1.6416254043579102\n",
            "epoch 24 step 43 loss 1.6133244037628174\n",
            "epoch 24 step 44 loss 1.630089521408081\n",
            "epoch 24 step 45 loss 1.6442662477493286\n",
            "epoch 24 step 46 loss 1.6671943664550781\n",
            "epoch 24 step 47 loss 1.646838665008545\n",
            "epoch 24 step 48 loss 1.62941575050354\n",
            "epoch 24 step 49 loss 1.6448990106582642\n",
            "epoch 24 step 50 loss 1.6449050903320312\n",
            "epoch 24 step 51 loss 1.6578569412231445\n",
            "epoch 24 step 52 loss 1.6524056196212769\n",
            "epoch 24 step 53 loss 1.6668897867202759\n",
            "epoch 24 step 54 loss 1.6313213109970093\n",
            "epoch 24 step 55 loss 1.6521340608596802\n",
            "epoch 24 step 56 loss 1.6502875089645386\n",
            "epoch 24 step 57 loss 1.6206244230270386\n",
            "epoch 24 step 58 loss 1.6313542127609253\n",
            "epoch 24 step 59 loss 1.6016032695770264\n",
            "epoch 24 step 60 loss 1.6705396175384521 test_accuracy 81.70000457763672 train_accuracy 86.9140625\n",
            "epoch 24 step 61 loss 1.6460480690002441\n",
            "epoch 24 step 62 loss 1.6285194158554077\n",
            "epoch 24 step 63 loss 1.6687815189361572\n",
            "epoch 24 step 64 loss 1.644504427909851\n",
            "epoch 24 step 65 loss 1.6696720123291016\n",
            "epoch 24 step 66 loss 1.6268415451049805\n",
            "epoch 24 step 67 loss 1.63302743434906\n",
            "epoch 24 step 68 loss 1.6261234283447266\n",
            "epoch 24 step 69 loss 1.6293359994888306\n",
            "epoch 24 step 70 loss 1.6537976264953613\n",
            "epoch 24 step 71 loss 1.6255296468734741\n",
            "epoch 24 step 72 loss 1.6302326917648315\n",
            "epoch 24 step 73 loss 1.6413511037826538\n",
            "epoch 24 step 74 loss 1.642042636871338\n",
            "epoch 24 step 75 loss 1.6444637775421143\n",
            "epoch 24 step 76 loss 1.6357554197311401\n",
            "epoch 24 step 77 loss 1.6458379030227661\n",
            "epoch 24 step 78 loss 1.656658411026001\n",
            "epoch 24 step 79 loss 1.6374386548995972\n",
            "epoch 24 step 80 loss 1.6103309392929077 test_accuracy 80.9000015258789 train_accuracy 86.328125\n",
            "epoch 24 step 81 loss 1.6761360168457031\n",
            "epoch 24 step 82 loss 1.6392501592636108\n",
            "epoch 24 step 83 loss 1.6249371767044067\n",
            "epoch 24 step 84 loss 1.6400549411773682\n",
            "epoch 24 step 85 loss 1.658322811126709\n",
            "epoch 24 step 86 loss 1.6577836275100708\n",
            "epoch 24 step 87 loss 1.668976902961731\n",
            "epoch 24 step 88 loss 1.6334049701690674\n",
            "epoch 24 step 89 loss 1.6262975931167603\n",
            "epoch 24 step 90 loss 1.656091332435608\n",
            "epoch 24 step 91 loss 1.6461573839187622\n",
            "epoch 24 step 92 loss 1.6487163305282593\n",
            "epoch 24 step 93 loss 1.6434307098388672\n",
            "epoch 24 step 94 loss 1.6459946632385254\n",
            "epoch 24 step 95 loss 1.631181240081787\n",
            "epoch 24 step 96 loss 1.6214592456817627\n",
            "epoch 24 step 97 loss 1.652547001838684\n",
            "epoch 24 step 98 loss 1.654817819595337\n",
            "epoch 24 step 99 loss 1.633984088897705\n",
            "epoch 24 step 100 loss 1.6347661018371582 test_accuracy 80.80000305175781 train_accuracy 85.7421875\n",
            "epoch 24 step 101 loss 1.6328588724136353\n",
            "epoch 24 step 102 loss 1.643056035041809\n",
            "epoch 24 step 103 loss 1.624683141708374\n",
            "epoch 24 step 104 loss 1.649600863456726\n",
            "epoch 24 step 105 loss 1.6613510847091675\n",
            "epoch 24 step 106 loss 1.6474425792694092\n",
            "epoch 24 step 107 loss 1.6382007598876953\n",
            "epoch 24 step 108 loss 1.6434377431869507\n",
            "epoch 24 step 109 loss 1.6620696783065796\n",
            "epoch 24 step 110 loss 1.6513488292694092\n",
            "epoch 24 step 111 loss 1.6204113960266113\n",
            "epoch 24 step 112 loss 1.6461893320083618\n",
            "epoch 24 step 113 loss 1.62940514087677\n",
            "epoch 24 step 114 loss 1.6113801002502441\n",
            "epoch 24 step 115 loss 1.6369097232818604\n",
            "epoch 24 step 116 loss 1.6388969421386719\n",
            "epoch 24 step 117 loss 1.6321924924850464\n",
            "epoch 24 step 118 loss 1.672781229019165\n",
            "epoch 24 step 119 loss 1.633009433746338\n",
            "epoch 24 step 120 loss 1.6300315856933594 test_accuracy 82.4000015258789 train_accuracy 86.5234375\n",
            "epoch 24 step 121 loss 1.6253548860549927\n",
            "epoch 24 step 122 loss 1.672024130821228\n",
            "epoch 24 step 123 loss 1.665764331817627\n",
            "epoch 24 step 124 loss 1.6713494062423706\n",
            "epoch 24 step 125 loss 1.6459002494812012\n",
            "epoch 24 step 126 loss 1.641358494758606\n",
            "epoch 24 step 127 loss 1.6389507055282593\n",
            "epoch 24 step 128 loss 1.652619481086731\n",
            "epoch 24 step 129 loss 1.6740307807922363\n",
            "epoch 24 step 130 loss 1.6444551944732666\n",
            "epoch 24 step 131 loss 1.6463088989257812\n",
            "epoch 24 step 132 loss 1.6564980745315552\n",
            "epoch 24 step 133 loss 1.642906665802002\n",
            "epoch 24 step 134 loss 1.6310343742370605\n",
            "epoch 24 step 135 loss 1.6437506675720215\n",
            "epoch 24 step 136 loss 1.649942398071289\n",
            "epoch 24 step 137 loss 1.6559451818466187\n",
            "epoch 24 step 138 loss 1.6649456024169922\n",
            "epoch 24 step 139 loss 1.6115738153457642\n",
            "epoch 24 step 140 loss 1.6409517526626587 test_accuracy 80.70000457763672 train_accuracy 87.109375\n",
            "epoch 24 step 141 loss 1.6583510637283325\n",
            "epoch 24 step 142 loss 1.6424275636672974\n",
            "epoch 24 step 143 loss 1.653192162513733\n",
            "epoch 24 step 144 loss 1.6306874752044678\n",
            "epoch 24 step 145 loss 1.6224271059036255\n",
            "epoch 24 step 146 loss 1.6103780269622803\n",
            "epoch 24 step 147 loss 1.6454757452011108\n",
            "epoch 24 step 148 loss 1.633978247642517\n",
            "epoch 24 step 149 loss 1.5997281074523926\n",
            "epoch 24 step 150 loss 1.6451960802078247\n",
            "epoch 24 step 151 loss 1.6457808017730713\n",
            "epoch 24 step 152 loss 1.6760425567626953\n",
            "epoch 24 step 153 loss 1.6331424713134766\n",
            "epoch 24 step 154 loss 1.6458399295806885\n",
            "epoch 24 step 155 loss 1.6532057523727417\n",
            "epoch 24 step 156 loss 1.6766284704208374\n",
            "epoch 24 step 157 loss 1.6483652591705322\n",
            "epoch 24 step 158 loss 1.632089614868164\n",
            "epoch 24 step 159 loss 1.6534337997436523\n",
            "epoch 24 step 160 loss 1.6580135822296143 test_accuracy 79.10000610351562 train_accuracy 85.546875\n",
            "epoch 24 step 161 loss 1.6619563102722168\n",
            "epoch 24 step 162 loss 1.633455753326416\n",
            "epoch 24 step 163 loss 1.6138461828231812\n",
            "epoch 24 step 164 loss 1.6408549547195435\n",
            "epoch 24 step 165 loss 1.6667574644088745\n",
            "epoch 24 step 166 loss 1.6387273073196411\n",
            "epoch 24 step 167 loss 1.6150246858596802\n",
            "epoch 24 step 168 loss 1.6258087158203125\n",
            "epoch 24 step 169 loss 1.657799243927002\n",
            "epoch 24 step 170 loss 1.6493868827819824\n",
            "epoch 24 step 171 loss 1.6496930122375488\n",
            "epoch 24 step 172 loss 1.6767686605453491\n",
            "epoch 24 step 173 loss 1.6345137357711792\n",
            "epoch 24 step 174 loss 1.6435555219650269\n",
            "epoch 24 step 175 loss 1.6295323371887207\n",
            "epoch 24 step 176 loss 1.6551414728164673\n",
            "epoch 24 step 177 loss 1.6611162424087524\n",
            "epoch 24 step 178 loss 1.6555830240249634\n",
            "epoch 24 step 179 loss 1.6398266553878784\n",
            "epoch 24 step 180 loss 1.6639317274093628 test_accuracy 79.9000015258789 train_accuracy 87.3046875\n",
            "epoch 24 step 181 loss 1.61421799659729\n",
            "epoch 24 step 182 loss 1.6340166330337524\n",
            "epoch 24 step 183 loss 1.6622334718704224\n",
            "epoch 24 step 184 loss 1.6398299932479858\n",
            "epoch 24 step 185 loss 1.6574864387512207\n",
            "epoch 24 step 186 loss 1.6547197103500366\n",
            "epoch 24 step 187 loss 1.6712815761566162\n",
            "epoch 24 step 188 loss 1.6177852153778076\n",
            "epoch 24 step 189 loss 1.654614806175232\n",
            "epoch 24 step 190 loss 1.6187283992767334\n",
            "epoch 24 step 191 loss 1.6392008066177368\n",
            "epoch 24 step 192 loss 1.6411597728729248\n",
            "epoch 24 step 193 loss 1.6174755096435547\n",
            "epoch 24 step 194 loss 1.6303257942199707\n",
            "epoch 24 step 195 loss 1.6813409328460693\n",
            "epoch 25 step 0 loss 1.6491533517837524 test_accuracy 80.0 train_accuracy 87.5\n",
            "epoch 25 step 1 loss 1.6649609804153442\n",
            "epoch 25 step 2 loss 1.6370131969451904\n",
            "epoch 25 step 3 loss 1.632773518562317\n",
            "epoch 25 step 4 loss 1.630717158317566\n",
            "epoch 25 step 5 loss 1.6154180765151978\n",
            "epoch 25 step 6 loss 1.6307475566864014\n",
            "epoch 25 step 7 loss 1.6485764980316162\n",
            "epoch 25 step 8 loss 1.6477829217910767\n",
            "epoch 25 step 9 loss 1.6404222249984741\n",
            "epoch 25 step 10 loss 1.6538727283477783\n",
            "epoch 25 step 11 loss 1.6506993770599365\n",
            "epoch 25 step 12 loss 1.683512806892395\n",
            "epoch 25 step 13 loss 1.6312029361724854\n",
            "epoch 25 step 14 loss 1.6618295907974243\n",
            "epoch 25 step 15 loss 1.6387863159179688\n",
            "epoch 25 step 16 loss 1.6050779819488525\n",
            "epoch 25 step 17 loss 1.6624282598495483\n",
            "epoch 25 step 18 loss 1.6591910123825073\n",
            "epoch 25 step 19 loss 1.633962631225586\n",
            "epoch 25 step 20 loss 1.6409624814987183 test_accuracy 79.80000305175781 train_accuracy 85.3515625\n",
            "epoch 25 step 21 loss 1.6273484230041504\n",
            "epoch 25 step 22 loss 1.6404364109039307\n",
            "epoch 25 step 23 loss 1.61429762840271\n",
            "epoch 25 step 24 loss 1.6197466850280762\n",
            "epoch 25 step 25 loss 1.6506078243255615\n",
            "epoch 25 step 26 loss 1.6401352882385254\n",
            "epoch 25 step 27 loss 1.6175209283828735\n",
            "epoch 25 step 28 loss 1.656806230545044\n",
            "epoch 25 step 29 loss 1.658455729484558\n",
            "epoch 25 step 30 loss 1.6403154134750366\n",
            "epoch 25 step 31 loss 1.6279773712158203\n",
            "epoch 25 step 32 loss 1.634896993637085\n",
            "epoch 25 step 33 loss 1.6578097343444824\n",
            "epoch 25 step 34 loss 1.6292309761047363\n",
            "epoch 25 step 35 loss 1.6673338413238525\n",
            "epoch 25 step 36 loss 1.6513036489486694\n",
            "epoch 25 step 37 loss 1.64723801612854\n",
            "epoch 25 step 38 loss 1.6618144512176514\n",
            "epoch 25 step 39 loss 1.631752848625183\n",
            "epoch 25 step 40 loss 1.6335737705230713 test_accuracy 80.80000305175781 train_accuracy 86.71875\n",
            "epoch 25 step 41 loss 1.6351436376571655\n",
            "epoch 25 step 42 loss 1.679185390472412\n",
            "epoch 25 step 43 loss 1.6282479763031006\n",
            "epoch 25 step 44 loss 1.6519920825958252\n",
            "epoch 25 step 45 loss 1.6602516174316406\n",
            "epoch 25 step 46 loss 1.6357396841049194\n",
            "epoch 25 step 47 loss 1.6150453090667725\n",
            "epoch 25 step 48 loss 1.630915641784668\n",
            "epoch 25 step 49 loss 1.630179524421692\n",
            "epoch 25 step 50 loss 1.6255133152008057\n",
            "epoch 25 step 51 loss 1.6272623538970947\n",
            "epoch 25 step 52 loss 1.6314401626586914\n",
            "epoch 25 step 53 loss 1.6448545455932617\n",
            "epoch 25 step 54 loss 1.6492657661437988\n",
            "epoch 25 step 55 loss 1.6311026811599731\n",
            "epoch 25 step 56 loss 1.6356489658355713\n",
            "epoch 25 step 57 loss 1.6660983562469482\n",
            "epoch 25 step 58 loss 1.634641408920288\n",
            "epoch 25 step 59 loss 1.6611683368682861\n",
            "epoch 25 step 60 loss 1.6485960483551025 test_accuracy 80.10000610351562 train_accuracy 84.375\n",
            "epoch 25 step 61 loss 1.633998990058899\n",
            "epoch 25 step 62 loss 1.6248663663864136\n",
            "epoch 25 step 63 loss 1.6077253818511963\n",
            "epoch 25 step 64 loss 1.6408838033676147\n",
            "epoch 25 step 65 loss 1.6478310823440552\n",
            "epoch 25 step 66 loss 1.6436735391616821\n",
            "epoch 25 step 67 loss 1.6483268737792969\n",
            "epoch 25 step 68 loss 1.643601894378662\n",
            "epoch 25 step 69 loss 1.6250795125961304\n",
            "epoch 25 step 70 loss 1.653952717781067\n",
            "epoch 25 step 71 loss 1.6695860624313354\n",
            "epoch 25 step 72 loss 1.6848869323730469\n",
            "epoch 25 step 73 loss 1.650709629058838\n",
            "epoch 25 step 74 loss 1.6473023891448975\n",
            "epoch 25 step 75 loss 1.6523643732070923\n",
            "epoch 25 step 76 loss 1.6460363864898682\n",
            "epoch 25 step 77 loss 1.6423430442810059\n",
            "epoch 25 step 78 loss 1.6419330835342407\n",
            "epoch 25 step 79 loss 1.6206958293914795\n",
            "epoch 25 step 80 loss 1.6414967775344849 test_accuracy 80.20000457763672 train_accuracy 84.5703125\n",
            "epoch 25 step 81 loss 1.6440279483795166\n",
            "epoch 25 step 82 loss 1.6310429573059082\n",
            "epoch 25 step 83 loss 1.6542824506759644\n",
            "epoch 25 step 84 loss 1.6230055093765259\n",
            "epoch 25 step 85 loss 1.635556936264038\n",
            "epoch 25 step 86 loss 1.6367051601409912\n",
            "epoch 25 step 87 loss 1.642011046409607\n",
            "epoch 25 step 88 loss 1.6441725492477417\n",
            "epoch 25 step 89 loss 1.632298469543457\n",
            "epoch 25 step 90 loss 1.6442060470581055\n",
            "epoch 25 step 91 loss 1.638534665107727\n",
            "epoch 25 step 92 loss 1.6352615356445312\n",
            "epoch 25 step 93 loss 1.6574550867080688\n",
            "epoch 25 step 94 loss 1.6402844190597534\n",
            "epoch 25 step 95 loss 1.646936058998108\n",
            "epoch 25 step 96 loss 1.6921091079711914\n",
            "epoch 25 step 97 loss 1.6331202983856201\n",
            "epoch 25 step 98 loss 1.6496561765670776\n",
            "epoch 25 step 99 loss 1.6559556722640991\n",
            "epoch 25 step 100 loss 1.6803661584854126 test_accuracy 80.30000305175781 train_accuracy 85.3515625\n",
            "epoch 25 step 101 loss 1.6539384126663208\n",
            "epoch 25 step 102 loss 1.6583179235458374\n",
            "epoch 25 step 103 loss 1.6435778141021729\n",
            "epoch 25 step 104 loss 1.6419317722320557\n",
            "epoch 25 step 105 loss 1.6458016633987427\n",
            "epoch 25 step 106 loss 1.6545252799987793\n",
            "epoch 25 step 107 loss 1.6244871616363525\n",
            "epoch 25 step 108 loss 1.6343936920166016\n",
            "epoch 25 step 109 loss 1.6340079307556152\n",
            "epoch 25 step 110 loss 1.6486163139343262\n",
            "epoch 25 step 111 loss 1.6371779441833496\n",
            "epoch 25 step 112 loss 1.6220660209655762\n",
            "epoch 25 step 113 loss 1.6457334756851196\n",
            "epoch 25 step 114 loss 1.6571751832962036\n",
            "epoch 25 step 115 loss 1.6750904321670532\n",
            "epoch 25 step 116 loss 1.6699732542037964\n",
            "epoch 25 step 117 loss 1.638393759727478\n",
            "epoch 25 step 118 loss 1.625398874282837\n",
            "epoch 25 step 119 loss 1.6350536346435547\n",
            "epoch 25 step 120 loss 1.6317819356918335 test_accuracy 81.30000305175781 train_accuracy 84.765625\n",
            "epoch 25 step 121 loss 1.623852014541626\n",
            "epoch 25 step 122 loss 1.655791163444519\n",
            "epoch 25 step 123 loss 1.6399409770965576\n",
            "epoch 25 step 124 loss 1.6443592309951782\n",
            "epoch 25 step 125 loss 1.6399270296096802\n",
            "epoch 25 step 126 loss 1.6452622413635254\n",
            "epoch 25 step 127 loss 1.6470022201538086\n",
            "epoch 25 step 128 loss 1.6398712396621704\n",
            "epoch 25 step 129 loss 1.6186189651489258\n",
            "epoch 25 step 130 loss 1.642844557762146\n",
            "epoch 25 step 131 loss 1.6500035524368286\n",
            "epoch 25 step 132 loss 1.6622803211212158\n",
            "epoch 25 step 133 loss 1.6372796297073364\n",
            "epoch 25 step 134 loss 1.6558616161346436\n",
            "epoch 25 step 135 loss 1.6370269060134888\n",
            "epoch 25 step 136 loss 1.6319531202316284\n",
            "epoch 25 step 137 loss 1.65433669090271\n",
            "epoch 25 step 138 loss 1.632174015045166\n",
            "epoch 25 step 139 loss 1.6566746234893799\n",
            "epoch 25 step 140 loss 1.6539572477340698 test_accuracy 81.10000610351562 train_accuracy 86.328125\n",
            "epoch 25 step 141 loss 1.6304243803024292\n",
            "epoch 25 step 142 loss 1.6466180086135864\n",
            "epoch 25 step 143 loss 1.6311208009719849\n",
            "epoch 25 step 144 loss 1.649616003036499\n",
            "epoch 25 step 145 loss 1.6255931854248047\n",
            "epoch 25 step 146 loss 1.6324529647827148\n",
            "epoch 25 step 147 loss 1.6637715101242065\n",
            "epoch 25 step 148 loss 1.6568057537078857\n",
            "epoch 25 step 149 loss 1.6281437873840332\n",
            "epoch 25 step 150 loss 1.6157464981079102\n",
            "epoch 25 step 151 loss 1.594591736793518\n",
            "epoch 25 step 152 loss 1.6513420343399048\n",
            "epoch 25 step 153 loss 1.6201192140579224\n",
            "epoch 25 step 154 loss 1.6454384326934814\n",
            "epoch 25 step 155 loss 1.6548357009887695\n",
            "epoch 25 step 156 loss 1.6187390089035034\n",
            "epoch 25 step 157 loss 1.63441002368927\n",
            "epoch 25 step 158 loss 1.6483335494995117\n",
            "epoch 25 step 159 loss 1.6392872333526611\n",
            "epoch 25 step 160 loss 1.6217925548553467 test_accuracy 82.50000762939453 train_accuracy 86.1328125\n",
            "epoch 25 step 161 loss 1.6605106592178345\n",
            "epoch 25 step 162 loss 1.6407361030578613\n",
            "epoch 25 step 163 loss 1.6344727277755737\n",
            "epoch 25 step 164 loss 1.6429516077041626\n",
            "epoch 25 step 165 loss 1.6285260915756226\n",
            "epoch 25 step 166 loss 1.6365457773208618\n",
            "epoch 25 step 167 loss 1.6463181972503662\n",
            "epoch 25 step 168 loss 1.6386520862579346\n",
            "epoch 25 step 169 loss 1.6555428504943848\n",
            "epoch 25 step 170 loss 1.639175534248352\n",
            "epoch 25 step 171 loss 1.6844233274459839\n",
            "epoch 25 step 172 loss 1.6433629989624023\n",
            "epoch 25 step 173 loss 1.6394466161727905\n",
            "epoch 25 step 174 loss 1.6380430459976196\n",
            "epoch 25 step 175 loss 1.6633296012878418\n",
            "epoch 25 step 176 loss 1.6298863887786865\n",
            "epoch 25 step 177 loss 1.6465375423431396\n",
            "epoch 25 step 178 loss 1.6139953136444092\n",
            "epoch 25 step 179 loss 1.6393460035324097\n",
            "epoch 25 step 180 loss 1.6499110460281372 test_accuracy 80.50000762939453 train_accuracy 87.6953125\n",
            "epoch 25 step 181 loss 1.6563997268676758\n",
            "epoch 25 step 182 loss 1.656512975692749\n",
            "epoch 25 step 183 loss 1.6560288667678833\n",
            "epoch 25 step 184 loss 1.6288526058197021\n",
            "epoch 25 step 185 loss 1.6573104858398438\n",
            "epoch 25 step 186 loss 1.642896294593811\n",
            "epoch 25 step 187 loss 1.623390793800354\n",
            "epoch 25 step 188 loss 1.6318060159683228\n",
            "epoch 25 step 189 loss 1.647902011871338\n",
            "epoch 25 step 190 loss 1.647476315498352\n",
            "epoch 25 step 191 loss 1.6232819557189941\n",
            "epoch 25 step 192 loss 1.6415640115737915\n",
            "epoch 25 step 193 loss 1.656172752380371\n",
            "epoch 25 step 194 loss 1.6316113471984863\n",
            "epoch 25 step 195 loss 1.6424884796142578\n",
            "epoch 26 step 0 loss 1.6291142702102661 test_accuracy 82.10000610351562 train_accuracy 86.5234375\n",
            "epoch 26 step 1 loss 1.6605916023254395\n",
            "epoch 26 step 2 loss 1.6460627317428589\n",
            "epoch 26 step 3 loss 1.6394784450531006\n",
            "epoch 26 step 4 loss 1.654544472694397\n",
            "epoch 26 step 5 loss 1.6556380987167358\n",
            "epoch 26 step 6 loss 1.6397273540496826\n",
            "epoch 26 step 7 loss 1.6413601636886597\n",
            "epoch 26 step 8 loss 1.6565792560577393\n",
            "epoch 26 step 9 loss 1.63966703414917\n",
            "epoch 26 step 10 loss 1.6305583715438843\n",
            "epoch 26 step 11 loss 1.6374945640563965\n",
            "epoch 26 step 12 loss 1.6504076719284058\n",
            "epoch 26 step 13 loss 1.6448386907577515\n",
            "epoch 26 step 14 loss 1.644915223121643\n",
            "epoch 26 step 15 loss 1.6501187086105347\n",
            "epoch 26 step 16 loss 1.6305310726165771\n",
            "epoch 26 step 17 loss 1.6493178606033325\n",
            "epoch 26 step 18 loss 1.637337565422058\n",
            "epoch 26 step 19 loss 1.6401327848434448\n",
            "epoch 26 step 20 loss 1.636544108390808 test_accuracy 81.80000305175781 train_accuracy 87.6953125\n",
            "epoch 26 step 21 loss 1.6556020975112915\n",
            "epoch 26 step 22 loss 1.646851658821106\n",
            "epoch 26 step 23 loss 1.643531084060669\n",
            "epoch 26 step 24 loss 1.6512365341186523\n",
            "epoch 26 step 25 loss 1.6132291555404663\n",
            "epoch 26 step 26 loss 1.6548566818237305\n",
            "epoch 26 step 27 loss 1.6458526849746704\n",
            "epoch 26 step 28 loss 1.6566871404647827\n",
            "epoch 26 step 29 loss 1.6787137985229492\n",
            "epoch 26 step 30 loss 1.6388919353485107\n",
            "epoch 26 step 31 loss 1.6651750802993774\n",
            "epoch 26 step 32 loss 1.6294121742248535\n",
            "epoch 26 step 33 loss 1.6374320983886719\n",
            "epoch 26 step 34 loss 1.6313525438308716\n",
            "epoch 26 step 35 loss 1.6613125801086426\n",
            "epoch 26 step 36 loss 1.637550950050354\n",
            "epoch 26 step 37 loss 1.6358580589294434\n",
            "epoch 26 step 38 loss 1.6633954048156738\n",
            "epoch 26 step 39 loss 1.6512902975082397\n",
            "epoch 26 step 40 loss 1.6549919843673706 test_accuracy 81.10000610351562 train_accuracy 86.1328125\n",
            "epoch 26 step 41 loss 1.653144121170044\n",
            "epoch 26 step 42 loss 1.6386559009552002\n",
            "epoch 26 step 43 loss 1.6345117092132568\n",
            "epoch 26 step 44 loss 1.6623207330703735\n",
            "epoch 26 step 45 loss 1.6488245725631714\n",
            "epoch 26 step 46 loss 1.6793805360794067\n",
            "epoch 26 step 47 loss 1.6942822933197021\n",
            "epoch 26 step 48 loss 1.6149623394012451\n",
            "epoch 26 step 49 loss 1.6514132022857666\n",
            "epoch 26 step 50 loss 1.6401352882385254\n",
            "epoch 26 step 51 loss 1.672396183013916\n",
            "epoch 26 step 52 loss 1.648071527481079\n",
            "epoch 26 step 53 loss 1.6397058963775635\n",
            "epoch 26 step 54 loss 1.654825210571289\n",
            "epoch 26 step 55 loss 1.6178321838378906\n",
            "epoch 26 step 56 loss 1.6386288404464722\n",
            "epoch 26 step 57 loss 1.6333532333374023\n",
            "epoch 26 step 58 loss 1.6568788290023804\n",
            "epoch 26 step 59 loss 1.6595890522003174\n",
            "epoch 26 step 60 loss 1.644567847251892 test_accuracy 82.70000457763672 train_accuracy 88.4765625\n",
            "epoch 26 step 61 loss 1.6143323183059692\n",
            "epoch 26 step 62 loss 1.6613460779190063\n",
            "epoch 26 step 63 loss 1.6127926111221313\n",
            "epoch 26 step 64 loss 1.6382849216461182\n",
            "epoch 26 step 65 loss 1.6519774198532104\n",
            "epoch 26 step 66 loss 1.6307809352874756\n",
            "epoch 26 step 67 loss 1.6092385053634644\n",
            "epoch 26 step 68 loss 1.652195692062378\n",
            "epoch 26 step 69 loss 1.6531713008880615\n",
            "epoch 26 step 70 loss 1.6190885305404663\n",
            "epoch 26 step 71 loss 1.6281890869140625\n",
            "epoch 26 step 72 loss 1.6816203594207764\n",
            "epoch 26 step 73 loss 1.6585028171539307\n",
            "epoch 26 step 74 loss 1.66231107711792\n",
            "epoch 26 step 75 loss 1.6361515522003174\n",
            "epoch 26 step 76 loss 1.669477105140686\n",
            "epoch 26 step 77 loss 1.6625055074691772\n",
            "epoch 26 step 78 loss 1.6312053203582764\n",
            "epoch 26 step 79 loss 1.63422691822052\n",
            "epoch 26 step 80 loss 1.6271064281463623 test_accuracy 80.9000015258789 train_accuracy 89.6484375\n",
            "epoch 26 step 81 loss 1.6810390949249268\n",
            "epoch 26 step 82 loss 1.670533537864685\n",
            "epoch 26 step 83 loss 1.6205862760543823\n",
            "epoch 26 step 84 loss 1.620943546295166\n",
            "epoch 26 step 85 loss 1.6039057970046997\n",
            "epoch 26 step 86 loss 1.6222223043441772\n",
            "epoch 26 step 87 loss 1.6689072847366333\n",
            "epoch 26 step 88 loss 1.6637545824050903\n",
            "epoch 26 step 89 loss 1.6170754432678223\n",
            "epoch 26 step 90 loss 1.6346596479415894\n",
            "epoch 26 step 91 loss 1.616431713104248\n",
            "epoch 26 step 92 loss 1.621971845626831\n",
            "epoch 26 step 93 loss 1.6410448551177979\n",
            "epoch 26 step 94 loss 1.627677321434021\n",
            "epoch 26 step 95 loss 1.6395988464355469\n",
            "epoch 26 step 96 loss 1.646554708480835\n",
            "epoch 26 step 97 loss 1.678990364074707\n",
            "epoch 26 step 98 loss 1.6329927444458008\n",
            "epoch 26 step 99 loss 1.633959412574768\n",
            "epoch 26 step 100 loss 1.6583843231201172 test_accuracy 81.4000015258789 train_accuracy 86.9140625\n",
            "epoch 26 step 101 loss 1.6577458381652832\n",
            "epoch 26 step 102 loss 1.6398839950561523\n",
            "epoch 26 step 103 loss 1.627907633781433\n",
            "epoch 26 step 104 loss 1.6431975364685059\n",
            "epoch 26 step 105 loss 1.6573717594146729\n",
            "epoch 26 step 106 loss 1.637238621711731\n",
            "epoch 26 step 107 loss 1.6396048069000244\n",
            "epoch 26 step 108 loss 1.6157046556472778\n",
            "epoch 26 step 109 loss 1.627379059791565\n",
            "epoch 26 step 110 loss 1.6212522983551025\n",
            "epoch 26 step 111 loss 1.6657614707946777\n",
            "epoch 26 step 112 loss 1.6315325498580933\n",
            "epoch 26 step 113 loss 1.6287715435028076\n",
            "epoch 26 step 114 loss 1.6155906915664673\n",
            "epoch 26 step 115 loss 1.647539734840393\n",
            "epoch 26 step 116 loss 1.6321558952331543\n",
            "epoch 26 step 117 loss 1.6492291688919067\n",
            "epoch 26 step 118 loss 1.64987313747406\n",
            "epoch 26 step 119 loss 1.6550564765930176\n",
            "epoch 26 step 120 loss 1.6343634128570557 test_accuracy 79.70000457763672 train_accuracy 85.15625\n",
            "epoch 26 step 121 loss 1.6393835544586182\n",
            "epoch 26 step 122 loss 1.6263840198516846\n",
            "epoch 26 step 123 loss 1.6350009441375732\n",
            "epoch 26 step 124 loss 1.65158212184906\n",
            "epoch 26 step 125 loss 1.645336627960205\n",
            "epoch 26 step 126 loss 1.6079001426696777\n",
            "epoch 26 step 127 loss 1.6201897859573364\n",
            "epoch 26 step 128 loss 1.6337692737579346\n",
            "epoch 26 step 129 loss 1.6716266870498657\n",
            "epoch 26 step 130 loss 1.6218860149383545\n",
            "epoch 26 step 131 loss 1.6440837383270264\n",
            "epoch 26 step 132 loss 1.6493028402328491\n",
            "epoch 26 step 133 loss 1.6258412599563599\n",
            "epoch 26 step 134 loss 1.6250488758087158\n",
            "epoch 26 step 135 loss 1.620896577835083\n",
            "epoch 26 step 136 loss 1.6384544372558594\n",
            "epoch 26 step 137 loss 1.6545932292938232\n",
            "epoch 26 step 138 loss 1.612966775894165\n",
            "epoch 26 step 139 loss 1.6567591428756714\n",
            "epoch 26 step 140 loss 1.6414223909378052 test_accuracy 79.20000457763672 train_accuracy 84.375\n",
            "epoch 26 step 141 loss 1.6541342735290527\n",
            "epoch 26 step 142 loss 1.6265326738357544\n",
            "epoch 26 step 143 loss 1.6762958765029907\n",
            "epoch 26 step 144 loss 1.6446806192398071\n",
            "epoch 26 step 145 loss 1.6421908140182495\n",
            "epoch 26 step 146 loss 1.6347386837005615\n",
            "epoch 26 step 147 loss 1.6314672231674194\n",
            "epoch 26 step 148 loss 1.6486629247665405\n",
            "epoch 26 step 149 loss 1.636340856552124\n",
            "epoch 26 step 150 loss 1.6499961614608765\n",
            "epoch 26 step 151 loss 1.6452499628067017\n",
            "epoch 26 step 152 loss 1.6538207530975342\n",
            "epoch 26 step 153 loss 1.6260194778442383\n",
            "epoch 26 step 154 loss 1.6310126781463623\n",
            "epoch 26 step 155 loss 1.662353515625\n",
            "epoch 26 step 156 loss 1.6379181146621704\n",
            "epoch 26 step 157 loss 1.6545968055725098\n",
            "epoch 26 step 158 loss 1.6474210023880005\n",
            "epoch 26 step 159 loss 1.6685075759887695\n",
            "epoch 26 step 160 loss 1.6709645986557007 test_accuracy 79.80000305175781 train_accuracy 85.15625\n",
            "epoch 26 step 161 loss 1.6444445848464966\n",
            "epoch 26 step 162 loss 1.6385047435760498\n",
            "epoch 26 step 163 loss 1.6688096523284912\n",
            "epoch 26 step 164 loss 1.6607877016067505\n",
            "epoch 26 step 165 loss 1.6596133708953857\n",
            "epoch 26 step 166 loss 1.6325466632843018\n",
            "epoch 26 step 167 loss 1.6134469509124756\n",
            "epoch 26 step 168 loss 1.63847815990448\n",
            "epoch 26 step 169 loss 1.6468971967697144\n",
            "epoch 26 step 170 loss 1.6667566299438477\n",
            "epoch 26 step 171 loss 1.6487221717834473\n",
            "epoch 26 step 172 loss 1.6391640901565552\n",
            "epoch 26 step 173 loss 1.6129510402679443\n",
            "epoch 26 step 174 loss 1.6551599502563477\n",
            "epoch 26 step 175 loss 1.6493492126464844\n",
            "epoch 26 step 176 loss 1.6626194715499878\n",
            "epoch 26 step 177 loss 1.6482797861099243\n",
            "epoch 26 step 178 loss 1.6328394412994385\n",
            "epoch 26 step 179 loss 1.630419373512268\n",
            "epoch 26 step 180 loss 1.6227482557296753 test_accuracy 82.20000457763672 train_accuracy 88.8671875\n",
            "epoch 26 step 181 loss 1.6560156345367432\n",
            "epoch 26 step 182 loss 1.6565089225769043\n",
            "epoch 26 step 183 loss 1.6717309951782227\n",
            "epoch 26 step 184 loss 1.6636569499969482\n",
            "epoch 26 step 185 loss 1.642152190208435\n",
            "epoch 26 step 186 loss 1.621995449066162\n",
            "epoch 26 step 187 loss 1.671868085861206\n",
            "epoch 26 step 188 loss 1.63486909866333\n",
            "epoch 26 step 189 loss 1.6089271306991577\n",
            "epoch 26 step 190 loss 1.6404322385787964\n",
            "epoch 26 step 191 loss 1.6471340656280518\n",
            "epoch 26 step 192 loss 1.647803783416748\n",
            "epoch 26 step 193 loss 1.6408573389053345\n",
            "epoch 26 step 194 loss 1.6207776069641113\n",
            "epoch 26 step 195 loss 1.6130863428115845\n",
            "epoch 27 step 0 loss 1.6533770561218262 test_accuracy 79.9000015258789 train_accuracy 88.671875\n",
            "epoch 27 step 1 loss 1.6381065845489502\n",
            "epoch 27 step 2 loss 1.6020305156707764\n",
            "epoch 27 step 3 loss 1.647615671157837\n",
            "epoch 27 step 4 loss 1.6263481378555298\n",
            "epoch 27 step 5 loss 1.6535005569458008\n",
            "epoch 27 step 6 loss 1.6163480281829834\n",
            "epoch 27 step 7 loss 1.6500184535980225\n",
            "epoch 27 step 8 loss 1.6644755601882935\n",
            "epoch 27 step 9 loss 1.6598005294799805\n",
            "epoch 27 step 10 loss 1.6380832195281982\n",
            "epoch 27 step 11 loss 1.6525746583938599\n",
            "epoch 27 step 12 loss 1.6238175630569458\n",
            "epoch 27 step 13 loss 1.6723556518554688\n",
            "epoch 27 step 14 loss 1.6379990577697754\n",
            "epoch 27 step 15 loss 1.6337236166000366\n",
            "epoch 27 step 16 loss 1.6546472311019897\n",
            "epoch 27 step 17 loss 1.6724172830581665\n",
            "epoch 27 step 18 loss 1.6558908224105835\n",
            "epoch 27 step 19 loss 1.6218270063400269\n",
            "epoch 27 step 20 loss 1.645861029624939 test_accuracy 78.4000015258789 train_accuracy 87.6953125\n",
            "epoch 27 step 21 loss 1.609935998916626\n",
            "epoch 27 step 22 loss 1.6397680044174194\n",
            "epoch 27 step 23 loss 1.645796298980713\n",
            "epoch 27 step 24 loss 1.6514531373977661\n",
            "epoch 27 step 25 loss 1.6485573053359985\n",
            "epoch 27 step 26 loss 1.6594041585922241\n",
            "epoch 27 step 27 loss 1.6615662574768066\n",
            "epoch 27 step 28 loss 1.62371826171875\n",
            "epoch 27 step 29 loss 1.6159777641296387\n",
            "epoch 27 step 30 loss 1.6382877826690674\n",
            "epoch 27 step 31 loss 1.655731201171875\n",
            "epoch 27 step 32 loss 1.6690119504928589\n",
            "epoch 27 step 33 loss 1.6671048402786255\n",
            "epoch 27 step 34 loss 1.6535494327545166\n",
            "epoch 27 step 35 loss 1.6263699531555176\n",
            "epoch 27 step 36 loss 1.666604995727539\n",
            "epoch 27 step 37 loss 1.6694526672363281\n",
            "epoch 27 step 38 loss 1.6820889711380005\n",
            "epoch 27 step 39 loss 1.6358180046081543\n",
            "epoch 27 step 40 loss 1.6398277282714844 test_accuracy 80.30000305175781 train_accuracy 84.765625\n",
            "epoch 27 step 41 loss 1.6282799243927002\n",
            "epoch 27 step 42 loss 1.6175965070724487\n",
            "epoch 27 step 43 loss 1.6548930406570435\n",
            "epoch 27 step 44 loss 1.6452480554580688\n",
            "epoch 27 step 45 loss 1.6637681722640991\n",
            "epoch 27 step 46 loss 1.6784669160842896\n",
            "epoch 27 step 47 loss 1.6473720073699951\n",
            "epoch 27 step 48 loss 1.6574753522872925\n",
            "epoch 27 step 49 loss 1.6360044479370117\n",
            "epoch 27 step 50 loss 1.6132148504257202\n",
            "epoch 27 step 51 loss 1.633882761001587\n",
            "epoch 27 step 52 loss 1.6633893251419067\n",
            "epoch 27 step 53 loss 1.6254498958587646\n",
            "epoch 27 step 54 loss 1.642716407775879\n",
            "epoch 27 step 55 loss 1.6229591369628906\n",
            "epoch 27 step 56 loss 1.649900197982788\n",
            "epoch 27 step 57 loss 1.6171075105667114\n",
            "epoch 27 step 58 loss 1.6232300996780396\n",
            "epoch 27 step 59 loss 1.6464228630065918\n",
            "epoch 27 step 60 loss 1.6339744329452515 test_accuracy 82.30000305175781 train_accuracy 85.9375\n",
            "epoch 27 step 61 loss 1.6690196990966797\n",
            "epoch 27 step 62 loss 1.6572339534759521\n",
            "epoch 27 step 63 loss 1.6504875421524048\n",
            "epoch 27 step 64 loss 1.6625922918319702\n",
            "epoch 27 step 65 loss 1.6158980131149292\n",
            "epoch 27 step 66 loss 1.6388425827026367\n",
            "epoch 27 step 67 loss 1.6406093835830688\n",
            "epoch 27 step 68 loss 1.6314160823822021\n",
            "epoch 27 step 69 loss 1.6236529350280762\n",
            "epoch 27 step 70 loss 1.668716311454773\n",
            "epoch 27 step 71 loss 1.6342616081237793\n",
            "epoch 27 step 72 loss 1.650756597518921\n",
            "epoch 27 step 73 loss 1.6383486986160278\n",
            "epoch 27 step 74 loss 1.679279088973999\n",
            "epoch 27 step 75 loss 1.6487969160079956\n",
            "epoch 27 step 76 loss 1.6419321298599243\n",
            "epoch 27 step 77 loss 1.6507658958435059\n",
            "epoch 27 step 78 loss 1.6184675693511963\n",
            "epoch 27 step 79 loss 1.6293748617172241\n",
            "epoch 27 step 80 loss 1.6414345502853394 test_accuracy 81.10000610351562 train_accuracy 85.9375\n",
            "epoch 27 step 81 loss 1.6535701751708984\n",
            "epoch 27 step 82 loss 1.625413179397583\n",
            "epoch 27 step 83 loss 1.6516196727752686\n",
            "epoch 27 step 84 loss 1.6616629362106323\n",
            "epoch 27 step 85 loss 1.6166802644729614\n",
            "epoch 27 step 86 loss 1.6436911821365356\n",
            "epoch 27 step 87 loss 1.6418594121932983\n",
            "epoch 27 step 88 loss 1.6474897861480713\n",
            "epoch 27 step 89 loss 1.6195347309112549\n",
            "epoch 27 step 90 loss 1.6263272762298584\n",
            "epoch 27 step 91 loss 1.6416137218475342\n",
            "epoch 27 step 92 loss 1.6360868215560913\n",
            "epoch 27 step 93 loss 1.660127878189087\n",
            "epoch 27 step 94 loss 1.656144142150879\n",
            "epoch 27 step 95 loss 1.6831283569335938\n",
            "epoch 27 step 96 loss 1.6553038358688354\n",
            "epoch 27 step 97 loss 1.6207038164138794\n",
            "epoch 27 step 98 loss 1.614949107170105\n",
            "epoch 27 step 99 loss 1.6343307495117188\n",
            "epoch 27 step 100 loss 1.6599317789077759 test_accuracy 79.5 train_accuracy 85.9375\n",
            "epoch 27 step 101 loss 1.6585347652435303\n",
            "epoch 27 step 102 loss 1.6762913465499878\n",
            "epoch 27 step 103 loss 1.6234228610992432\n",
            "epoch 27 step 104 loss 1.6074389219284058\n",
            "epoch 27 step 105 loss 1.625008463859558\n",
            "epoch 27 step 106 loss 1.6457396745681763\n",
            "epoch 27 step 107 loss 1.6420234441757202\n",
            "epoch 27 step 108 loss 1.6549217700958252\n",
            "epoch 27 step 109 loss 1.6595042943954468\n",
            "epoch 27 step 110 loss 1.6492191553115845\n",
            "epoch 27 step 111 loss 1.6277570724487305\n",
            "epoch 27 step 112 loss 1.6351935863494873\n",
            "epoch 27 step 113 loss 1.6348437070846558\n",
            "epoch 27 step 114 loss 1.6318864822387695\n",
            "epoch 27 step 115 loss 1.6163477897644043\n",
            "epoch 27 step 116 loss 1.6422923803329468\n",
            "epoch 27 step 117 loss 1.640650987625122\n",
            "epoch 27 step 118 loss 1.632016658782959\n",
            "epoch 27 step 119 loss 1.668581247329712\n",
            "epoch 27 step 120 loss 1.6362860202789307 test_accuracy 81.10000610351562 train_accuracy 84.5703125\n",
            "epoch 27 step 121 loss 1.6352206468582153\n",
            "epoch 27 step 122 loss 1.6455188989639282\n",
            "epoch 27 step 123 loss 1.6694750785827637\n",
            "epoch 27 step 124 loss 1.6384295225143433\n",
            "epoch 27 step 125 loss 1.6415321826934814\n",
            "epoch 27 step 126 loss 1.6359174251556396\n",
            "epoch 27 step 127 loss 1.6157548427581787\n",
            "epoch 27 step 128 loss 1.677017092704773\n",
            "epoch 27 step 129 loss 1.6690255403518677\n",
            "epoch 27 step 130 loss 1.6179983615875244\n",
            "epoch 27 step 131 loss 1.628701090812683\n",
            "epoch 27 step 132 loss 1.6557611227035522\n",
            "epoch 27 step 133 loss 1.6660789251327515\n",
            "epoch 27 step 134 loss 1.6328457593917847\n",
            "epoch 27 step 135 loss 1.641313910484314\n",
            "epoch 27 step 136 loss 1.6487658023834229\n",
            "epoch 27 step 137 loss 1.67060387134552\n",
            "epoch 27 step 138 loss 1.6645482778549194\n",
            "epoch 27 step 139 loss 1.6309990882873535\n",
            "epoch 27 step 140 loss 1.6304750442504883 test_accuracy 82.20000457763672 train_accuracy 85.7421875\n",
            "epoch 27 step 141 loss 1.6486202478408813\n",
            "epoch 27 step 142 loss 1.6105564832687378\n",
            "epoch 27 step 143 loss 1.6616666316986084\n",
            "epoch 27 step 144 loss 1.6421194076538086\n",
            "epoch 27 step 145 loss 1.6556341648101807\n",
            "epoch 27 step 146 loss 1.6847569942474365\n",
            "epoch 27 step 147 loss 1.6696903705596924\n",
            "epoch 27 step 148 loss 1.6421116590499878\n",
            "epoch 27 step 149 loss 1.6550668478012085\n",
            "epoch 27 step 150 loss 1.6474177837371826\n",
            "epoch 27 step 151 loss 1.6158051490783691\n",
            "epoch 27 step 152 loss 1.61700439453125\n",
            "epoch 27 step 153 loss 1.6413614749908447\n",
            "epoch 27 step 154 loss 1.617631196975708\n",
            "epoch 27 step 155 loss 1.6569854021072388\n",
            "epoch 27 step 156 loss 1.6460789442062378\n",
            "epoch 27 step 157 loss 1.6666563749313354\n",
            "epoch 27 step 158 loss 1.588136076927185\n",
            "epoch 27 step 159 loss 1.6481809616088867\n",
            "epoch 27 step 160 loss 1.6131024360656738 test_accuracy 80.70000457763672 train_accuracy 83.984375\n",
            "epoch 27 step 161 loss 1.651218295097351\n",
            "epoch 27 step 162 loss 1.6472065448760986\n",
            "epoch 27 step 163 loss 1.6609792709350586\n",
            "epoch 27 step 164 loss 1.620051622390747\n",
            "epoch 27 step 165 loss 1.640821099281311\n",
            "epoch 27 step 166 loss 1.6475058794021606\n",
            "epoch 27 step 167 loss 1.6491820812225342\n",
            "epoch 27 step 168 loss 1.612567663192749\n",
            "epoch 27 step 169 loss 1.6390960216522217\n",
            "epoch 27 step 170 loss 1.6407067775726318\n",
            "epoch 27 step 171 loss 1.6489477157592773\n",
            "epoch 27 step 172 loss 1.6219110488891602\n",
            "epoch 27 step 173 loss 1.6535418033599854\n",
            "epoch 27 step 174 loss 1.6290920972824097\n",
            "epoch 27 step 175 loss 1.645613431930542\n",
            "epoch 27 step 176 loss 1.640872597694397\n",
            "epoch 27 step 177 loss 1.6448527574539185\n",
            "epoch 27 step 178 loss 1.6600565910339355\n",
            "epoch 27 step 179 loss 1.6328344345092773\n",
            "epoch 27 step 180 loss 1.6449594497680664 test_accuracy 78.70000457763672 train_accuracy 85.9375\n",
            "epoch 27 step 181 loss 1.6477491855621338\n",
            "epoch 27 step 182 loss 1.6419097185134888\n",
            "epoch 27 step 183 loss 1.6497416496276855\n",
            "epoch 27 step 184 loss 1.6400856971740723\n",
            "epoch 27 step 185 loss 1.6343849897384644\n",
            "epoch 27 step 186 loss 1.6576812267303467\n",
            "epoch 27 step 187 loss 1.6459333896636963\n",
            "epoch 27 step 188 loss 1.6266674995422363\n",
            "epoch 27 step 189 loss 1.6426005363464355\n",
            "epoch 27 step 190 loss 1.620507836341858\n",
            "epoch 27 step 191 loss 1.6062960624694824\n",
            "epoch 27 step 192 loss 1.6319012641906738\n",
            "epoch 27 step 193 loss 1.6533907651901245\n",
            "epoch 27 step 194 loss 1.6520248651504517\n",
            "epoch 27 step 195 loss 1.6194446086883545\n",
            "epoch 28 step 0 loss 1.6303409337997437 test_accuracy 79.70000457763672 train_accuracy 86.328125\n",
            "epoch 28 step 1 loss 1.668357491493225\n",
            "epoch 28 step 2 loss 1.6289716958999634\n",
            "epoch 28 step 3 loss 1.6069697141647339\n",
            "epoch 28 step 4 loss 1.6313475370407104\n",
            "epoch 28 step 5 loss 1.6263943910598755\n",
            "epoch 28 step 6 loss 1.6124085187911987\n",
            "epoch 28 step 7 loss 1.6584203243255615\n",
            "epoch 28 step 8 loss 1.6315641403198242\n",
            "epoch 28 step 9 loss 1.6705490350723267\n",
            "epoch 28 step 10 loss 1.6674816608428955\n",
            "epoch 28 step 11 loss 1.6444385051727295\n",
            "epoch 28 step 12 loss 1.6498843431472778\n",
            "epoch 28 step 13 loss 1.6765892505645752\n",
            "epoch 28 step 14 loss 1.6590864658355713\n",
            "epoch 28 step 15 loss 1.636405110359192\n",
            "epoch 28 step 16 loss 1.6581308841705322\n",
            "epoch 28 step 17 loss 1.6402455568313599\n",
            "epoch 28 step 18 loss 1.6181631088256836\n",
            "epoch 28 step 19 loss 1.610144853591919\n",
            "epoch 28 step 20 loss 1.635423183441162 test_accuracy 78.0 train_accuracy 83.203125\n",
            "epoch 28 step 21 loss 1.630361557006836\n",
            "epoch 28 step 22 loss 1.6334563493728638\n",
            "epoch 28 step 23 loss 1.6407908201217651\n",
            "epoch 28 step 24 loss 1.659205675125122\n",
            "epoch 28 step 25 loss 1.6556159257888794\n",
            "epoch 28 step 26 loss 1.6227887868881226\n",
            "epoch 28 step 27 loss 1.6496058702468872\n",
            "epoch 28 step 28 loss 1.6394703388214111\n",
            "epoch 28 step 29 loss 1.6413506269454956\n",
            "epoch 28 step 30 loss 1.6494053602218628\n",
            "epoch 28 step 31 loss 1.625590205192566\n",
            "epoch 28 step 32 loss 1.6483514308929443\n",
            "epoch 28 step 33 loss 1.653151512145996\n",
            "epoch 28 step 34 loss 1.63139009475708\n",
            "epoch 28 step 35 loss 1.6557493209838867\n",
            "epoch 28 step 36 loss 1.6538909673690796\n",
            "epoch 28 step 37 loss 1.6526672840118408\n",
            "epoch 28 step 38 loss 1.6273798942565918\n",
            "epoch 28 step 39 loss 1.619209885597229\n",
            "epoch 28 step 40 loss 1.614802360534668 test_accuracy 81.70000457763672 train_accuracy 86.5234375\n",
            "epoch 28 step 41 loss 1.6512336730957031\n",
            "epoch 28 step 42 loss 1.64197838306427\n",
            "epoch 28 step 43 loss 1.6262937784194946\n",
            "epoch 28 step 44 loss 1.6288442611694336\n",
            "epoch 28 step 45 loss 1.6206717491149902\n",
            "epoch 28 step 46 loss 1.6337628364562988\n",
            "epoch 28 step 47 loss 1.6338231563568115\n",
            "epoch 28 step 48 loss 1.6614575386047363\n",
            "epoch 28 step 49 loss 1.6467698812484741\n",
            "epoch 28 step 50 loss 1.6368441581726074\n",
            "epoch 28 step 51 loss 1.6472389698028564\n",
            "epoch 28 step 52 loss 1.6367876529693604\n",
            "epoch 28 step 53 loss 1.636370062828064\n",
            "epoch 28 step 54 loss 1.641091227531433\n",
            "epoch 28 step 55 loss 1.6479696035385132\n",
            "epoch 28 step 56 loss 1.6348164081573486\n",
            "epoch 28 step 57 loss 1.645950436592102\n",
            "epoch 28 step 58 loss 1.6516283750534058\n",
            "epoch 28 step 59 loss 1.639412760734558\n",
            "epoch 28 step 60 loss 1.6452125310897827 test_accuracy 80.0 train_accuracy 85.7421875\n",
            "epoch 28 step 61 loss 1.6350317001342773\n",
            "epoch 28 step 62 loss 1.637224793434143\n",
            "epoch 28 step 63 loss 1.6448613405227661\n",
            "epoch 28 step 64 loss 1.6142168045043945\n",
            "epoch 28 step 65 loss 1.6106648445129395\n",
            "epoch 28 step 66 loss 1.6382083892822266\n",
            "epoch 28 step 67 loss 1.6517914533615112\n",
            "epoch 28 step 68 loss 1.6607208251953125\n",
            "epoch 28 step 69 loss 1.636722207069397\n",
            "epoch 28 step 70 loss 1.6437257528305054\n",
            "epoch 28 step 71 loss 1.661554217338562\n",
            "epoch 28 step 72 loss 1.6678701639175415\n",
            "epoch 28 step 73 loss 1.6238497495651245\n",
            "epoch 28 step 74 loss 1.667046070098877\n",
            "epoch 28 step 75 loss 1.6739091873168945\n",
            "epoch 28 step 76 loss 1.634211778640747\n",
            "epoch 28 step 77 loss 1.6755443811416626\n",
            "epoch 28 step 78 loss 1.6442559957504272\n",
            "epoch 28 step 79 loss 1.6460119485855103\n",
            "epoch 28 step 80 loss 1.645004391670227 test_accuracy 81.60000610351562 train_accuracy 88.0859375\n",
            "epoch 28 step 81 loss 1.642711877822876\n",
            "epoch 28 step 82 loss 1.6353167295455933\n",
            "epoch 28 step 83 loss 1.6307843923568726\n",
            "epoch 28 step 84 loss 1.6122393608093262\n",
            "epoch 28 step 85 loss 1.638961911201477\n",
            "epoch 28 step 86 loss 1.6345728635787964\n",
            "epoch 28 step 87 loss 1.6282169818878174\n",
            "epoch 28 step 88 loss 1.6433148384094238\n",
            "epoch 28 step 89 loss 1.6157783269882202\n",
            "epoch 28 step 90 loss 1.666343092918396\n",
            "epoch 28 step 91 loss 1.639586091041565\n",
            "epoch 28 step 92 loss 1.6176223754882812\n",
            "epoch 28 step 93 loss 1.6342434883117676\n",
            "epoch 28 step 94 loss 1.6662687063217163\n",
            "epoch 28 step 95 loss 1.6634342670440674\n",
            "epoch 28 step 96 loss 1.6277141571044922\n",
            "epoch 28 step 97 loss 1.6596754789352417\n",
            "epoch 28 step 98 loss 1.6238423585891724\n",
            "epoch 28 step 99 loss 1.6497721672058105\n",
            "epoch 28 step 100 loss 1.6443012952804565 test_accuracy 79.5 train_accuracy 85.7421875\n",
            "epoch 28 step 101 loss 1.6380232572555542\n",
            "epoch 28 step 102 loss 1.6563845872879028\n",
            "epoch 28 step 103 loss 1.6567341089248657\n",
            "epoch 28 step 104 loss 1.6264969110488892\n",
            "epoch 28 step 105 loss 1.6252164840698242\n",
            "epoch 28 step 106 loss 1.6407986879348755\n",
            "epoch 28 step 107 loss 1.650894284248352\n",
            "epoch 28 step 108 loss 1.652493953704834\n",
            "epoch 28 step 109 loss 1.661178708076477\n",
            "epoch 28 step 110 loss 1.6410945653915405\n",
            "epoch 28 step 111 loss 1.6598461866378784\n",
            "epoch 28 step 112 loss 1.6286468505859375\n",
            "epoch 28 step 113 loss 1.6642214059829712\n",
            "epoch 28 step 114 loss 1.6367847919464111\n",
            "epoch 28 step 115 loss 1.6537548303604126\n",
            "epoch 28 step 116 loss 1.62996244430542\n",
            "epoch 28 step 117 loss 1.6264206171035767\n",
            "epoch 28 step 118 loss 1.614332914352417\n",
            "epoch 28 step 119 loss 1.6228784322738647\n",
            "epoch 28 step 120 loss 1.6220126152038574 test_accuracy 82.20000457763672 train_accuracy 88.28125\n",
            "epoch 28 step 121 loss 1.6633036136627197\n",
            "epoch 28 step 122 loss 1.6782416105270386\n",
            "epoch 28 step 123 loss 1.6546056270599365\n",
            "epoch 28 step 124 loss 1.6155126094818115\n",
            "epoch 28 step 125 loss 1.6495124101638794\n",
            "epoch 28 step 126 loss 1.6344890594482422\n",
            "epoch 28 step 127 loss 1.6305712461471558\n",
            "epoch 28 step 128 loss 1.6433866024017334\n",
            "epoch 28 step 129 loss 1.6283396482467651\n",
            "epoch 28 step 130 loss 1.634143352508545\n",
            "epoch 28 step 131 loss 1.6349865198135376\n",
            "epoch 28 step 132 loss 1.6812186241149902\n",
            "epoch 28 step 133 loss 1.625503420829773\n",
            "epoch 28 step 134 loss 1.6505762338638306\n",
            "epoch 28 step 135 loss 1.6400182247161865\n",
            "epoch 28 step 136 loss 1.6317826509475708\n",
            "epoch 28 step 137 loss 1.6406910419464111\n",
            "epoch 28 step 138 loss 1.6280452013015747\n",
            "epoch 28 step 139 loss 1.61525297164917\n",
            "epoch 28 step 140 loss 1.6686803102493286 test_accuracy 80.60000610351562 train_accuracy 85.9375\n",
            "epoch 28 step 141 loss 1.6424484252929688\n",
            "epoch 28 step 142 loss 1.6374609470367432\n",
            "epoch 28 step 143 loss 1.6498936414718628\n",
            "epoch 28 step 144 loss 1.6254194974899292\n",
            "epoch 28 step 145 loss 1.634629726409912\n",
            "epoch 28 step 146 loss 1.6612961292266846\n",
            "epoch 28 step 147 loss 1.6579718589782715\n",
            "epoch 28 step 148 loss 1.621462106704712\n",
            "epoch 28 step 149 loss 1.614337682723999\n",
            "epoch 28 step 150 loss 1.6283947229385376\n",
            "epoch 28 step 151 loss 1.6469552516937256\n",
            "epoch 28 step 152 loss 1.6395964622497559\n",
            "epoch 28 step 153 loss 1.6519261598587036\n",
            "epoch 28 step 154 loss 1.6422582864761353\n",
            "epoch 28 step 155 loss 1.6524760723114014\n",
            "epoch 28 step 156 loss 1.6306794881820679\n",
            "epoch 28 step 157 loss 1.6170810461044312\n",
            "epoch 28 step 158 loss 1.6442596912384033\n",
            "epoch 28 step 159 loss 1.661198377609253\n",
            "epoch 28 step 160 loss 1.6310863494873047 test_accuracy 82.20000457763672 train_accuracy 85.15625\n",
            "epoch 28 step 161 loss 1.6624093055725098\n",
            "epoch 28 step 162 loss 1.6356829404830933\n",
            "epoch 28 step 163 loss 1.6380457878112793\n",
            "epoch 28 step 164 loss 1.6395957469940186\n",
            "epoch 28 step 165 loss 1.6268333196640015\n",
            "epoch 28 step 166 loss 1.6230411529541016\n",
            "epoch 28 step 167 loss 1.6525301933288574\n",
            "epoch 28 step 168 loss 1.6563295125961304\n",
            "epoch 28 step 169 loss 1.6518511772155762\n",
            "epoch 28 step 170 loss 1.6335999965667725\n",
            "epoch 28 step 171 loss 1.634117603302002\n",
            "epoch 28 step 172 loss 1.6223219633102417\n",
            "epoch 28 step 173 loss 1.6322129964828491\n",
            "epoch 28 step 174 loss 1.641100287437439\n",
            "epoch 28 step 175 loss 1.631587028503418\n",
            "epoch 28 step 176 loss 1.6403896808624268\n",
            "epoch 28 step 177 loss 1.648200511932373\n",
            "epoch 28 step 178 loss 1.6259695291519165\n",
            "epoch 28 step 179 loss 1.6510828733444214\n",
            "epoch 28 step 180 loss 1.6515471935272217 test_accuracy 79.4000015258789 train_accuracy 87.3046875\n",
            "epoch 28 step 181 loss 1.6529325246810913\n",
            "epoch 28 step 182 loss 1.6424968242645264\n",
            "epoch 28 step 183 loss 1.6399073600769043\n",
            "epoch 28 step 184 loss 1.6565680503845215\n",
            "epoch 28 step 185 loss 1.6377555131912231\n",
            "epoch 28 step 186 loss 1.652279019355774\n",
            "epoch 28 step 187 loss 1.6279125213623047\n",
            "epoch 28 step 188 loss 1.6572355031967163\n",
            "epoch 28 step 189 loss 1.6266493797302246\n",
            "epoch 28 step 190 loss 1.6390979290008545\n",
            "epoch 28 step 191 loss 1.6193726062774658\n",
            "epoch 28 step 192 loss 1.6376341581344604\n",
            "epoch 28 step 193 loss 1.6463593244552612\n",
            "epoch 28 step 194 loss 1.6153470277786255\n",
            "epoch 28 step 195 loss 1.6072677373886108\n",
            "epoch 29 step 0 loss 1.6472076177597046 test_accuracy 81.20000457763672 train_accuracy 85.546875\n",
            "epoch 29 step 1 loss 1.6390504837036133\n",
            "epoch 29 step 2 loss 1.6552473306655884\n",
            "epoch 29 step 3 loss 1.653382420539856\n",
            "epoch 29 step 4 loss 1.6461695432662964\n",
            "epoch 29 step 5 loss 1.6062366962432861\n",
            "epoch 29 step 6 loss 1.6316730976104736\n",
            "epoch 29 step 7 loss 1.6255956888198853\n",
            "epoch 29 step 8 loss 1.6699482202529907\n",
            "epoch 29 step 9 loss 1.6325745582580566\n",
            "epoch 29 step 10 loss 1.6505621671676636\n",
            "epoch 29 step 11 loss 1.6109626293182373\n",
            "epoch 29 step 12 loss 1.6441985368728638\n",
            "epoch 29 step 13 loss 1.6486120223999023\n",
            "epoch 29 step 14 loss 1.6420981884002686\n",
            "epoch 29 step 15 loss 1.6425660848617554\n",
            "epoch 29 step 16 loss 1.6494091749191284\n",
            "epoch 29 step 17 loss 1.6416748762130737\n",
            "epoch 29 step 18 loss 1.6505835056304932\n",
            "epoch 29 step 19 loss 1.6370322704315186\n",
            "epoch 29 step 20 loss 1.6677759885787964 test_accuracy 80.20000457763672 train_accuracy 88.28125\n",
            "epoch 29 step 21 loss 1.6790846586227417\n",
            "epoch 29 step 22 loss 1.6610697507858276\n",
            "epoch 29 step 23 loss 1.6196413040161133\n",
            "epoch 29 step 24 loss 1.651710867881775\n",
            "epoch 29 step 25 loss 1.6414365768432617\n",
            "epoch 29 step 26 loss 1.6483557224273682\n",
            "epoch 29 step 27 loss 1.6310111284255981\n",
            "epoch 29 step 28 loss 1.6412382125854492\n",
            "epoch 29 step 29 loss 1.6362204551696777\n",
            "epoch 29 step 30 loss 1.627018928527832\n",
            "epoch 29 step 31 loss 1.6299488544464111\n",
            "epoch 29 step 32 loss 1.619025468826294\n",
            "epoch 29 step 33 loss 1.6678144931793213\n",
            "epoch 29 step 34 loss 1.6329622268676758\n",
            "epoch 29 step 35 loss 1.6229631900787354\n",
            "epoch 29 step 36 loss 1.6135034561157227\n",
            "epoch 29 step 37 loss 1.632129430770874\n",
            "epoch 29 step 38 loss 1.6313496828079224\n",
            "epoch 29 step 39 loss 1.6456713676452637\n",
            "epoch 29 step 40 loss 1.6510655879974365 test_accuracy 81.10000610351562 train_accuracy 85.9375\n",
            "epoch 29 step 41 loss 1.6109440326690674\n",
            "epoch 29 step 42 loss 1.6147958040237427\n",
            "epoch 29 step 43 loss 1.6580785512924194\n",
            "epoch 29 step 44 loss 1.6361093521118164\n",
            "epoch 29 step 45 loss 1.6186274290084839\n",
            "epoch 29 step 46 loss 1.6354718208312988\n",
            "epoch 29 step 47 loss 1.625442624092102\n",
            "epoch 29 step 48 loss 1.6104609966278076\n",
            "epoch 29 step 49 loss 1.6442289352416992\n",
            "epoch 29 step 50 loss 1.6208622455596924\n",
            "epoch 29 step 51 loss 1.655887246131897\n",
            "epoch 29 step 52 loss 1.6324138641357422\n",
            "epoch 29 step 53 loss 1.6475645303726196\n",
            "epoch 29 step 54 loss 1.6293439865112305\n",
            "epoch 29 step 55 loss 1.6257054805755615\n",
            "epoch 29 step 56 loss 1.6355146169662476\n",
            "epoch 29 step 57 loss 1.623509407043457\n",
            "epoch 29 step 58 loss 1.6383990049362183\n",
            "epoch 29 step 59 loss 1.6332261562347412\n",
            "epoch 29 step 60 loss 1.6454715728759766 test_accuracy 80.60000610351562 train_accuracy 86.71875\n",
            "epoch 29 step 61 loss 1.636908769607544\n",
            "epoch 29 step 62 loss 1.6407405138015747\n",
            "epoch 29 step 63 loss 1.648361325263977\n",
            "epoch 29 step 64 loss 1.636853814125061\n",
            "epoch 29 step 65 loss 1.6501376628875732\n",
            "epoch 29 step 66 loss 1.64130437374115\n",
            "epoch 29 step 67 loss 1.6680041551589966\n",
            "epoch 29 step 68 loss 1.643537998199463\n",
            "epoch 29 step 69 loss 1.6169042587280273\n",
            "epoch 29 step 70 loss 1.6087568998336792\n",
            "epoch 29 step 71 loss 1.6557884216308594\n",
            "epoch 29 step 72 loss 1.6445047855377197\n",
            "epoch 29 step 73 loss 1.6586649417877197\n",
            "epoch 29 step 74 loss 1.6334859132766724\n",
            "epoch 29 step 75 loss 1.6304126977920532\n",
            "epoch 29 step 76 loss 1.6562690734863281\n",
            "epoch 29 step 77 loss 1.6467373371124268\n",
            "epoch 29 step 78 loss 1.6545313596725464\n",
            "epoch 29 step 79 loss 1.6540673971176147\n",
            "epoch 29 step 80 loss 1.6366554498672485 test_accuracy 81.60000610351562 train_accuracy 86.9140625\n",
            "epoch 29 step 81 loss 1.6599245071411133\n",
            "epoch 29 step 82 loss 1.6453399658203125\n",
            "epoch 29 step 83 loss 1.6323964595794678\n",
            "epoch 29 step 84 loss 1.6307505369186401\n",
            "epoch 29 step 85 loss 1.6239677667617798\n",
            "epoch 29 step 86 loss 1.6716617345809937\n",
            "epoch 29 step 87 loss 1.6398305892944336\n",
            "epoch 29 step 88 loss 1.6387710571289062\n",
            "epoch 29 step 89 loss 1.622673511505127\n",
            "epoch 29 step 90 loss 1.6423473358154297\n",
            "epoch 29 step 91 loss 1.6236765384674072\n",
            "epoch 29 step 92 loss 1.6160979270935059\n",
            "epoch 29 step 93 loss 1.6320350170135498\n",
            "epoch 29 step 94 loss 1.6332647800445557\n",
            "epoch 29 step 95 loss 1.6369106769561768\n",
            "epoch 29 step 96 loss 1.6348826885223389\n",
            "epoch 29 step 97 loss 1.6295435428619385\n",
            "epoch 29 step 98 loss 1.6366816759109497\n",
            "epoch 29 step 99 loss 1.618988037109375\n",
            "epoch 29 step 100 loss 1.631535530090332 test_accuracy 82.70000457763672 train_accuracy 85.9375\n",
            "epoch 29 step 101 loss 1.6525143384933472\n",
            "epoch 29 step 102 loss 1.6207455396652222\n",
            "epoch 29 step 103 loss 1.6343419551849365\n",
            "epoch 29 step 104 loss 1.652791142463684\n",
            "epoch 29 step 105 loss 1.6639626026153564\n",
            "epoch 29 step 106 loss 1.6344040632247925\n",
            "epoch 29 step 107 loss 1.6429952383041382\n",
            "epoch 29 step 108 loss 1.6365240812301636\n",
            "epoch 29 step 109 loss 1.6311523914337158\n",
            "epoch 29 step 110 loss 1.6385648250579834\n",
            "epoch 29 step 111 loss 1.6488807201385498\n",
            "epoch 29 step 112 loss 1.6534240245819092\n",
            "epoch 29 step 113 loss 1.642845630645752\n",
            "epoch 29 step 114 loss 1.6603209972381592\n",
            "epoch 29 step 115 loss 1.6154826879501343\n",
            "epoch 29 step 116 loss 1.6381250619888306\n",
            "epoch 29 step 117 loss 1.6183524131774902\n",
            "epoch 29 step 118 loss 1.6290959119796753\n",
            "epoch 29 step 119 loss 1.6448994874954224\n",
            "epoch 29 step 120 loss 1.645993709564209 test_accuracy 82.00000762939453 train_accuracy 87.6953125\n",
            "epoch 29 step 121 loss 1.6148937940597534\n",
            "epoch 29 step 122 loss 1.6697758436203003\n",
            "epoch 29 step 123 loss 1.649200677871704\n",
            "epoch 29 step 124 loss 1.6224647760391235\n",
            "epoch 29 step 125 loss 1.6621427536010742\n",
            "epoch 29 step 126 loss 1.6231586933135986\n",
            "epoch 29 step 127 loss 1.616620659828186\n",
            "epoch 29 step 128 loss 1.6537797451019287\n",
            "epoch 29 step 129 loss 1.6752619743347168\n",
            "epoch 29 step 130 loss 1.643647313117981\n",
            "epoch 29 step 131 loss 1.6375030279159546\n",
            "epoch 29 step 132 loss 1.6176402568817139\n",
            "epoch 29 step 133 loss 1.6543729305267334\n",
            "epoch 29 step 134 loss 1.6366921663284302\n",
            "epoch 29 step 135 loss 1.6350964307785034\n",
            "epoch 29 step 136 loss 1.6293171644210815\n",
            "epoch 29 step 137 loss 1.6511669158935547\n",
            "epoch 29 step 138 loss 1.6098506450653076\n",
            "epoch 29 step 139 loss 1.645093321800232\n",
            "epoch 29 step 140 loss 1.6218370199203491 test_accuracy 82.60000610351562 train_accuracy 88.8671875\n",
            "epoch 29 step 141 loss 1.6495333909988403\n",
            "epoch 29 step 142 loss 1.6338467597961426\n",
            "epoch 29 step 143 loss 1.6185023784637451\n",
            "epoch 29 step 144 loss 1.6488566398620605\n",
            "epoch 29 step 145 loss 1.652590036392212\n",
            "epoch 29 step 146 loss 1.610097050666809\n",
            "epoch 29 step 147 loss 1.6484150886535645\n",
            "epoch 29 step 148 loss 1.6363614797592163\n",
            "epoch 29 step 149 loss 1.641158938407898\n",
            "epoch 29 step 150 loss 1.6623866558074951\n",
            "epoch 29 step 151 loss 1.6462664604187012\n",
            "epoch 29 step 152 loss 1.6394624710083008\n",
            "epoch 29 step 153 loss 1.6289680004119873\n",
            "epoch 29 step 154 loss 1.6493775844573975\n",
            "epoch 29 step 155 loss 1.648465871810913\n",
            "epoch 29 step 156 loss 1.655443549156189\n",
            "epoch 29 step 157 loss 1.6170510053634644\n",
            "epoch 29 step 158 loss 1.6746931076049805\n",
            "epoch 29 step 159 loss 1.6519453525543213\n",
            "epoch 29 step 160 loss 1.6612251996994019 test_accuracy 80.4000015258789 train_accuracy 87.6953125\n",
            "epoch 29 step 161 loss 1.6605398654937744\n",
            "epoch 29 step 162 loss 1.6417715549468994\n",
            "epoch 29 step 163 loss 1.6410391330718994\n",
            "epoch 29 step 164 loss 1.6324468851089478\n",
            "epoch 29 step 165 loss 1.6313931941986084\n",
            "epoch 29 step 166 loss 1.6395658254623413\n",
            "epoch 29 step 167 loss 1.6500705480575562\n",
            "epoch 29 step 168 loss 1.6494911909103394\n",
            "epoch 29 step 169 loss 1.6105766296386719\n",
            "epoch 29 step 170 loss 1.6404922008514404\n",
            "epoch 29 step 171 loss 1.6409633159637451\n",
            "epoch 29 step 172 loss 1.6288514137268066\n",
            "epoch 29 step 173 loss 1.6818559169769287\n",
            "epoch 29 step 174 loss 1.6261075735092163\n",
            "epoch 29 step 175 loss 1.6323624849319458\n",
            "epoch 29 step 176 loss 1.634206771850586\n",
            "epoch 29 step 177 loss 1.6447153091430664\n",
            "epoch 29 step 178 loss 1.6350177526474\n",
            "epoch 29 step 179 loss 1.641054630279541\n",
            "epoch 29 step 180 loss 1.6301722526550293 test_accuracy 81.30000305175781 train_accuracy 85.15625\n",
            "epoch 29 step 181 loss 1.6256805658340454\n",
            "epoch 29 step 182 loss 1.6621931791305542\n",
            "epoch 29 step 183 loss 1.664872646331787\n",
            "epoch 29 step 184 loss 1.6333526372909546\n",
            "epoch 29 step 185 loss 1.6430864334106445\n",
            "epoch 29 step 186 loss 1.6445746421813965\n",
            "epoch 29 step 187 loss 1.6245378255844116\n",
            "epoch 29 step 188 loss 1.6331363916397095\n",
            "epoch 29 step 189 loss 1.642741084098816\n",
            "epoch 29 step 190 loss 1.6750099658966064\n",
            "epoch 29 step 191 loss 1.6362637281417847\n",
            "epoch 29 step 192 loss 1.6742653846740723\n",
            "epoch 29 step 193 loss 1.648581624031067\n",
            "epoch 29 step 194 loss 1.6427693367004395\n",
            "epoch 29 step 195 loss 1.656201958656311\n",
            "epoch 30 step 0 loss 1.6523778438568115 test_accuracy 80.20000457763672 train_accuracy 87.6953125\n",
            "epoch 30 step 1 loss 1.6370227336883545\n",
            "epoch 30 step 2 loss 1.6528451442718506\n",
            "epoch 30 step 3 loss 1.6398231983184814\n",
            "epoch 30 step 4 loss 1.6322693824768066\n",
            "epoch 30 step 5 loss 1.6356931924819946\n",
            "epoch 30 step 6 loss 1.6612441539764404\n",
            "epoch 30 step 7 loss 1.6529574394226074\n",
            "epoch 30 step 8 loss 1.6471754312515259\n",
            "epoch 30 step 9 loss 1.642374873161316\n",
            "epoch 30 step 10 loss 1.6479886770248413\n",
            "epoch 30 step 11 loss 1.641538381576538\n",
            "epoch 30 step 12 loss 1.6285536289215088\n",
            "epoch 30 step 13 loss 1.6524620056152344\n",
            "epoch 30 step 14 loss 1.6351393461227417\n",
            "epoch 30 step 15 loss 1.640756368637085\n",
            "epoch 30 step 16 loss 1.6360076665878296\n",
            "epoch 30 step 17 loss 1.6175038814544678\n",
            "epoch 30 step 18 loss 1.6570686101913452\n",
            "epoch 30 step 19 loss 1.6268863677978516\n",
            "epoch 30 step 20 loss 1.6340941190719604 test_accuracy 81.80000305175781 train_accuracy 84.5703125\n",
            "epoch 30 step 21 loss 1.6240922212600708\n",
            "epoch 30 step 22 loss 1.639755129814148\n",
            "epoch 30 step 23 loss 1.6691550016403198\n",
            "epoch 30 step 24 loss 1.6290473937988281\n",
            "epoch 30 step 25 loss 1.6242719888687134\n",
            "epoch 30 step 26 loss 1.647109031677246\n",
            "epoch 30 step 27 loss 1.6443760395050049\n",
            "epoch 30 step 28 loss 1.6557199954986572\n",
            "epoch 30 step 29 loss 1.6538910865783691\n",
            "epoch 30 step 30 loss 1.6309033632278442\n",
            "epoch 30 step 31 loss 1.632771611213684\n",
            "epoch 30 step 32 loss 1.652282953262329\n",
            "epoch 30 step 33 loss 1.6250392198562622\n",
            "epoch 30 step 34 loss 1.6303143501281738\n",
            "epoch 30 step 35 loss 1.6103416681289673\n",
            "epoch 30 step 36 loss 1.6222445964813232\n",
            "epoch 30 step 37 loss 1.6473735570907593\n",
            "epoch 30 step 38 loss 1.6687912940979004\n",
            "epoch 30 step 39 loss 1.6708754301071167\n",
            "epoch 30 step 40 loss 1.6312421560287476 test_accuracy 79.60000610351562 train_accuracy 82.6171875\n",
            "epoch 30 step 41 loss 1.651294231414795\n",
            "epoch 30 step 42 loss 1.6030982732772827\n",
            "epoch 30 step 43 loss 1.6021013259887695\n",
            "epoch 30 step 44 loss 1.6386828422546387\n",
            "epoch 30 step 45 loss 1.6134674549102783\n",
            "epoch 30 step 46 loss 1.6381844282150269\n",
            "epoch 30 step 47 loss 1.6936115026474\n",
            "epoch 30 step 48 loss 1.650804042816162\n",
            "epoch 30 step 49 loss 1.626418113708496\n",
            "epoch 30 step 50 loss 1.6385544538497925\n",
            "epoch 30 step 51 loss 1.6168239116668701\n",
            "epoch 30 step 52 loss 1.6263436079025269\n",
            "epoch 30 step 53 loss 1.6627241373062134\n",
            "epoch 30 step 54 loss 1.667481541633606\n",
            "epoch 30 step 55 loss 1.6668676137924194\n",
            "epoch 30 step 56 loss 1.6383534669876099\n",
            "epoch 30 step 57 loss 1.6492512226104736\n",
            "epoch 30 step 58 loss 1.6632682085037231\n",
            "epoch 30 step 59 loss 1.6749684810638428\n",
            "epoch 30 step 60 loss 1.659582495689392 test_accuracy 80.4000015258789 train_accuracy 87.3046875\n",
            "epoch 30 step 61 loss 1.6428780555725098\n",
            "epoch 30 step 62 loss 1.65530526638031\n",
            "epoch 30 step 63 loss 1.6391891241073608\n",
            "epoch 30 step 64 loss 1.6252180337905884\n",
            "epoch 30 step 65 loss 1.6294238567352295\n",
            "epoch 30 step 66 loss 1.6211128234863281\n",
            "epoch 30 step 67 loss 1.6318647861480713\n",
            "epoch 30 step 68 loss 1.6063969135284424\n",
            "epoch 30 step 69 loss 1.6152915954589844\n",
            "epoch 30 step 70 loss 1.6326102018356323\n",
            "epoch 30 step 71 loss 1.622161865234375\n",
            "epoch 30 step 72 loss 1.634238600730896\n",
            "epoch 30 step 73 loss 1.6412274837493896\n",
            "epoch 30 step 74 loss 1.6174424886703491\n",
            "epoch 30 step 75 loss 1.6653670072555542\n",
            "epoch 30 step 76 loss 1.625195860862732\n",
            "epoch 30 step 77 loss 1.630297303199768\n",
            "epoch 30 step 78 loss 1.6551530361175537\n",
            "epoch 30 step 79 loss 1.6214814186096191\n",
            "epoch 30 step 80 loss 1.6019166707992554 test_accuracy 82.20000457763672 train_accuracy 87.3046875\n",
            "epoch 30 step 81 loss 1.6406477689743042\n",
            "epoch 30 step 82 loss 1.6441864967346191\n",
            "epoch 30 step 83 loss 1.653489351272583\n",
            "epoch 30 step 84 loss 1.6623455286026\n",
            "epoch 30 step 85 loss 1.6696690320968628\n",
            "epoch 30 step 86 loss 1.6689894199371338\n",
            "epoch 30 step 87 loss 1.6212950944900513\n",
            "epoch 30 step 88 loss 1.637866735458374\n",
            "epoch 30 step 89 loss 1.6552751064300537\n",
            "epoch 30 step 90 loss 1.663488745689392\n",
            "epoch 30 step 91 loss 1.6340446472167969\n",
            "epoch 30 step 92 loss 1.633360505104065\n",
            "epoch 30 step 93 loss 1.6157413721084595\n",
            "epoch 30 step 94 loss 1.6451740264892578\n",
            "epoch 30 step 95 loss 1.6143659353256226\n",
            "epoch 30 step 96 loss 1.6457655429840088\n",
            "epoch 30 step 97 loss 1.6495697498321533\n",
            "epoch 30 step 98 loss 1.6380128860473633\n",
            "epoch 30 step 99 loss 1.6294385194778442\n",
            "epoch 30 step 100 loss 1.6334993839263916 test_accuracy 81.60000610351562 train_accuracy 86.5234375\n",
            "epoch 30 step 101 loss 1.630043864250183\n",
            "epoch 30 step 102 loss 1.6282762289047241\n",
            "epoch 30 step 103 loss 1.6401127576828003\n",
            "epoch 30 step 104 loss 1.673137903213501\n",
            "epoch 30 step 105 loss 1.646805763244629\n",
            "epoch 30 step 106 loss 1.6390271186828613\n",
            "epoch 30 step 107 loss 1.6386960744857788\n",
            "epoch 30 step 108 loss 1.649213194847107\n",
            "epoch 30 step 109 loss 1.6169670820236206\n",
            "epoch 30 step 110 loss 1.6707079410552979\n",
            "epoch 30 step 111 loss 1.6515754461288452\n",
            "epoch 30 step 112 loss 1.6525132656097412\n",
            "epoch 30 step 113 loss 1.6212563514709473\n",
            "epoch 30 step 114 loss 1.6361373662948608\n",
            "epoch 30 step 115 loss 1.620872139930725\n",
            "epoch 30 step 116 loss 1.6192069053649902\n",
            "epoch 30 step 117 loss 1.6626648902893066\n",
            "epoch 30 step 118 loss 1.6334487199783325\n",
            "epoch 30 step 119 loss 1.6490018367767334\n",
            "epoch 30 step 120 loss 1.6274772882461548 test_accuracy 81.70000457763672 train_accuracy 83.0078125\n",
            "epoch 30 step 121 loss 1.6308704614639282\n",
            "epoch 30 step 122 loss 1.6416608095169067\n",
            "epoch 30 step 123 loss 1.635364055633545\n",
            "epoch 30 step 124 loss 1.633801817893982\n",
            "epoch 30 step 125 loss 1.6387509107589722\n",
            "epoch 30 step 126 loss 1.6507989168167114\n",
            "epoch 30 step 127 loss 1.649044156074524\n",
            "epoch 30 step 128 loss 1.6577762365341187\n",
            "epoch 30 step 129 loss 1.6516207456588745\n",
            "epoch 30 step 130 loss 1.6609809398651123\n",
            "epoch 30 step 131 loss 1.652050495147705\n",
            "epoch 30 step 132 loss 1.6486632823944092\n",
            "epoch 30 step 133 loss 1.6072152853012085\n",
            "epoch 30 step 134 loss 1.6443028450012207\n",
            "epoch 30 step 135 loss 1.6332467794418335\n",
            "epoch 30 step 136 loss 1.6544523239135742\n",
            "epoch 30 step 137 loss 1.6335283517837524\n",
            "epoch 30 step 138 loss 1.6227424144744873\n",
            "epoch 30 step 139 loss 1.6403436660766602\n",
            "epoch 30 step 140 loss 1.624176025390625 test_accuracy 80.20000457763672 train_accuracy 86.9140625\n",
            "epoch 30 step 141 loss 1.664371371269226\n",
            "epoch 30 step 142 loss 1.6341111660003662\n",
            "epoch 30 step 143 loss 1.6132951974868774\n",
            "epoch 30 step 144 loss 1.6149295568466187\n",
            "epoch 30 step 145 loss 1.627024531364441\n",
            "epoch 30 step 146 loss 1.6413935422897339\n",
            "epoch 30 step 147 loss 1.6309740543365479\n",
            "epoch 30 step 148 loss 1.62876558303833\n",
            "epoch 30 step 149 loss 1.6520565748214722\n",
            "epoch 30 step 150 loss 1.6159769296646118\n",
            "epoch 30 step 151 loss 1.6514077186584473\n",
            "epoch 30 step 152 loss 1.6472018957138062\n",
            "epoch 30 step 153 loss 1.6158945560455322\n",
            "epoch 30 step 154 loss 1.622093915939331\n",
            "epoch 30 step 155 loss 1.6757616996765137\n",
            "epoch 30 step 156 loss 1.6630939245224\n",
            "epoch 30 step 157 loss 1.644986867904663\n",
            "epoch 30 step 158 loss 1.6344516277313232\n",
            "epoch 30 step 159 loss 1.6532491445541382\n",
            "epoch 30 step 160 loss 1.6517022848129272 test_accuracy 80.60000610351562 train_accuracy 88.671875\n",
            "epoch 30 step 161 loss 1.6460853815078735\n",
            "epoch 30 step 162 loss 1.6261334419250488\n",
            "epoch 30 step 163 loss 1.6598193645477295\n",
            "epoch 30 step 164 loss 1.6439114809036255\n",
            "epoch 30 step 165 loss 1.6548514366149902\n",
            "epoch 30 step 166 loss 1.6482151746749878\n",
            "epoch 30 step 167 loss 1.6481796503067017\n",
            "epoch 30 step 168 loss 1.64130437374115\n",
            "epoch 30 step 169 loss 1.6502301692962646\n",
            "epoch 30 step 170 loss 1.6565148830413818\n",
            "epoch 30 step 171 loss 1.642492651939392\n",
            "epoch 30 step 172 loss 1.6371760368347168\n",
            "epoch 30 step 173 loss 1.6479058265686035\n",
            "epoch 30 step 174 loss 1.6463849544525146\n",
            "epoch 30 step 175 loss 1.6378238201141357\n",
            "epoch 30 step 176 loss 1.6399052143096924\n",
            "epoch 30 step 177 loss 1.6703828573226929\n",
            "epoch 30 step 178 loss 1.6323610544204712\n",
            "epoch 30 step 179 loss 1.6465826034545898\n",
            "epoch 30 step 180 loss 1.6446266174316406 test_accuracy 79.10000610351562 train_accuracy 86.5234375\n",
            "epoch 30 step 181 loss 1.6561506986618042\n",
            "epoch 30 step 182 loss 1.6433119773864746\n",
            "epoch 30 step 183 loss 1.6547669172286987\n",
            "epoch 30 step 184 loss 1.6401910781860352\n",
            "epoch 30 step 185 loss 1.651008129119873\n",
            "epoch 30 step 186 loss 1.6195731163024902\n",
            "epoch 30 step 187 loss 1.64102041721344\n",
            "epoch 30 step 188 loss 1.647640347480774\n",
            "epoch 30 step 189 loss 1.658231258392334\n",
            "epoch 30 step 190 loss 1.6318360567092896\n",
            "epoch 30 step 191 loss 1.6283527612686157\n",
            "epoch 30 step 192 loss 1.66847825050354\n",
            "epoch 30 step 193 loss 1.6331405639648438\n",
            "epoch 30 step 194 loss 1.6578158140182495\n",
            "epoch 30 step 195 loss 1.5901025533676147\n",
            "epoch 31 step 0 loss 1.6478030681610107 test_accuracy 83.10000610351562 train_accuracy 84.9609375\n",
            "epoch 31 step 1 loss 1.635765552520752\n",
            "epoch 31 step 2 loss 1.6430362462997437\n",
            "epoch 31 step 3 loss 1.6664754152297974\n",
            "epoch 31 step 4 loss 1.641076683998108\n",
            "epoch 31 step 5 loss 1.6506571769714355\n",
            "epoch 31 step 6 loss 1.6579385995864868\n",
            "epoch 31 step 7 loss 1.6457239389419556\n",
            "epoch 31 step 8 loss 1.6651091575622559\n",
            "epoch 31 step 9 loss 1.6526682376861572\n",
            "epoch 31 step 10 loss 1.637405276298523\n",
            "epoch 31 step 11 loss 1.6369471549987793\n",
            "epoch 31 step 12 loss 1.6534373760223389\n",
            "epoch 31 step 13 loss 1.659604549407959\n",
            "epoch 31 step 14 loss 1.6468929052352905\n",
            "epoch 31 step 15 loss 1.6232960224151611\n",
            "epoch 31 step 16 loss 1.636916160583496\n",
            "epoch 31 step 17 loss 1.6426798105239868\n",
            "epoch 31 step 18 loss 1.6355984210968018\n",
            "epoch 31 step 19 loss 1.6386829614639282\n",
            "epoch 31 step 20 loss 1.6183117628097534 test_accuracy 81.00000762939453 train_accuracy 89.6484375\n",
            "epoch 31 step 21 loss 1.632721185684204\n",
            "epoch 31 step 22 loss 1.6531842947006226\n",
            "epoch 31 step 23 loss 1.6554689407348633\n",
            "epoch 31 step 24 loss 1.6571789979934692\n",
            "epoch 31 step 25 loss 1.6501266956329346\n",
            "epoch 31 step 26 loss 1.619128704071045\n",
            "epoch 31 step 27 loss 1.6358997821807861\n",
            "epoch 31 step 28 loss 1.6168144941329956\n",
            "epoch 31 step 29 loss 1.6270076036453247\n",
            "epoch 31 step 30 loss 1.6605128049850464\n",
            "epoch 31 step 31 loss 1.6477152109146118\n",
            "epoch 31 step 32 loss 1.6191526651382446\n",
            "epoch 31 step 33 loss 1.6342276334762573\n",
            "epoch 31 step 34 loss 1.6190274953842163\n",
            "epoch 31 step 35 loss 1.6474584341049194\n",
            "epoch 31 step 36 loss 1.650057315826416\n",
            "epoch 31 step 37 loss 1.6257925033569336\n",
            "epoch 31 step 38 loss 1.641970157623291\n",
            "epoch 31 step 39 loss 1.6391047239303589\n",
            "epoch 31 step 40 loss 1.6536953449249268 test_accuracy 82.20000457763672 train_accuracy 88.4765625\n",
            "epoch 31 step 41 loss 1.6468501091003418\n",
            "epoch 31 step 42 loss 1.6475969552993774\n",
            "epoch 31 step 43 loss 1.6270562410354614\n",
            "epoch 31 step 44 loss 1.6229052543640137\n",
            "epoch 31 step 45 loss 1.6554144620895386\n",
            "epoch 31 step 46 loss 1.652535319328308\n",
            "epoch 31 step 47 loss 1.6335315704345703\n",
            "epoch 31 step 48 loss 1.6377160549163818\n",
            "epoch 31 step 49 loss 1.6443519592285156\n",
            "epoch 31 step 50 loss 1.6236594915390015\n",
            "epoch 31 step 51 loss 1.6781039237976074\n",
            "epoch 31 step 52 loss 1.6555982828140259\n",
            "epoch 31 step 53 loss 1.6514396667480469\n",
            "epoch 31 step 54 loss 1.6265144348144531\n",
            "epoch 31 step 55 loss 1.6036012172698975\n",
            "epoch 31 step 56 loss 1.6398142576217651\n",
            "epoch 31 step 57 loss 1.659547209739685\n",
            "epoch 31 step 58 loss 1.6619808673858643\n",
            "epoch 31 step 59 loss 1.636849045753479\n",
            "epoch 31 step 60 loss 1.634513258934021 test_accuracy 82.00000762939453 train_accuracy 87.890625\n",
            "epoch 31 step 61 loss 1.596922755241394\n",
            "epoch 31 step 62 loss 1.654921531677246\n",
            "epoch 31 step 63 loss 1.6330347061157227\n",
            "epoch 31 step 64 loss 1.6351178884506226\n",
            "epoch 31 step 65 loss 1.6619527339935303\n",
            "epoch 31 step 66 loss 1.6191004514694214\n",
            "epoch 31 step 67 loss 1.6409578323364258\n",
            "epoch 31 step 68 loss 1.6239852905273438\n",
            "epoch 31 step 69 loss 1.6435718536376953\n",
            "epoch 31 step 70 loss 1.6373835802078247\n",
            "epoch 31 step 71 loss 1.6354904174804688\n",
            "epoch 31 step 72 loss 1.6260498762130737\n",
            "epoch 31 step 73 loss 1.6300098896026611\n",
            "epoch 31 step 74 loss 1.6399955749511719\n",
            "epoch 31 step 75 loss 1.6581809520721436\n",
            "epoch 31 step 76 loss 1.6331074237823486\n",
            "epoch 31 step 77 loss 1.6363568305969238\n",
            "epoch 31 step 78 loss 1.6437300443649292\n",
            "epoch 31 step 79 loss 1.6396045684814453\n",
            "epoch 31 step 80 loss 1.6342662572860718 test_accuracy 80.80000305175781 train_accuracy 89.6484375\n",
            "epoch 31 step 81 loss 1.6235095262527466\n",
            "epoch 31 step 82 loss 1.6135764122009277\n",
            "epoch 31 step 83 loss 1.623011827468872\n",
            "epoch 31 step 84 loss 1.648281216621399\n",
            "epoch 31 step 85 loss 1.6617330312728882\n",
            "epoch 31 step 86 loss 1.626358985900879\n",
            "epoch 31 step 87 loss 1.6456435918807983\n",
            "epoch 31 step 88 loss 1.6265345811843872\n",
            "epoch 31 step 89 loss 1.6472219228744507\n",
            "epoch 31 step 90 loss 1.649520993232727\n",
            "epoch 31 step 91 loss 1.6375486850738525\n",
            "epoch 31 step 92 loss 1.6155657768249512\n",
            "epoch 31 step 93 loss 1.6488053798675537\n",
            "epoch 31 step 94 loss 1.6324697732925415\n",
            "epoch 31 step 95 loss 1.6161574125289917\n",
            "epoch 31 step 96 loss 1.636038899421692\n",
            "epoch 31 step 97 loss 1.6590721607208252\n",
            "epoch 31 step 98 loss 1.6506831645965576\n",
            "epoch 31 step 99 loss 1.6389096975326538\n",
            "epoch 31 step 100 loss 1.6321262121200562 test_accuracy 81.4000015258789 train_accuracy 90.4296875\n",
            "epoch 31 step 101 loss 1.6258740425109863\n",
            "epoch 31 step 102 loss 1.6231645345687866\n",
            "epoch 31 step 103 loss 1.627717137336731\n",
            "epoch 31 step 104 loss 1.637176275253296\n",
            "epoch 31 step 105 loss 1.6258140802383423\n",
            "epoch 31 step 106 loss 1.635522723197937\n",
            "epoch 31 step 107 loss 1.6495389938354492\n",
            "epoch 31 step 108 loss 1.6301867961883545\n",
            "epoch 31 step 109 loss 1.6143746376037598\n",
            "epoch 31 step 110 loss 1.6386748552322388\n",
            "epoch 31 step 111 loss 1.6460325717926025\n",
            "epoch 31 step 112 loss 1.6704274415969849\n",
            "epoch 31 step 113 loss 1.6366775035858154\n",
            "epoch 31 step 114 loss 1.6526784896850586\n",
            "epoch 31 step 115 loss 1.6526893377304077\n",
            "epoch 31 step 116 loss 1.624133825302124\n",
            "epoch 31 step 117 loss 1.659454345703125\n",
            "epoch 31 step 118 loss 1.6025928258895874\n",
            "epoch 31 step 119 loss 1.6416270732879639\n",
            "epoch 31 step 120 loss 1.6399286985397339 test_accuracy 82.50000762939453 train_accuracy 85.15625\n",
            "epoch 31 step 121 loss 1.6412407159805298\n",
            "epoch 31 step 122 loss 1.648146629333496\n",
            "epoch 31 step 123 loss 1.646782398223877\n",
            "epoch 31 step 124 loss 1.6289201974868774\n",
            "epoch 31 step 125 loss 1.6678824424743652\n",
            "epoch 31 step 126 loss 1.6482380628585815\n",
            "epoch 31 step 127 loss 1.6084791421890259\n",
            "epoch 31 step 128 loss 1.634752631187439\n",
            "epoch 31 step 129 loss 1.6479166746139526\n",
            "epoch 31 step 130 loss 1.6301883459091187\n",
            "epoch 31 step 131 loss 1.6475876569747925\n",
            "epoch 31 step 132 loss 1.6424789428710938\n",
            "epoch 31 step 133 loss 1.6369930505752563\n",
            "epoch 31 step 134 loss 1.6607905626296997\n",
            "epoch 31 step 135 loss 1.6367909908294678\n",
            "epoch 31 step 136 loss 1.6464792490005493\n",
            "epoch 31 step 137 loss 1.6375056505203247\n",
            "epoch 31 step 138 loss 1.637730598449707\n",
            "epoch 31 step 139 loss 1.6347867250442505\n",
            "epoch 31 step 140 loss 1.6367136240005493 test_accuracy 83.30000305175781 train_accuracy 85.9375\n",
            "epoch 31 step 141 loss 1.6625701189041138\n",
            "epoch 31 step 142 loss 1.644447684288025\n",
            "epoch 31 step 143 loss 1.6193023920059204\n",
            "epoch 31 step 144 loss 1.6551334857940674\n",
            "epoch 31 step 145 loss 1.6339551210403442\n",
            "epoch 31 step 146 loss 1.6715360879898071\n",
            "epoch 31 step 147 loss 1.6479415893554688\n",
            "epoch 31 step 148 loss 1.6550883054733276\n",
            "epoch 31 step 149 loss 1.6679359674453735\n",
            "epoch 31 step 150 loss 1.6486992835998535\n",
            "epoch 31 step 151 loss 1.652867317199707\n",
            "epoch 31 step 152 loss 1.6544514894485474\n",
            "epoch 31 step 153 loss 1.6554501056671143\n",
            "epoch 31 step 154 loss 1.61563241481781\n",
            "epoch 31 step 155 loss 1.648240327835083\n",
            "epoch 31 step 156 loss 1.6715384721755981\n",
            "epoch 31 step 157 loss 1.6377623081207275\n",
            "epoch 31 step 158 loss 1.6390076875686646\n",
            "epoch 31 step 159 loss 1.6382678747177124\n",
            "epoch 31 step 160 loss 1.663857340812683 test_accuracy 83.4000015258789 train_accuracy 88.0859375\n",
            "epoch 31 step 161 loss 1.6053729057312012\n",
            "epoch 31 step 162 loss 1.6346982717514038\n",
            "epoch 31 step 163 loss 1.6603144407272339\n",
            "epoch 31 step 164 loss 1.6579927206039429\n",
            "epoch 31 step 165 loss 1.6624889373779297\n",
            "epoch 31 step 166 loss 1.6298110485076904\n",
            "epoch 31 step 167 loss 1.6294504404067993\n",
            "epoch 31 step 168 loss 1.650164008140564\n",
            "epoch 31 step 169 loss 1.6250941753387451\n",
            "epoch 31 step 170 loss 1.6488033533096313\n",
            "epoch 31 step 171 loss 1.6231712102890015\n",
            "epoch 31 step 172 loss 1.6395909786224365\n",
            "epoch 31 step 173 loss 1.6403473615646362\n",
            "epoch 31 step 174 loss 1.6423925161361694\n",
            "epoch 31 step 175 loss 1.6492202281951904\n",
            "epoch 31 step 176 loss 1.6243849992752075\n",
            "epoch 31 step 177 loss 1.6790333986282349\n",
            "epoch 31 step 178 loss 1.6532714366912842\n",
            "epoch 31 step 179 loss 1.6417744159698486\n",
            "epoch 31 step 180 loss 1.6405634880065918 test_accuracy 81.20000457763672 train_accuracy 88.671875\n",
            "epoch 31 step 181 loss 1.6178765296936035\n",
            "epoch 31 step 182 loss 1.6428847312927246\n",
            "epoch 31 step 183 loss 1.6647506952285767\n",
            "epoch 31 step 184 loss 1.6403188705444336\n",
            "epoch 31 step 185 loss 1.6115272045135498\n",
            "epoch 31 step 186 loss 1.6436195373535156\n",
            "epoch 31 step 187 loss 1.6213489770889282\n",
            "epoch 31 step 188 loss 1.6547502279281616\n",
            "epoch 31 step 189 loss 1.6219547986984253\n",
            "epoch 31 step 190 loss 1.6445311307907104\n",
            "epoch 31 step 191 loss 1.6413679122924805\n",
            "epoch 31 step 192 loss 1.6348682641983032\n",
            "epoch 31 step 193 loss 1.6381115913391113\n",
            "epoch 31 step 194 loss 1.6566212177276611\n",
            "epoch 31 step 195 loss 1.6112737655639648\n",
            "epoch 32 step 0 loss 1.6044822931289673 test_accuracy 80.50000762939453 train_accuracy 89.0625\n",
            "epoch 32 step 1 loss 1.626181721687317\n",
            "epoch 32 step 2 loss 1.6537712812423706\n",
            "epoch 32 step 3 loss 1.6827760934829712\n",
            "epoch 32 step 4 loss 1.6004652976989746\n",
            "epoch 32 step 5 loss 1.6631367206573486\n",
            "epoch 32 step 6 loss 1.637229323387146\n",
            "epoch 32 step 7 loss 1.6587772369384766\n",
            "epoch 32 step 8 loss 1.5951834917068481\n",
            "epoch 32 step 9 loss 1.647573709487915\n",
            "epoch 32 step 10 loss 1.617238998413086\n",
            "epoch 32 step 11 loss 1.6444257497787476\n",
            "epoch 32 step 12 loss 1.6389315128326416\n",
            "epoch 32 step 13 loss 1.6507784128189087\n",
            "epoch 32 step 14 loss 1.6397454738616943\n",
            "epoch 32 step 15 loss 1.6376060247421265\n",
            "epoch 32 step 16 loss 1.6476601362228394\n",
            "epoch 32 step 17 loss 1.6158150434494019\n",
            "epoch 32 step 18 loss 1.6376502513885498\n",
            "epoch 32 step 19 loss 1.6519770622253418\n",
            "epoch 32 step 20 loss 1.6802324056625366 test_accuracy 80.50000762939453 train_accuracy 87.5\n",
            "epoch 32 step 21 loss 1.6264533996582031\n",
            "epoch 32 step 22 loss 1.6445380449295044\n",
            "epoch 32 step 23 loss 1.6711657047271729\n",
            "epoch 32 step 24 loss 1.6189650297164917\n",
            "epoch 32 step 25 loss 1.6216415166854858\n",
            "epoch 32 step 26 loss 1.6706825494766235\n",
            "epoch 32 step 27 loss 1.6086671352386475\n",
            "epoch 32 step 28 loss 1.6273962259292603\n",
            "epoch 32 step 29 loss 1.6514323949813843\n",
            "epoch 32 step 30 loss 1.6053345203399658\n",
            "epoch 32 step 31 loss 1.6319966316223145\n",
            "epoch 32 step 32 loss 1.636452078819275\n",
            "epoch 32 step 33 loss 1.668392539024353\n",
            "epoch 32 step 34 loss 1.6142255067825317\n",
            "epoch 32 step 35 loss 1.6453349590301514\n",
            "epoch 32 step 36 loss 1.63279128074646\n",
            "epoch 32 step 37 loss 1.6616110801696777\n",
            "epoch 32 step 38 loss 1.6467268466949463\n",
            "epoch 32 step 39 loss 1.6272728443145752\n",
            "epoch 32 step 40 loss 1.647857427597046 test_accuracy 80.70000457763672 train_accuracy 88.8671875\n",
            "epoch 32 step 41 loss 1.6141793727874756\n",
            "epoch 32 step 42 loss 1.640176773071289\n",
            "epoch 32 step 43 loss 1.634339451789856\n",
            "epoch 32 step 44 loss 1.6374188661575317\n",
            "epoch 32 step 45 loss 1.6547603607177734\n",
            "epoch 32 step 46 loss 1.6320116519927979\n",
            "epoch 32 step 47 loss 1.6471600532531738\n",
            "epoch 32 step 48 loss 1.6385462284088135\n",
            "epoch 32 step 49 loss 1.642005205154419\n",
            "epoch 32 step 50 loss 1.6532613039016724\n",
            "epoch 32 step 51 loss 1.6224780082702637\n",
            "epoch 32 step 52 loss 1.6378203630447388\n",
            "epoch 32 step 53 loss 1.6110047101974487\n",
            "epoch 32 step 54 loss 1.6520198583602905\n",
            "epoch 32 step 55 loss 1.6398543119430542\n",
            "epoch 32 step 56 loss 1.650937795639038\n",
            "epoch 32 step 57 loss 1.645263671875\n",
            "epoch 32 step 58 loss 1.652091383934021\n",
            "epoch 32 step 59 loss 1.6291444301605225\n",
            "epoch 32 step 60 loss 1.6262061595916748 test_accuracy 80.30000305175781 train_accuracy 88.671875\n",
            "epoch 32 step 61 loss 1.6143091917037964\n",
            "epoch 32 step 62 loss 1.6463075876235962\n",
            "epoch 32 step 63 loss 1.633478045463562\n",
            "epoch 32 step 64 loss 1.6208972930908203\n",
            "epoch 32 step 65 loss 1.6416624784469604\n",
            "epoch 32 step 66 loss 1.632383108139038\n",
            "epoch 32 step 67 loss 1.6286201477050781\n",
            "epoch 32 step 68 loss 1.6344645023345947\n",
            "epoch 32 step 69 loss 1.6258108615875244\n",
            "epoch 32 step 70 loss 1.6557496786117554\n",
            "epoch 32 step 71 loss 1.6386116743087769\n",
            "epoch 32 step 72 loss 1.6357395648956299\n",
            "epoch 32 step 73 loss 1.6323801279067993\n",
            "epoch 32 step 74 loss 1.6152781248092651\n",
            "epoch 32 step 75 loss 1.5905802249908447\n",
            "epoch 32 step 76 loss 1.6369616985321045\n",
            "epoch 32 step 77 loss 1.6356086730957031\n",
            "epoch 32 step 78 loss 1.6411771774291992\n",
            "epoch 32 step 79 loss 1.6258596181869507\n",
            "epoch 32 step 80 loss 1.6512537002563477 test_accuracy 82.20000457763672 train_accuracy 87.109375\n",
            "epoch 32 step 81 loss 1.609542965888977\n",
            "epoch 32 step 82 loss 1.640182375907898\n",
            "epoch 32 step 83 loss 1.6431890726089478\n",
            "epoch 32 step 84 loss 1.6285209655761719\n",
            "epoch 32 step 85 loss 1.639854073524475\n",
            "epoch 32 step 86 loss 1.6161466836929321\n",
            "epoch 32 step 87 loss 1.6548120975494385\n",
            "epoch 32 step 88 loss 1.6273480653762817\n",
            "epoch 32 step 89 loss 1.6426048278808594\n",
            "epoch 32 step 90 loss 1.6530077457427979\n",
            "epoch 32 step 91 loss 1.6320399045944214\n",
            "epoch 32 step 92 loss 1.6406947374343872\n",
            "epoch 32 step 93 loss 1.6495505571365356\n",
            "epoch 32 step 94 loss 1.643512487411499\n",
            "epoch 32 step 95 loss 1.6082876920700073\n",
            "epoch 32 step 96 loss 1.6400878429412842\n",
            "epoch 32 step 97 loss 1.6162875890731812\n",
            "epoch 32 step 98 loss 1.6333403587341309\n",
            "epoch 32 step 99 loss 1.6326987743377686\n",
            "epoch 32 step 100 loss 1.6372467279434204 test_accuracy 79.4000015258789 train_accuracy 86.328125\n",
            "epoch 32 step 101 loss 1.6531740427017212\n",
            "epoch 32 step 102 loss 1.5887575149536133\n",
            "epoch 32 step 103 loss 1.6298648118972778\n",
            "epoch 32 step 104 loss 1.669395089149475\n",
            "epoch 32 step 105 loss 1.6213045120239258\n",
            "epoch 32 step 106 loss 1.6228872537612915\n",
            "epoch 32 step 107 loss 1.6273987293243408\n",
            "epoch 32 step 108 loss 1.6525375843048096\n",
            "epoch 32 step 109 loss 1.6395870447158813\n",
            "epoch 32 step 110 loss 1.637668490409851\n",
            "epoch 32 step 111 loss 1.659521460533142\n",
            "epoch 32 step 112 loss 1.6353683471679688\n",
            "epoch 32 step 113 loss 1.6628413200378418\n",
            "epoch 32 step 114 loss 1.6341614723205566\n",
            "epoch 32 step 115 loss 1.6289488077163696\n",
            "epoch 32 step 116 loss 1.6375333070755005\n",
            "epoch 32 step 117 loss 1.630403757095337\n",
            "epoch 32 step 118 loss 1.614769458770752\n",
            "epoch 32 step 119 loss 1.6287174224853516\n",
            "epoch 32 step 120 loss 1.649925708770752 test_accuracy 80.60000610351562 train_accuracy 85.9375\n",
            "epoch 32 step 121 loss 1.6325112581253052\n",
            "epoch 32 step 122 loss 1.6345218420028687\n",
            "epoch 32 step 123 loss 1.6258989572525024\n",
            "epoch 32 step 124 loss 1.628774642944336\n",
            "epoch 32 step 125 loss 1.6393507719039917\n",
            "epoch 32 step 126 loss 1.6425803899765015\n",
            "epoch 32 step 127 loss 1.62129807472229\n",
            "epoch 32 step 128 loss 1.6363024711608887\n",
            "epoch 32 step 129 loss 1.6475549936294556\n",
            "epoch 32 step 130 loss 1.6142756938934326\n",
            "epoch 32 step 131 loss 1.6472201347351074\n",
            "epoch 32 step 132 loss 1.6409087181091309\n",
            "epoch 32 step 133 loss 1.6427943706512451\n",
            "epoch 32 step 134 loss 1.644428014755249\n",
            "epoch 32 step 135 loss 1.643599033355713\n",
            "epoch 32 step 136 loss 1.6483855247497559\n",
            "epoch 32 step 137 loss 1.6268229484558105\n",
            "epoch 32 step 138 loss 1.6523834466934204\n",
            "epoch 32 step 139 loss 1.6319611072540283\n",
            "epoch 32 step 140 loss 1.5978044271469116 test_accuracy 79.9000015258789 train_accuracy 85.9375\n",
            "epoch 32 step 141 loss 1.6467905044555664\n",
            "epoch 32 step 142 loss 1.6729339361190796\n",
            "epoch 32 step 143 loss 1.6344112157821655\n",
            "epoch 32 step 144 loss 1.6586549282073975\n",
            "epoch 32 step 145 loss 1.6382644176483154\n",
            "epoch 32 step 146 loss 1.6365097761154175\n",
            "epoch 32 step 147 loss 1.6421867609024048\n",
            "epoch 32 step 148 loss 1.6053003072738647\n",
            "epoch 32 step 149 loss 1.6697946786880493\n",
            "epoch 32 step 150 loss 1.6433095932006836\n",
            "epoch 32 step 151 loss 1.662465214729309\n",
            "epoch 32 step 152 loss 1.638121247291565\n",
            "epoch 32 step 153 loss 1.621208667755127\n",
            "epoch 32 step 154 loss 1.6888341903686523\n",
            "epoch 32 step 155 loss 1.636165738105774\n",
            "epoch 32 step 156 loss 1.6415880918502808\n",
            "epoch 32 step 157 loss 1.6314013004302979\n",
            "epoch 32 step 158 loss 1.6546674966812134\n",
            "epoch 32 step 159 loss 1.6634478569030762\n",
            "epoch 32 step 160 loss 1.6224437952041626 test_accuracy 80.50000762939453 train_accuracy 88.671875\n",
            "epoch 32 step 161 loss 1.6263877153396606\n",
            "epoch 32 step 162 loss 1.6563893556594849\n",
            "epoch 32 step 163 loss 1.6489620208740234\n",
            "epoch 32 step 164 loss 1.6234893798828125\n",
            "epoch 32 step 165 loss 1.6571643352508545\n",
            "epoch 32 step 166 loss 1.6376622915267944\n",
            "epoch 32 step 167 loss 1.6587053537368774\n",
            "epoch 32 step 168 loss 1.6429702043533325\n",
            "epoch 32 step 169 loss 1.6606485843658447\n",
            "epoch 32 step 170 loss 1.6316062211990356\n",
            "epoch 32 step 171 loss 1.6445794105529785\n",
            "epoch 32 step 172 loss 1.6776707172393799\n",
            "epoch 32 step 173 loss 1.6164642572402954\n",
            "epoch 32 step 174 loss 1.6344115734100342\n",
            "epoch 32 step 175 loss 1.6473363637924194\n",
            "epoch 32 step 176 loss 1.6428223848342896\n",
            "epoch 32 step 177 loss 1.6168209314346313\n",
            "epoch 32 step 178 loss 1.6497201919555664\n",
            "epoch 32 step 179 loss 1.6455161571502686\n",
            "epoch 32 step 180 loss 1.634775161743164 test_accuracy 81.30000305175781 train_accuracy 85.9375\n",
            "epoch 32 step 181 loss 1.6553515195846558\n",
            "epoch 32 step 182 loss 1.6529330015182495\n",
            "epoch 32 step 183 loss 1.6331709623336792\n",
            "epoch 32 step 184 loss 1.6292414665222168\n",
            "epoch 32 step 185 loss 1.607637882232666\n",
            "epoch 32 step 186 loss 1.6424479484558105\n",
            "epoch 32 step 187 loss 1.6398746967315674\n",
            "epoch 32 step 188 loss 1.6442604064941406\n",
            "epoch 32 step 189 loss 1.6658822298049927\n",
            "epoch 32 step 190 loss 1.6276512145996094\n",
            "epoch 32 step 191 loss 1.6551200151443481\n",
            "epoch 32 step 192 loss 1.6496250629425049\n",
            "epoch 32 step 193 loss 1.6439425945281982\n",
            "epoch 32 step 194 loss 1.6380672454833984\n",
            "epoch 32 step 195 loss 1.6016738414764404\n",
            "epoch 33 step 0 loss 1.6319128274917603 test_accuracy 80.9000015258789 train_accuracy 86.9140625\n",
            "epoch 33 step 1 loss 1.6567085981369019\n",
            "epoch 33 step 2 loss 1.6339843273162842\n",
            "epoch 33 step 3 loss 1.658629059791565\n",
            "epoch 33 step 4 loss 1.6300280094146729\n",
            "epoch 33 step 5 loss 1.6222455501556396\n",
            "epoch 33 step 6 loss 1.6404671669006348\n",
            "epoch 33 step 7 loss 1.6507338285446167\n",
            "epoch 33 step 8 loss 1.6464674472808838\n",
            "epoch 33 step 9 loss 1.6451051235198975\n",
            "epoch 33 step 10 loss 1.6289979219436646\n",
            "epoch 33 step 11 loss 1.6368846893310547\n",
            "epoch 33 step 12 loss 1.6394509077072144\n",
            "epoch 33 step 13 loss 1.6354808807373047\n",
            "epoch 33 step 14 loss 1.6465516090393066\n",
            "epoch 33 step 15 loss 1.6414146423339844\n",
            "epoch 33 step 16 loss 1.632949948310852\n",
            "epoch 33 step 17 loss 1.6097522974014282\n",
            "epoch 33 step 18 loss 1.642968773841858\n",
            "epoch 33 step 19 loss 1.6488893032073975\n",
            "epoch 33 step 20 loss 1.610914707183838 test_accuracy 81.60000610351562 train_accuracy 87.890625\n",
            "epoch 33 step 21 loss 1.6547951698303223\n",
            "epoch 33 step 22 loss 1.6384437084197998\n",
            "epoch 33 step 23 loss 1.662229299545288\n",
            "epoch 33 step 24 loss 1.642637014389038\n",
            "epoch 33 step 25 loss 1.6526859998703003\n",
            "epoch 33 step 26 loss 1.6256377696990967\n",
            "epoch 33 step 27 loss 1.6546629667282104\n",
            "epoch 33 step 28 loss 1.655009150505066\n",
            "epoch 33 step 29 loss 1.6429502964019775\n",
            "epoch 33 step 30 loss 1.652390718460083\n",
            "epoch 33 step 31 loss 1.6374597549438477\n",
            "epoch 33 step 32 loss 1.637696385383606\n",
            "epoch 33 step 33 loss 1.6125565767288208\n",
            "epoch 33 step 34 loss 1.658893346786499\n",
            "epoch 33 step 35 loss 1.651042103767395\n",
            "epoch 33 step 36 loss 1.5988335609436035\n",
            "epoch 33 step 37 loss 1.6026945114135742\n",
            "epoch 33 step 38 loss 1.656541109085083\n",
            "epoch 33 step 39 loss 1.649399995803833\n",
            "epoch 33 step 40 loss 1.6251215934753418 test_accuracy 81.10000610351562 train_accuracy 85.9375\n",
            "epoch 33 step 41 loss 1.6571303606033325\n",
            "epoch 33 step 42 loss 1.6386300325393677\n",
            "epoch 33 step 43 loss 1.618774175643921\n",
            "epoch 33 step 44 loss 1.648303508758545\n",
            "epoch 33 step 45 loss 1.6243202686309814\n",
            "epoch 33 step 46 loss 1.6399412155151367\n",
            "epoch 33 step 47 loss 1.6059918403625488\n",
            "epoch 33 step 48 loss 1.6380327939987183\n",
            "epoch 33 step 49 loss 1.6297560930252075\n",
            "epoch 33 step 50 loss 1.634771466255188\n",
            "epoch 33 step 51 loss 1.6346548795700073\n",
            "epoch 33 step 52 loss 1.631895661354065\n",
            "epoch 33 step 53 loss 1.6388649940490723\n",
            "epoch 33 step 54 loss 1.639660358428955\n",
            "epoch 33 step 55 loss 1.6325185298919678\n",
            "epoch 33 step 56 loss 1.6537377834320068\n",
            "epoch 33 step 57 loss 1.6444549560546875\n",
            "epoch 33 step 58 loss 1.6358299255371094\n",
            "epoch 33 step 59 loss 1.6323376893997192\n",
            "epoch 33 step 60 loss 1.622379183769226 test_accuracy 83.00000762939453 train_accuracy 88.28125\n",
            "epoch 33 step 61 loss 1.6325680017471313\n",
            "epoch 33 step 62 loss 1.6063358783721924\n",
            "epoch 33 step 63 loss 1.6410058736801147\n",
            "epoch 33 step 64 loss 1.638939619064331\n",
            "epoch 33 step 65 loss 1.6516249179840088\n",
            "epoch 33 step 66 loss 1.6430855989456177\n",
            "epoch 33 step 67 loss 1.6264592409133911\n",
            "epoch 33 step 68 loss 1.6506117582321167\n",
            "epoch 33 step 69 loss 1.6397840976715088\n",
            "epoch 33 step 70 loss 1.6303595304489136\n",
            "epoch 33 step 71 loss 1.6547598838806152\n",
            "epoch 33 step 72 loss 1.6333839893341064\n",
            "epoch 33 step 73 loss 1.6489872932434082\n",
            "epoch 33 step 74 loss 1.6387617588043213\n",
            "epoch 33 step 75 loss 1.6221925020217896\n",
            "epoch 33 step 76 loss 1.6231849193572998\n",
            "epoch 33 step 77 loss 1.656764268875122\n",
            "epoch 33 step 78 loss 1.6323671340942383\n",
            "epoch 33 step 79 loss 1.6389747858047485\n",
            "epoch 33 step 80 loss 1.6444989442825317 test_accuracy 80.50000762939453 train_accuracy 87.5\n",
            "epoch 33 step 81 loss 1.6426115036010742\n",
            "epoch 33 step 82 loss 1.6356251239776611\n",
            "epoch 33 step 83 loss 1.6472628116607666\n",
            "epoch 33 step 84 loss 1.6382503509521484\n",
            "epoch 33 step 85 loss 1.6444001197814941\n",
            "epoch 33 step 86 loss 1.6536016464233398\n",
            "epoch 33 step 87 loss 1.6529197692871094\n",
            "epoch 33 step 88 loss 1.658711552619934\n",
            "epoch 33 step 89 loss 1.6220996379852295\n",
            "epoch 33 step 90 loss 1.6259206533432007\n",
            "epoch 33 step 91 loss 1.654128074645996\n",
            "epoch 33 step 92 loss 1.6495081186294556\n",
            "epoch 33 step 93 loss 1.6213456392288208\n",
            "epoch 33 step 94 loss 1.6424906253814697\n",
            "epoch 33 step 95 loss 1.6179901361465454\n",
            "epoch 33 step 96 loss 1.6413575410842896\n",
            "epoch 33 step 97 loss 1.642013669013977\n",
            "epoch 33 step 98 loss 1.6587269306182861\n",
            "epoch 33 step 99 loss 1.6416137218475342\n",
            "epoch 33 step 100 loss 1.6559624671936035 test_accuracy 82.30000305175781 train_accuracy 89.6484375\n",
            "epoch 33 step 101 loss 1.6458641290664673\n",
            "epoch 33 step 102 loss 1.650152325630188\n",
            "epoch 33 step 103 loss 1.6437349319458008\n",
            "epoch 33 step 104 loss 1.672797441482544\n",
            "epoch 33 step 105 loss 1.6430109739303589\n",
            "epoch 33 step 106 loss 1.628609299659729\n",
            "epoch 33 step 107 loss 1.664796233177185\n",
            "epoch 33 step 108 loss 1.6435343027114868\n",
            "epoch 33 step 109 loss 1.6280685663223267\n",
            "epoch 33 step 110 loss 1.6255087852478027\n",
            "epoch 33 step 111 loss 1.661308765411377\n",
            "epoch 33 step 112 loss 1.6331849098205566\n",
            "epoch 33 step 113 loss 1.6554977893829346\n",
            "epoch 33 step 114 loss 1.6247550249099731\n",
            "epoch 33 step 115 loss 1.6394098997116089\n",
            "epoch 33 step 116 loss 1.6218880414962769\n",
            "epoch 33 step 117 loss 1.652785301208496\n",
            "epoch 33 step 118 loss 1.6515663862228394\n",
            "epoch 33 step 119 loss 1.6522021293640137\n",
            "epoch 33 step 120 loss 1.6436505317687988 test_accuracy 82.50000762939453 train_accuracy 87.890625\n",
            "epoch 33 step 121 loss 1.6417505741119385\n",
            "epoch 33 step 122 loss 1.6423312425613403\n",
            "epoch 33 step 123 loss 1.6287555694580078\n",
            "epoch 33 step 124 loss 1.642664909362793\n",
            "epoch 33 step 125 loss 1.619706630706787\n",
            "epoch 33 step 126 loss 1.6212282180786133\n",
            "epoch 33 step 127 loss 1.633653163909912\n",
            "epoch 33 step 128 loss 1.596232533454895\n",
            "epoch 33 step 129 loss 1.6387652158737183\n",
            "epoch 33 step 130 loss 1.6349714994430542\n",
            "epoch 33 step 131 loss 1.6412904262542725\n",
            "epoch 33 step 132 loss 1.620206356048584\n",
            "epoch 33 step 133 loss 1.6530566215515137\n",
            "epoch 33 step 134 loss 1.6358559131622314\n",
            "epoch 33 step 135 loss 1.6269879341125488\n",
            "epoch 33 step 136 loss 1.6489689350128174\n",
            "epoch 33 step 137 loss 1.6431397199630737\n",
            "epoch 33 step 138 loss 1.6570147275924683\n",
            "epoch 33 step 139 loss 1.6442776918411255\n",
            "epoch 33 step 140 loss 1.6302348375320435 test_accuracy 82.4000015258789 train_accuracy 91.40625\n",
            "epoch 33 step 141 loss 1.6437199115753174\n",
            "epoch 33 step 142 loss 1.6418360471725464\n",
            "epoch 33 step 143 loss 1.6196391582489014\n",
            "epoch 33 step 144 loss 1.6540882587432861\n",
            "epoch 33 step 145 loss 1.6456879377365112\n",
            "epoch 33 step 146 loss 1.6531347036361694\n",
            "epoch 33 step 147 loss 1.6217708587646484\n",
            "epoch 33 step 148 loss 1.644189476966858\n",
            "epoch 33 step 149 loss 1.6057054996490479\n",
            "epoch 33 step 150 loss 1.6479369401931763\n",
            "epoch 33 step 151 loss 1.6410194635391235\n",
            "epoch 33 step 152 loss 1.6394836902618408\n",
            "epoch 33 step 153 loss 1.6395540237426758\n",
            "epoch 33 step 154 loss 1.6374160051345825\n",
            "epoch 33 step 155 loss 1.6264744997024536\n",
            "epoch 33 step 156 loss 1.6672052145004272\n",
            "epoch 33 step 157 loss 1.636862874031067\n",
            "epoch 33 step 158 loss 1.643463373184204\n",
            "epoch 33 step 159 loss 1.668424129486084\n",
            "epoch 33 step 160 loss 1.6311979293823242 test_accuracy 81.9000015258789 train_accuracy 86.5234375\n",
            "epoch 33 step 161 loss 1.6691025495529175\n",
            "epoch 33 step 162 loss 1.6138310432434082\n",
            "epoch 33 step 163 loss 1.6151403188705444\n",
            "epoch 33 step 164 loss 1.6820058822631836\n",
            "epoch 33 step 165 loss 1.6385948657989502\n",
            "epoch 33 step 166 loss 1.628458023071289\n",
            "epoch 33 step 167 loss 1.6512185335159302\n",
            "epoch 33 step 168 loss 1.6565937995910645\n",
            "epoch 33 step 169 loss 1.6323432922363281\n",
            "epoch 33 step 170 loss 1.6295698881149292\n",
            "epoch 33 step 171 loss 1.6801084280014038\n",
            "epoch 33 step 172 loss 1.6362613439559937\n",
            "epoch 33 step 173 loss 1.6535509824752808\n",
            "epoch 33 step 174 loss 1.6473333835601807\n",
            "epoch 33 step 175 loss 1.6193617582321167\n",
            "epoch 33 step 176 loss 1.661736249923706\n",
            "epoch 33 step 177 loss 1.6525663137435913\n",
            "epoch 33 step 178 loss 1.6346657276153564\n",
            "epoch 33 step 179 loss 1.6196759939193726\n",
            "epoch 33 step 180 loss 1.6640650033950806 test_accuracy 84.4000015258789 train_accuracy 87.6953125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrp1t-T9hXnK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89886bdf-cfdd-4789-a03e-d97afe8055e0"
      },
      "source": [
        "test_loader1 = DataLoader(dataset = test_dataset,batch_size = len(test_dataset),shuffle=True)\n",
        "test(test_loader1)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81.22999572753906"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CM3sL0FB9n0"
      },
      "source": [
        "#%% saving state\n",
        "from google.colab import files\n",
        "torch.save(model, '/model 75')\n",
        "torch.save(optimizer.state_dict(), '/optim_state 75')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "II9-LJ9lP03H",
        "outputId": "fbc4119c-6a58-4d18-ad54-e3d2cf3d813a"
      },
      "source": [
        "#%% test\n",
        "raw_image = torch.ones((2,3,32,32))\n",
        "raw_image = raw_image*255\n",
        "raw_image = raw_image.to(device)\n",
        "raw_image.requires_grad = True\n",
        "image_description = torch.Tensor([1,1])\n",
        "image_description = image_description.to(torch.int64)\n",
        "image_description = image_description.to(device)\n",
        "optimizer_make = torch.optim.Adam([raw_image],lr = 0.001)\n",
        "for i in model.parameters():\n",
        "    i.requires_grad = False\n",
        "runs = 1000\n",
        "for i in range(runs):\n",
        "    y_pred = model(raw_image)\n",
        "    loss = cost(y_pred,image_description)\n",
        "    optimizer_make.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_make.step()\n",
        "    print(f'epoch {i+1} loss {loss}')\n",
        "raw_image = raw_image.squeeze()\n",
        "t = transforms.ToPILImage()\n",
        "image = t(raw_image[0])\n",
        "display(image)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1 loss 2.303746223449707\n",
            "epoch 2 loss 2.303746223449707\n",
            "epoch 3 loss 2.303746223449707\n",
            "epoch 4 loss 2.303746223449707\n",
            "epoch 5 loss 2.303746223449707\n",
            "epoch 6 loss 2.303746223449707\n",
            "epoch 7 loss 2.303746223449707\n",
            "epoch 8 loss 2.303746223449707\n",
            "epoch 9 loss 2.303746223449707\n",
            "epoch 10 loss 2.303746223449707\n",
            "epoch 11 loss 2.303746223449707\n",
            "epoch 12 loss 2.303746223449707\n",
            "epoch 13 loss 2.303746223449707\n",
            "epoch 14 loss 2.303746223449707\n",
            "epoch 15 loss 2.303746223449707\n",
            "epoch 16 loss 2.303746223449707\n",
            "epoch 17 loss 2.303746223449707\n",
            "epoch 18 loss 2.303746223449707\n",
            "epoch 19 loss 2.303746223449707\n",
            "epoch 20 loss 2.303746223449707\n",
            "epoch 21 loss 2.303746223449707\n",
            "epoch 22 loss 2.303746223449707\n",
            "epoch 23 loss 2.303746223449707\n",
            "epoch 24 loss 2.303746223449707\n",
            "epoch 25 loss 2.303746223449707\n",
            "epoch 26 loss 2.303746223449707\n",
            "epoch 27 loss 2.303746223449707\n",
            "epoch 28 loss 2.303746223449707\n",
            "epoch 29 loss 2.303746223449707\n",
            "epoch 30 loss 2.303746223449707\n",
            "epoch 31 loss 2.303746223449707\n",
            "epoch 32 loss 2.303746223449707\n",
            "epoch 33 loss 2.303746223449707\n",
            "epoch 34 loss 2.303746223449707\n",
            "epoch 35 loss 2.303746223449707\n",
            "epoch 36 loss 2.303746223449707\n",
            "epoch 37 loss 2.303746223449707\n",
            "epoch 38 loss 2.303746223449707\n",
            "epoch 39 loss 2.303746223449707\n",
            "epoch 40 loss 2.303746223449707\n",
            "epoch 41 loss 2.303746223449707\n",
            "epoch 42 loss 2.303746223449707\n",
            "epoch 43 loss 2.303746223449707\n",
            "epoch 44 loss 2.303746223449707\n",
            "epoch 45 loss 2.303746223449707\n",
            "epoch 46 loss 2.303746223449707\n",
            "epoch 47 loss 2.303746223449707\n",
            "epoch 48 loss 2.303746223449707\n",
            "epoch 49 loss 2.303746223449707\n",
            "epoch 50 loss 2.303746223449707\n",
            "epoch 51 loss 2.303746223449707\n",
            "epoch 52 loss 2.303746223449707\n",
            "epoch 53 loss 2.303746223449707\n",
            "epoch 54 loss 2.303746223449707\n",
            "epoch 55 loss 2.303746223449707\n",
            "epoch 56 loss 2.303746223449707\n",
            "epoch 57 loss 2.303746223449707\n",
            "epoch 58 loss 2.303746223449707\n",
            "epoch 59 loss 2.303746223449707\n",
            "epoch 60 loss 2.303746223449707\n",
            "epoch 61 loss 2.303746223449707\n",
            "epoch 62 loss 2.303746223449707\n",
            "epoch 63 loss 2.303746223449707\n",
            "epoch 64 loss 2.303746223449707\n",
            "epoch 65 loss 2.303746223449707\n",
            "epoch 66 loss 2.303746223449707\n",
            "epoch 67 loss 2.303746223449707\n",
            "epoch 68 loss 2.303746223449707\n",
            "epoch 69 loss 2.303746223449707\n",
            "epoch 70 loss 2.303746223449707\n",
            "epoch 71 loss 2.303746223449707\n",
            "epoch 72 loss 2.303746223449707\n",
            "epoch 73 loss 2.303746223449707\n",
            "epoch 74 loss 2.303746223449707\n",
            "epoch 75 loss 2.303746223449707\n",
            "epoch 76 loss 2.303746223449707\n",
            "epoch 77 loss 2.303746223449707\n",
            "epoch 78 loss 2.303746223449707\n",
            "epoch 79 loss 2.303746223449707\n",
            "epoch 80 loss 2.303746223449707\n",
            "epoch 81 loss 2.303746223449707\n",
            "epoch 82 loss 2.303746223449707\n",
            "epoch 83 loss 2.303746223449707\n",
            "epoch 84 loss 2.303746223449707\n",
            "epoch 85 loss 2.303746223449707\n",
            "epoch 86 loss 2.303746223449707\n",
            "epoch 87 loss 2.303746223449707\n",
            "epoch 88 loss 2.303746223449707\n",
            "epoch 89 loss 2.303746223449707\n",
            "epoch 90 loss 2.303746223449707\n",
            "epoch 91 loss 2.303746223449707\n",
            "epoch 92 loss 2.303746223449707\n",
            "epoch 93 loss 2.303746223449707\n",
            "epoch 94 loss 2.303746223449707\n",
            "epoch 95 loss 2.303746223449707\n",
            "epoch 96 loss 2.303746223449707\n",
            "epoch 97 loss 2.303746223449707\n",
            "epoch 98 loss 2.303746223449707\n",
            "epoch 99 loss 2.303746223449707\n",
            "epoch 100 loss 2.303746223449707\n",
            "epoch 101 loss 2.303746223449707\n",
            "epoch 102 loss 2.303746223449707\n",
            "epoch 103 loss 2.303746223449707\n",
            "epoch 104 loss 2.303746223449707\n",
            "epoch 105 loss 2.303746223449707\n",
            "epoch 106 loss 2.303746223449707\n",
            "epoch 107 loss 2.303746223449707\n",
            "epoch 108 loss 2.303746223449707\n",
            "epoch 109 loss 2.303746223449707\n",
            "epoch 110 loss 2.303746223449707\n",
            "epoch 111 loss 2.303746223449707\n",
            "epoch 112 loss 2.303746223449707\n",
            "epoch 113 loss 2.303746223449707\n",
            "epoch 114 loss 2.303746223449707\n",
            "epoch 115 loss 2.303746223449707\n",
            "epoch 116 loss 2.303746223449707\n",
            "epoch 117 loss 2.303746223449707\n",
            "epoch 118 loss 2.303746223449707\n",
            "epoch 119 loss 2.303746223449707\n",
            "epoch 120 loss 2.303746223449707\n",
            "epoch 121 loss 2.303746223449707\n",
            "epoch 122 loss 2.303746223449707\n",
            "epoch 123 loss 2.303746223449707\n",
            "epoch 124 loss 2.303746223449707\n",
            "epoch 125 loss 2.303746223449707\n",
            "epoch 126 loss 2.303746223449707\n",
            "epoch 127 loss 2.303746223449707\n",
            "epoch 128 loss 2.303746223449707\n",
            "epoch 129 loss 2.303746223449707\n",
            "epoch 130 loss 2.303746223449707\n",
            "epoch 131 loss 2.303746223449707\n",
            "epoch 132 loss 2.303746223449707\n",
            "epoch 133 loss 2.303746223449707\n",
            "epoch 134 loss 2.303746223449707\n",
            "epoch 135 loss 2.303746223449707\n",
            "epoch 136 loss 2.303746223449707\n",
            "epoch 137 loss 2.303746223449707\n",
            "epoch 138 loss 2.303746223449707\n",
            "epoch 139 loss 2.303746223449707\n",
            "epoch 140 loss 2.303746223449707\n",
            "epoch 141 loss 2.303746223449707\n",
            "epoch 142 loss 2.303746223449707\n",
            "epoch 143 loss 2.303746223449707\n",
            "epoch 144 loss 2.303746223449707\n",
            "epoch 145 loss 2.303746223449707\n",
            "epoch 146 loss 2.303746223449707\n",
            "epoch 147 loss 2.303746223449707\n",
            "epoch 148 loss 2.303746223449707\n",
            "epoch 149 loss 2.303746223449707\n",
            "epoch 150 loss 2.303746223449707\n",
            "epoch 151 loss 2.303746223449707\n",
            "epoch 152 loss 2.303746223449707\n",
            "epoch 153 loss 2.303746223449707\n",
            "epoch 154 loss 2.303746223449707\n",
            "epoch 155 loss 2.303746223449707\n",
            "epoch 156 loss 2.303746223449707\n",
            "epoch 157 loss 2.303746223449707\n",
            "epoch 158 loss 2.303746223449707\n",
            "epoch 159 loss 2.303746223449707\n",
            "epoch 160 loss 2.303746223449707\n",
            "epoch 161 loss 2.303746223449707\n",
            "epoch 162 loss 2.303746223449707\n",
            "epoch 163 loss 2.303746223449707\n",
            "epoch 164 loss 2.303746223449707\n",
            "epoch 165 loss 2.303746223449707\n",
            "epoch 166 loss 2.303746223449707\n",
            "epoch 167 loss 2.303746223449707\n",
            "epoch 168 loss 2.303746223449707\n",
            "epoch 169 loss 2.303746223449707\n",
            "epoch 170 loss 2.303746223449707\n",
            "epoch 171 loss 2.303746223449707\n",
            "epoch 172 loss 2.303746223449707\n",
            "epoch 173 loss 2.303746223449707\n",
            "epoch 174 loss 2.303746223449707\n",
            "epoch 175 loss 2.303746223449707\n",
            "epoch 176 loss 2.303746223449707\n",
            "epoch 177 loss 2.303746223449707\n",
            "epoch 178 loss 2.303746223449707\n",
            "epoch 179 loss 2.303746223449707\n",
            "epoch 180 loss 2.303746223449707\n",
            "epoch 181 loss 2.303746223449707\n",
            "epoch 182 loss 2.303746223449707\n",
            "epoch 183 loss 2.303746223449707\n",
            "epoch 184 loss 2.303746223449707\n",
            "epoch 185 loss 2.303746223449707\n",
            "epoch 186 loss 2.303746223449707\n",
            "epoch 187 loss 2.303746223449707\n",
            "epoch 188 loss 2.303746223449707\n",
            "epoch 189 loss 2.303746223449707\n",
            "epoch 190 loss 2.303746223449707\n",
            "epoch 191 loss 2.303746223449707\n",
            "epoch 192 loss 2.303746223449707\n",
            "epoch 193 loss 2.303746223449707\n",
            "epoch 194 loss 2.303746223449707\n",
            "epoch 195 loss 2.303746223449707\n",
            "epoch 196 loss 2.303746223449707\n",
            "epoch 197 loss 2.303746223449707\n",
            "epoch 198 loss 2.303746223449707\n",
            "epoch 199 loss 2.303746223449707\n",
            "epoch 200 loss 2.303746223449707\n",
            "epoch 201 loss 2.303746223449707\n",
            "epoch 202 loss 2.303746223449707\n",
            "epoch 203 loss 2.303746223449707\n",
            "epoch 204 loss 2.303746223449707\n",
            "epoch 205 loss 2.303746223449707\n",
            "epoch 206 loss 2.303746223449707\n",
            "epoch 207 loss 2.303746223449707\n",
            "epoch 208 loss 2.303746223449707\n",
            "epoch 209 loss 2.303746223449707\n",
            "epoch 210 loss 2.303746223449707\n",
            "epoch 211 loss 2.303746223449707\n",
            "epoch 212 loss 2.303746223449707\n",
            "epoch 213 loss 2.303746223449707\n",
            "epoch 214 loss 2.303746223449707\n",
            "epoch 215 loss 2.303746223449707\n",
            "epoch 216 loss 2.303746223449707\n",
            "epoch 217 loss 2.303746223449707\n",
            "epoch 218 loss 2.303746223449707\n",
            "epoch 219 loss 2.303746223449707\n",
            "epoch 220 loss 2.303746223449707\n",
            "epoch 221 loss 2.303746223449707\n",
            "epoch 222 loss 2.303746223449707\n",
            "epoch 223 loss 2.303746223449707\n",
            "epoch 224 loss 2.303746223449707\n",
            "epoch 225 loss 2.303746223449707\n",
            "epoch 226 loss 2.303746223449707\n",
            "epoch 227 loss 2.303746223449707\n",
            "epoch 228 loss 2.303746223449707\n",
            "epoch 229 loss 2.303746223449707\n",
            "epoch 230 loss 2.303746223449707\n",
            "epoch 231 loss 2.303746223449707\n",
            "epoch 232 loss 2.303746223449707\n",
            "epoch 233 loss 2.303746223449707\n",
            "epoch 234 loss 2.303746223449707\n",
            "epoch 235 loss 2.303746223449707\n",
            "epoch 236 loss 2.303746223449707\n",
            "epoch 237 loss 2.303746223449707\n",
            "epoch 238 loss 2.303746223449707\n",
            "epoch 239 loss 2.303746223449707\n",
            "epoch 240 loss 2.303746223449707\n",
            "epoch 241 loss 2.303746223449707\n",
            "epoch 242 loss 2.303746223449707\n",
            "epoch 243 loss 2.303746223449707\n",
            "epoch 244 loss 2.303746223449707\n",
            "epoch 245 loss 2.303746223449707\n",
            "epoch 246 loss 2.303746223449707\n",
            "epoch 247 loss 2.303746223449707\n",
            "epoch 248 loss 2.303746223449707\n",
            "epoch 249 loss 2.303746223449707\n",
            "epoch 250 loss 2.303746223449707\n",
            "epoch 251 loss 2.303746223449707\n",
            "epoch 252 loss 2.303746223449707\n",
            "epoch 253 loss 2.303746223449707\n",
            "epoch 254 loss 2.303746223449707\n",
            "epoch 255 loss 2.303746223449707\n",
            "epoch 256 loss 2.303746223449707\n",
            "epoch 257 loss 2.303746223449707\n",
            "epoch 258 loss 2.303746223449707\n",
            "epoch 259 loss 2.303746223449707\n",
            "epoch 260 loss 2.303746223449707\n",
            "epoch 261 loss 2.303746223449707\n",
            "epoch 262 loss 2.303746223449707\n",
            "epoch 263 loss 2.303746223449707\n",
            "epoch 264 loss 2.303746223449707\n",
            "epoch 265 loss 2.303746223449707\n",
            "epoch 266 loss 2.303746223449707\n",
            "epoch 267 loss 2.303746223449707\n",
            "epoch 268 loss 2.303746223449707\n",
            "epoch 269 loss 2.303746223449707\n",
            "epoch 270 loss 2.303746223449707\n",
            "epoch 271 loss 2.303746223449707\n",
            "epoch 272 loss 2.303746223449707\n",
            "epoch 273 loss 2.303746223449707\n",
            "epoch 274 loss 2.303746223449707\n",
            "epoch 275 loss 2.303746223449707\n",
            "epoch 276 loss 2.303746223449707\n",
            "epoch 277 loss 2.303746223449707\n",
            "epoch 278 loss 2.303746223449707\n",
            "epoch 279 loss 2.303746223449707\n",
            "epoch 280 loss 2.303746223449707\n",
            "epoch 281 loss 2.303746223449707\n",
            "epoch 282 loss 2.303746223449707\n",
            "epoch 283 loss 2.303746223449707\n",
            "epoch 284 loss 2.303746223449707\n",
            "epoch 285 loss 2.303746223449707\n",
            "epoch 286 loss 2.303746223449707\n",
            "epoch 287 loss 2.303746223449707\n",
            "epoch 288 loss 2.303746223449707\n",
            "epoch 289 loss 2.303746223449707\n",
            "epoch 290 loss 2.303746223449707\n",
            "epoch 291 loss 2.303746223449707\n",
            "epoch 292 loss 2.303746223449707\n",
            "epoch 293 loss 2.303746223449707\n",
            "epoch 294 loss 2.303746223449707\n",
            "epoch 295 loss 2.303746223449707\n",
            "epoch 296 loss 2.303746223449707\n",
            "epoch 297 loss 2.303746223449707\n",
            "epoch 298 loss 2.303746223449707\n",
            "epoch 299 loss 2.303746223449707\n",
            "epoch 300 loss 2.303746223449707\n",
            "epoch 301 loss 2.303746223449707\n",
            "epoch 302 loss 2.303746223449707\n",
            "epoch 303 loss 2.303746223449707\n",
            "epoch 304 loss 2.303746223449707\n",
            "epoch 305 loss 2.303746223449707\n",
            "epoch 306 loss 2.303746223449707\n",
            "epoch 307 loss 2.303746223449707\n",
            "epoch 308 loss 2.303746223449707\n",
            "epoch 309 loss 2.303746223449707\n",
            "epoch 310 loss 2.303746223449707\n",
            "epoch 311 loss 2.303746223449707\n",
            "epoch 312 loss 2.303746223449707\n",
            "epoch 313 loss 2.303746223449707\n",
            "epoch 314 loss 2.303746223449707\n",
            "epoch 315 loss 2.303746223449707\n",
            "epoch 316 loss 2.303746223449707\n",
            "epoch 317 loss 2.303746223449707\n",
            "epoch 318 loss 2.303746223449707\n",
            "epoch 319 loss 2.303746223449707\n",
            "epoch 320 loss 2.303746223449707\n",
            "epoch 321 loss 2.303746223449707\n",
            "epoch 322 loss 2.303746223449707\n",
            "epoch 323 loss 2.303746223449707\n",
            "epoch 324 loss 2.303746223449707\n",
            "epoch 325 loss 2.303746223449707\n",
            "epoch 326 loss 2.303746223449707\n",
            "epoch 327 loss 2.303746223449707\n",
            "epoch 328 loss 2.303746223449707\n",
            "epoch 329 loss 2.303746223449707\n",
            "epoch 330 loss 2.303746223449707\n",
            "epoch 331 loss 2.303746223449707\n",
            "epoch 332 loss 2.303746223449707\n",
            "epoch 333 loss 2.303746223449707\n",
            "epoch 334 loss 2.303746223449707\n",
            "epoch 335 loss 2.303746223449707\n",
            "epoch 336 loss 2.303746223449707\n",
            "epoch 337 loss 2.303746223449707\n",
            "epoch 338 loss 2.303746223449707\n",
            "epoch 339 loss 2.303746223449707\n",
            "epoch 340 loss 2.303746223449707\n",
            "epoch 341 loss 2.303746223449707\n",
            "epoch 342 loss 2.303746223449707\n",
            "epoch 343 loss 2.303746223449707\n",
            "epoch 344 loss 2.303746223449707\n",
            "epoch 345 loss 2.303746223449707\n",
            "epoch 346 loss 2.303746223449707\n",
            "epoch 347 loss 2.303746223449707\n",
            "epoch 348 loss 2.303746223449707\n",
            "epoch 349 loss 2.303746223449707\n",
            "epoch 350 loss 2.303746223449707\n",
            "epoch 351 loss 2.303746223449707\n",
            "epoch 352 loss 2.303746223449707\n",
            "epoch 353 loss 2.303746223449707\n",
            "epoch 354 loss 2.303746223449707\n",
            "epoch 355 loss 2.303746223449707\n",
            "epoch 356 loss 2.303746223449707\n",
            "epoch 357 loss 2.303746223449707\n",
            "epoch 358 loss 2.303746223449707\n",
            "epoch 359 loss 2.303746223449707\n",
            "epoch 360 loss 2.303746223449707\n",
            "epoch 361 loss 2.303746223449707\n",
            "epoch 362 loss 2.303746223449707\n",
            "epoch 363 loss 2.303746223449707\n",
            "epoch 364 loss 2.303746223449707\n",
            "epoch 365 loss 2.303746223449707\n",
            "epoch 366 loss 2.303746223449707\n",
            "epoch 367 loss 2.303746223449707\n",
            "epoch 368 loss 2.303746223449707\n",
            "epoch 369 loss 2.303746223449707\n",
            "epoch 370 loss 2.303746223449707\n",
            "epoch 371 loss 2.303746223449707\n",
            "epoch 372 loss 2.303746223449707\n",
            "epoch 373 loss 2.303746223449707\n",
            "epoch 374 loss 2.303746223449707\n",
            "epoch 375 loss 2.303746223449707\n",
            "epoch 376 loss 2.303746223449707\n",
            "epoch 377 loss 2.303746223449707\n",
            "epoch 378 loss 2.303746223449707\n",
            "epoch 379 loss 2.303746223449707\n",
            "epoch 380 loss 2.303746223449707\n",
            "epoch 381 loss 2.303746223449707\n",
            "epoch 382 loss 2.303746223449707\n",
            "epoch 383 loss 2.303746223449707\n",
            "epoch 384 loss 2.303746223449707\n",
            "epoch 385 loss 2.303746223449707\n",
            "epoch 386 loss 2.303746223449707\n",
            "epoch 387 loss 2.303746223449707\n",
            "epoch 388 loss 2.303746223449707\n",
            "epoch 389 loss 2.303746223449707\n",
            "epoch 390 loss 2.303746223449707\n",
            "epoch 391 loss 2.303746223449707\n",
            "epoch 392 loss 2.303746223449707\n",
            "epoch 393 loss 2.303746223449707\n",
            "epoch 394 loss 2.303746223449707\n",
            "epoch 395 loss 2.303746223449707\n",
            "epoch 396 loss 2.303746223449707\n",
            "epoch 397 loss 2.303746223449707\n",
            "epoch 398 loss 2.303746223449707\n",
            "epoch 399 loss 2.303746223449707\n",
            "epoch 400 loss 2.303746223449707\n",
            "epoch 401 loss 2.303746223449707\n",
            "epoch 402 loss 2.303746223449707\n",
            "epoch 403 loss 2.303746223449707\n",
            "epoch 404 loss 2.303746223449707\n",
            "epoch 405 loss 2.303746223449707\n",
            "epoch 406 loss 2.303746223449707\n",
            "epoch 407 loss 2.303746223449707\n",
            "epoch 408 loss 2.303746223449707\n",
            "epoch 409 loss 2.303746223449707\n",
            "epoch 410 loss 2.303746223449707\n",
            "epoch 411 loss 2.303746223449707\n",
            "epoch 412 loss 2.303746223449707\n",
            "epoch 413 loss 2.303746223449707\n",
            "epoch 414 loss 2.303746223449707\n",
            "epoch 415 loss 2.303746223449707\n",
            "epoch 416 loss 2.303746223449707\n",
            "epoch 417 loss 2.303746223449707\n",
            "epoch 418 loss 2.303746223449707\n",
            "epoch 419 loss 2.303746223449707\n",
            "epoch 420 loss 2.303746223449707\n",
            "epoch 421 loss 2.303746223449707\n",
            "epoch 422 loss 2.303746223449707\n",
            "epoch 423 loss 2.303746223449707\n",
            "epoch 424 loss 2.303746223449707\n",
            "epoch 425 loss 2.303746223449707\n",
            "epoch 426 loss 2.303746223449707\n",
            "epoch 427 loss 2.303746223449707\n",
            "epoch 428 loss 2.303746223449707\n",
            "epoch 429 loss 2.303746223449707\n",
            "epoch 430 loss 2.303746223449707\n",
            "epoch 431 loss 2.303746223449707\n",
            "epoch 432 loss 2.303746223449707\n",
            "epoch 433 loss 2.303746223449707\n",
            "epoch 434 loss 2.303746223449707\n",
            "epoch 435 loss 2.303746223449707\n",
            "epoch 436 loss 2.303746223449707\n",
            "epoch 437 loss 2.303746223449707\n",
            "epoch 438 loss 2.303746223449707\n",
            "epoch 439 loss 2.303746223449707\n",
            "epoch 440 loss 2.303746223449707\n",
            "epoch 441 loss 2.303746223449707\n",
            "epoch 442 loss 2.303746223449707\n",
            "epoch 443 loss 2.303746223449707\n",
            "epoch 444 loss 2.303746223449707\n",
            "epoch 445 loss 2.303746223449707\n",
            "epoch 446 loss 2.303746223449707\n",
            "epoch 447 loss 2.303746223449707\n",
            "epoch 448 loss 2.303746223449707\n",
            "epoch 449 loss 2.303746223449707\n",
            "epoch 450 loss 2.303746223449707\n",
            "epoch 451 loss 2.303746223449707\n",
            "epoch 452 loss 2.303746223449707\n",
            "epoch 453 loss 2.303746223449707\n",
            "epoch 454 loss 2.303746223449707\n",
            "epoch 455 loss 2.303746223449707\n",
            "epoch 456 loss 2.303746223449707\n",
            "epoch 457 loss 2.303746223449707\n",
            "epoch 458 loss 2.303746223449707\n",
            "epoch 459 loss 2.303746223449707\n",
            "epoch 460 loss 2.303746223449707\n",
            "epoch 461 loss 2.303746223449707\n",
            "epoch 462 loss 2.303746223449707\n",
            "epoch 463 loss 2.303746223449707\n",
            "epoch 464 loss 2.303746223449707\n",
            "epoch 465 loss 2.303746223449707\n",
            "epoch 466 loss 2.303746223449707\n",
            "epoch 467 loss 2.303746223449707\n",
            "epoch 468 loss 2.303746223449707\n",
            "epoch 469 loss 2.303746223449707\n",
            "epoch 470 loss 2.303746223449707\n",
            "epoch 471 loss 2.303746223449707\n",
            "epoch 472 loss 2.303746223449707\n",
            "epoch 473 loss 2.303746223449707\n",
            "epoch 474 loss 2.303746223449707\n",
            "epoch 475 loss 2.303746223449707\n",
            "epoch 476 loss 2.303746223449707\n",
            "epoch 477 loss 2.303746223449707\n",
            "epoch 478 loss 2.303746223449707\n",
            "epoch 479 loss 2.303746223449707\n",
            "epoch 480 loss 2.303746223449707\n",
            "epoch 481 loss 2.303746223449707\n",
            "epoch 482 loss 2.303746223449707\n",
            "epoch 483 loss 2.303746223449707\n",
            "epoch 484 loss 2.303746223449707\n",
            "epoch 485 loss 2.303746223449707\n",
            "epoch 486 loss 2.303746223449707\n",
            "epoch 487 loss 2.303746223449707\n",
            "epoch 488 loss 2.303746223449707\n",
            "epoch 489 loss 2.303746223449707\n",
            "epoch 490 loss 2.303746223449707\n",
            "epoch 491 loss 2.303746223449707\n",
            "epoch 492 loss 2.303746223449707\n",
            "epoch 493 loss 2.303746223449707\n",
            "epoch 494 loss 2.303746223449707\n",
            "epoch 495 loss 2.303746223449707\n",
            "epoch 496 loss 2.303746223449707\n",
            "epoch 497 loss 2.303746223449707\n",
            "epoch 498 loss 2.303746223449707\n",
            "epoch 499 loss 2.303746223449707\n",
            "epoch 500 loss 2.303746223449707\n",
            "epoch 501 loss 2.303746223449707\n",
            "epoch 502 loss 2.303746223449707\n",
            "epoch 503 loss 2.303746223449707\n",
            "epoch 504 loss 2.303746223449707\n",
            "epoch 505 loss 2.303746223449707\n",
            "epoch 506 loss 2.303746223449707\n",
            "epoch 507 loss 2.303746223449707\n",
            "epoch 508 loss 2.303746223449707\n",
            "epoch 509 loss 2.303746223449707\n",
            "epoch 510 loss 2.303746223449707\n",
            "epoch 511 loss 2.303746223449707\n",
            "epoch 512 loss 2.303746223449707\n",
            "epoch 513 loss 2.303746223449707\n",
            "epoch 514 loss 2.303746223449707\n",
            "epoch 515 loss 2.303746223449707\n",
            "epoch 516 loss 2.303746223449707\n",
            "epoch 517 loss 2.303746223449707\n",
            "epoch 518 loss 2.303746223449707\n",
            "epoch 519 loss 2.303746223449707\n",
            "epoch 520 loss 2.303746223449707\n",
            "epoch 521 loss 2.303746223449707\n",
            "epoch 522 loss 2.303746223449707\n",
            "epoch 523 loss 2.303746223449707\n",
            "epoch 524 loss 2.303746223449707\n",
            "epoch 525 loss 2.303746223449707\n",
            "epoch 526 loss 2.303746223449707\n",
            "epoch 527 loss 2.303746223449707\n",
            "epoch 528 loss 2.303746223449707\n",
            "epoch 529 loss 2.303746223449707\n",
            "epoch 530 loss 2.303746223449707\n",
            "epoch 531 loss 2.303746223449707\n",
            "epoch 532 loss 2.303746223449707\n",
            "epoch 533 loss 2.303746223449707\n",
            "epoch 534 loss 2.303746223449707\n",
            "epoch 535 loss 2.303746223449707\n",
            "epoch 536 loss 2.303746223449707\n",
            "epoch 537 loss 2.303746223449707\n",
            "epoch 538 loss 2.303746223449707\n",
            "epoch 539 loss 2.303746223449707\n",
            "epoch 540 loss 2.303746223449707\n",
            "epoch 541 loss 2.303746223449707\n",
            "epoch 542 loss 2.303746223449707\n",
            "epoch 543 loss 2.303746223449707\n",
            "epoch 544 loss 2.303746223449707\n",
            "epoch 545 loss 2.303746223449707\n",
            "epoch 546 loss 2.303746223449707\n",
            "epoch 547 loss 2.303746223449707\n",
            "epoch 548 loss 2.303746223449707\n",
            "epoch 549 loss 2.303746223449707\n",
            "epoch 550 loss 2.303746223449707\n",
            "epoch 551 loss 2.303746223449707\n",
            "epoch 552 loss 2.303746223449707\n",
            "epoch 553 loss 2.303746223449707\n",
            "epoch 554 loss 2.303746223449707\n",
            "epoch 555 loss 2.303746223449707\n",
            "epoch 556 loss 2.303746223449707\n",
            "epoch 557 loss 2.303746223449707\n",
            "epoch 558 loss 2.303746223449707\n",
            "epoch 559 loss 2.303746223449707\n",
            "epoch 560 loss 2.303746223449707\n",
            "epoch 561 loss 2.303746223449707\n",
            "epoch 562 loss 2.303746223449707\n",
            "epoch 563 loss 2.303746223449707\n",
            "epoch 564 loss 2.303746223449707\n",
            "epoch 565 loss 2.303746223449707\n",
            "epoch 566 loss 2.303746223449707\n",
            "epoch 567 loss 2.303746223449707\n",
            "epoch 568 loss 2.303746223449707\n",
            "epoch 569 loss 2.303746223449707\n",
            "epoch 570 loss 2.303746223449707\n",
            "epoch 571 loss 2.303746223449707\n",
            "epoch 572 loss 2.303746223449707\n",
            "epoch 573 loss 2.303746223449707\n",
            "epoch 574 loss 2.303746223449707\n",
            "epoch 575 loss 2.303746223449707\n",
            "epoch 576 loss 2.303746223449707\n",
            "epoch 577 loss 2.303746223449707\n",
            "epoch 578 loss 2.303746223449707\n",
            "epoch 579 loss 2.303746223449707\n",
            "epoch 580 loss 2.303746223449707\n",
            "epoch 581 loss 2.303746223449707\n",
            "epoch 582 loss 2.303746223449707\n",
            "epoch 583 loss 2.303746223449707\n",
            "epoch 584 loss 2.303746223449707\n",
            "epoch 585 loss 2.303746223449707\n",
            "epoch 586 loss 2.303746223449707\n",
            "epoch 587 loss 2.303746223449707\n",
            "epoch 588 loss 2.303746223449707\n",
            "epoch 589 loss 2.303746223449707\n",
            "epoch 590 loss 2.303746223449707\n",
            "epoch 591 loss 2.303746223449707\n",
            "epoch 592 loss 2.303746223449707\n",
            "epoch 593 loss 2.303746223449707\n",
            "epoch 594 loss 2.303746223449707\n",
            "epoch 595 loss 2.303746223449707\n",
            "epoch 596 loss 2.303746223449707\n",
            "epoch 597 loss 2.303746223449707\n",
            "epoch 598 loss 2.303746223449707\n",
            "epoch 599 loss 2.303746223449707\n",
            "epoch 600 loss 2.303746223449707\n",
            "epoch 601 loss 2.303746223449707\n",
            "epoch 602 loss 2.303746223449707\n",
            "epoch 603 loss 2.303746223449707\n",
            "epoch 604 loss 2.303746223449707\n",
            "epoch 605 loss 2.303746223449707\n",
            "epoch 606 loss 2.303746223449707\n",
            "epoch 607 loss 2.303746223449707\n",
            "epoch 608 loss 2.303746223449707\n",
            "epoch 609 loss 2.303746223449707\n",
            "epoch 610 loss 2.303746223449707\n",
            "epoch 611 loss 2.303746223449707\n",
            "epoch 612 loss 2.303746223449707\n",
            "epoch 613 loss 2.303746223449707\n",
            "epoch 614 loss 2.303746223449707\n",
            "epoch 615 loss 2.303746223449707\n",
            "epoch 616 loss 2.303746223449707\n",
            "epoch 617 loss 2.303746223449707\n",
            "epoch 618 loss 2.303746223449707\n",
            "epoch 619 loss 2.303746223449707\n",
            "epoch 620 loss 2.303746223449707\n",
            "epoch 621 loss 2.303746223449707\n",
            "epoch 622 loss 2.303746223449707\n",
            "epoch 623 loss 2.303746223449707\n",
            "epoch 624 loss 2.303746223449707\n",
            "epoch 625 loss 2.303746223449707\n",
            "epoch 626 loss 2.303746223449707\n",
            "epoch 627 loss 2.303746223449707\n",
            "epoch 628 loss 2.303746223449707\n",
            "epoch 629 loss 2.303746223449707\n",
            "epoch 630 loss 2.303746223449707\n",
            "epoch 631 loss 2.303746223449707\n",
            "epoch 632 loss 2.303746223449707\n",
            "epoch 633 loss 2.303746223449707\n",
            "epoch 634 loss 2.303746223449707\n",
            "epoch 635 loss 2.303746223449707\n",
            "epoch 636 loss 2.303746223449707\n",
            "epoch 637 loss 2.303746223449707\n",
            "epoch 638 loss 2.303746223449707\n",
            "epoch 639 loss 2.303746223449707\n",
            "epoch 640 loss 2.303746223449707\n",
            "epoch 641 loss 2.303746223449707\n",
            "epoch 642 loss 2.303746223449707\n",
            "epoch 643 loss 2.303746223449707\n",
            "epoch 644 loss 2.303746223449707\n",
            "epoch 645 loss 2.303746223449707\n",
            "epoch 646 loss 2.303746223449707\n",
            "epoch 647 loss 2.303746223449707\n",
            "epoch 648 loss 2.303746223449707\n",
            "epoch 649 loss 2.303746223449707\n",
            "epoch 650 loss 2.303746223449707\n",
            "epoch 651 loss 2.303746223449707\n",
            "epoch 652 loss 2.303746223449707\n",
            "epoch 653 loss 2.303746223449707\n",
            "epoch 654 loss 2.303746223449707\n",
            "epoch 655 loss 2.303746223449707\n",
            "epoch 656 loss 2.303746223449707\n",
            "epoch 657 loss 2.303746223449707\n",
            "epoch 658 loss 2.303746223449707\n",
            "epoch 659 loss 2.303746223449707\n",
            "epoch 660 loss 2.303746223449707\n",
            "epoch 661 loss 2.303746223449707\n",
            "epoch 662 loss 2.303746223449707\n",
            "epoch 663 loss 2.303746223449707\n",
            "epoch 664 loss 2.303746223449707\n",
            "epoch 665 loss 2.303746223449707\n",
            "epoch 666 loss 2.303746223449707\n",
            "epoch 667 loss 2.303746223449707\n",
            "epoch 668 loss 2.303746223449707\n",
            "epoch 669 loss 2.303746223449707\n",
            "epoch 670 loss 2.303746223449707\n",
            "epoch 671 loss 2.303746223449707\n",
            "epoch 672 loss 2.303746223449707\n",
            "epoch 673 loss 2.303746223449707\n",
            "epoch 674 loss 2.303746223449707\n",
            "epoch 675 loss 2.303746223449707\n",
            "epoch 676 loss 2.303746223449707\n",
            "epoch 677 loss 2.303746223449707\n",
            "epoch 678 loss 2.303746223449707\n",
            "epoch 679 loss 2.303746223449707\n",
            "epoch 680 loss 2.303746223449707\n",
            "epoch 681 loss 2.303746223449707\n",
            "epoch 682 loss 2.303746223449707\n",
            "epoch 683 loss 2.303746223449707\n",
            "epoch 684 loss 2.303746223449707\n",
            "epoch 685 loss 2.303746223449707\n",
            "epoch 686 loss 2.303746223449707\n",
            "epoch 687 loss 2.303746223449707\n",
            "epoch 688 loss 2.303746223449707\n",
            "epoch 689 loss 2.303746223449707\n",
            "epoch 690 loss 2.303746223449707\n",
            "epoch 691 loss 2.303746223449707\n",
            "epoch 692 loss 2.303746223449707\n",
            "epoch 693 loss 2.303746223449707\n",
            "epoch 694 loss 2.303746223449707\n",
            "epoch 695 loss 2.303746223449707\n",
            "epoch 696 loss 2.303746223449707\n",
            "epoch 697 loss 2.303746223449707\n",
            "epoch 698 loss 2.303746223449707\n",
            "epoch 699 loss 2.303746223449707\n",
            "epoch 700 loss 2.303746223449707\n",
            "epoch 701 loss 2.303746223449707\n",
            "epoch 702 loss 2.303746223449707\n",
            "epoch 703 loss 2.303746223449707\n",
            "epoch 704 loss 2.303746223449707\n",
            "epoch 705 loss 2.303746223449707\n",
            "epoch 706 loss 2.303746223449707\n",
            "epoch 707 loss 2.303746223449707\n",
            "epoch 708 loss 2.303746223449707\n",
            "epoch 709 loss 2.303746223449707\n",
            "epoch 710 loss 2.303746223449707\n",
            "epoch 711 loss 2.303746223449707\n",
            "epoch 712 loss 2.303746223449707\n",
            "epoch 713 loss 2.303746223449707\n",
            "epoch 714 loss 2.303746223449707\n",
            "epoch 715 loss 2.303746223449707\n",
            "epoch 716 loss 2.303746223449707\n",
            "epoch 717 loss 2.303746223449707\n",
            "epoch 718 loss 2.303746223449707\n",
            "epoch 719 loss 2.303746223449707\n",
            "epoch 720 loss 2.303746223449707\n",
            "epoch 721 loss 2.303746223449707\n",
            "epoch 722 loss 2.303746223449707\n",
            "epoch 723 loss 2.303746223449707\n",
            "epoch 724 loss 2.303746223449707\n",
            "epoch 725 loss 2.303746223449707\n",
            "epoch 726 loss 2.303746223449707\n",
            "epoch 727 loss 2.303746223449707\n",
            "epoch 728 loss 2.303746223449707\n",
            "epoch 729 loss 2.303746223449707\n",
            "epoch 730 loss 2.303746223449707\n",
            "epoch 731 loss 2.303746223449707\n",
            "epoch 732 loss 2.303746223449707\n",
            "epoch 733 loss 2.303746223449707\n",
            "epoch 734 loss 2.303746223449707\n",
            "epoch 735 loss 2.303746223449707\n",
            "epoch 736 loss 2.303746223449707\n",
            "epoch 737 loss 2.303746223449707\n",
            "epoch 738 loss 2.303746223449707\n",
            "epoch 739 loss 2.303746223449707\n",
            "epoch 740 loss 2.303746223449707\n",
            "epoch 741 loss 2.303746223449707\n",
            "epoch 742 loss 2.303746223449707\n",
            "epoch 743 loss 2.303746223449707\n",
            "epoch 744 loss 2.303746223449707\n",
            "epoch 745 loss 2.303746223449707\n",
            "epoch 746 loss 2.303746223449707\n",
            "epoch 747 loss 2.303746223449707\n",
            "epoch 748 loss 2.303746223449707\n",
            "epoch 749 loss 2.303746223449707\n",
            "epoch 750 loss 2.303746223449707\n",
            "epoch 751 loss 2.303746223449707\n",
            "epoch 752 loss 2.303746223449707\n",
            "epoch 753 loss 2.303746223449707\n",
            "epoch 754 loss 2.303746223449707\n",
            "epoch 755 loss 2.303746223449707\n",
            "epoch 756 loss 2.303746223449707\n",
            "epoch 757 loss 2.303746223449707\n",
            "epoch 758 loss 2.303746223449707\n",
            "epoch 759 loss 2.303746223449707\n",
            "epoch 760 loss 2.303746223449707\n",
            "epoch 761 loss 2.303746223449707\n",
            "epoch 762 loss 2.303746223449707\n",
            "epoch 763 loss 2.303746223449707\n",
            "epoch 764 loss 2.303746223449707\n",
            "epoch 765 loss 2.303746223449707\n",
            "epoch 766 loss 2.303746223449707\n",
            "epoch 767 loss 2.303746223449707\n",
            "epoch 768 loss 2.303746223449707\n",
            "epoch 769 loss 2.303746223449707\n",
            "epoch 770 loss 2.303746223449707\n",
            "epoch 771 loss 2.303746223449707\n",
            "epoch 772 loss 2.303746223449707\n",
            "epoch 773 loss 2.303746223449707\n",
            "epoch 774 loss 2.303746223449707\n",
            "epoch 775 loss 2.303746223449707\n",
            "epoch 776 loss 2.303746223449707\n",
            "epoch 777 loss 2.303746223449707\n",
            "epoch 778 loss 2.303746223449707\n",
            "epoch 779 loss 2.303746223449707\n",
            "epoch 780 loss 2.303746223449707\n",
            "epoch 781 loss 2.303746223449707\n",
            "epoch 782 loss 2.303746223449707\n",
            "epoch 783 loss 2.303746223449707\n",
            "epoch 784 loss 2.303746223449707\n",
            "epoch 785 loss 2.303746223449707\n",
            "epoch 786 loss 2.303746223449707\n",
            "epoch 787 loss 2.303746223449707\n",
            "epoch 788 loss 2.303746223449707\n",
            "epoch 789 loss 2.303746223449707\n",
            "epoch 790 loss 2.303746223449707\n",
            "epoch 791 loss 2.303746223449707\n",
            "epoch 792 loss 2.303746223449707\n",
            "epoch 793 loss 2.303746223449707\n",
            "epoch 794 loss 2.303746223449707\n",
            "epoch 795 loss 2.303746223449707\n",
            "epoch 796 loss 2.303746223449707\n",
            "epoch 797 loss 2.303746223449707\n",
            "epoch 798 loss 2.303746223449707\n",
            "epoch 799 loss 2.303746223449707\n",
            "epoch 800 loss 2.303746223449707\n",
            "epoch 801 loss 2.303746223449707\n",
            "epoch 802 loss 2.303746223449707\n",
            "epoch 803 loss 2.303746223449707\n",
            "epoch 804 loss 2.303746223449707\n",
            "epoch 805 loss 2.303746223449707\n",
            "epoch 806 loss 2.303746223449707\n",
            "epoch 807 loss 2.303746223449707\n",
            "epoch 808 loss 2.303746223449707\n",
            "epoch 809 loss 2.303746223449707\n",
            "epoch 810 loss 2.303746223449707\n",
            "epoch 811 loss 2.303746223449707\n",
            "epoch 812 loss 2.303746223449707\n",
            "epoch 813 loss 2.303746223449707\n",
            "epoch 814 loss 2.303746223449707\n",
            "epoch 815 loss 2.303746223449707\n",
            "epoch 816 loss 2.303746223449707\n",
            "epoch 817 loss 2.303746223449707\n",
            "epoch 818 loss 2.303746223449707\n",
            "epoch 819 loss 2.303746223449707\n",
            "epoch 820 loss 2.303746223449707\n",
            "epoch 821 loss 2.303746223449707\n",
            "epoch 822 loss 2.303746223449707\n",
            "epoch 823 loss 2.303746223449707\n",
            "epoch 824 loss 2.303746223449707\n",
            "epoch 825 loss 2.303746223449707\n",
            "epoch 826 loss 2.303746223449707\n",
            "epoch 827 loss 2.303746223449707\n",
            "epoch 828 loss 2.303746223449707\n",
            "epoch 829 loss 2.303746223449707\n",
            "epoch 830 loss 2.303746223449707\n",
            "epoch 831 loss 2.303746223449707\n",
            "epoch 832 loss 2.303746223449707\n",
            "epoch 833 loss 2.303746223449707\n",
            "epoch 834 loss 2.303746223449707\n",
            "epoch 835 loss 2.303746223449707\n",
            "epoch 836 loss 2.303746223449707\n",
            "epoch 837 loss 2.303746223449707\n",
            "epoch 838 loss 2.303746223449707\n",
            "epoch 839 loss 2.303746223449707\n",
            "epoch 840 loss 2.303746223449707\n",
            "epoch 841 loss 2.303746223449707\n",
            "epoch 842 loss 2.303746223449707\n",
            "epoch 843 loss 2.303746223449707\n",
            "epoch 844 loss 2.303746223449707\n",
            "epoch 845 loss 2.303746223449707\n",
            "epoch 846 loss 2.303746223449707\n",
            "epoch 847 loss 2.303746223449707\n",
            "epoch 848 loss 2.303746223449707\n",
            "epoch 849 loss 2.303746223449707\n",
            "epoch 850 loss 2.303746223449707\n",
            "epoch 851 loss 2.303746223449707\n",
            "epoch 852 loss 2.303746223449707\n",
            "epoch 853 loss 2.303746223449707\n",
            "epoch 854 loss 2.303746223449707\n",
            "epoch 855 loss 2.303746223449707\n",
            "epoch 856 loss 2.303746223449707\n",
            "epoch 857 loss 2.303746223449707\n",
            "epoch 858 loss 2.303746223449707\n",
            "epoch 859 loss 2.303746223449707\n",
            "epoch 860 loss 2.303746223449707\n",
            "epoch 861 loss 2.303746223449707\n",
            "epoch 862 loss 2.303746223449707\n",
            "epoch 863 loss 2.303746223449707\n",
            "epoch 864 loss 2.303746223449707\n",
            "epoch 865 loss 2.303746223449707\n",
            "epoch 866 loss 2.303746223449707\n",
            "epoch 867 loss 2.303746223449707\n",
            "epoch 868 loss 2.303746223449707\n",
            "epoch 869 loss 2.303746223449707\n",
            "epoch 870 loss 2.303746223449707\n",
            "epoch 871 loss 2.303746223449707\n",
            "epoch 872 loss 2.303746223449707\n",
            "epoch 873 loss 2.303746223449707\n",
            "epoch 874 loss 2.303746223449707\n",
            "epoch 875 loss 2.303746223449707\n",
            "epoch 876 loss 2.303746223449707\n",
            "epoch 877 loss 2.303746223449707\n",
            "epoch 878 loss 2.303746223449707\n",
            "epoch 879 loss 2.303746223449707\n",
            "epoch 880 loss 2.303746223449707\n",
            "epoch 881 loss 2.303746223449707\n",
            "epoch 882 loss 2.303746223449707\n",
            "epoch 883 loss 2.303746223449707\n",
            "epoch 884 loss 2.303746223449707\n",
            "epoch 885 loss 2.303746223449707\n",
            "epoch 886 loss 2.303746223449707\n",
            "epoch 887 loss 2.303746223449707\n",
            "epoch 888 loss 2.303746223449707\n",
            "epoch 889 loss 2.303746223449707\n",
            "epoch 890 loss 2.303746223449707\n",
            "epoch 891 loss 2.303746223449707\n",
            "epoch 892 loss 2.303746223449707\n",
            "epoch 893 loss 2.303746223449707\n",
            "epoch 894 loss 2.303746223449707\n",
            "epoch 895 loss 2.303746223449707\n",
            "epoch 896 loss 2.303746223449707\n",
            "epoch 897 loss 2.303746223449707\n",
            "epoch 898 loss 2.303746223449707\n",
            "epoch 899 loss 2.303746223449707\n",
            "epoch 900 loss 2.303746223449707\n",
            "epoch 901 loss 2.303746223449707\n",
            "epoch 902 loss 2.303746223449707\n",
            "epoch 903 loss 2.303746223449707\n",
            "epoch 904 loss 2.303746223449707\n",
            "epoch 905 loss 2.303746223449707\n",
            "epoch 906 loss 2.303746223449707\n",
            "epoch 907 loss 2.303746223449707\n",
            "epoch 908 loss 2.303746223449707\n",
            "epoch 909 loss 2.303746223449707\n",
            "epoch 910 loss 2.303746223449707\n",
            "epoch 911 loss 2.303746223449707\n",
            "epoch 912 loss 2.303746223449707\n",
            "epoch 913 loss 2.303746223449707\n",
            "epoch 914 loss 2.303746223449707\n",
            "epoch 915 loss 2.303746223449707\n",
            "epoch 916 loss 2.303746223449707\n",
            "epoch 917 loss 2.303746223449707\n",
            "epoch 918 loss 2.303746223449707\n",
            "epoch 919 loss 2.303746223449707\n",
            "epoch 920 loss 2.303746223449707\n",
            "epoch 921 loss 2.303746223449707\n",
            "epoch 922 loss 2.303746223449707\n",
            "epoch 923 loss 2.303746223449707\n",
            "epoch 924 loss 2.303746223449707\n",
            "epoch 925 loss 2.303746223449707\n",
            "epoch 926 loss 2.303746223449707\n",
            "epoch 927 loss 2.303746223449707\n",
            "epoch 928 loss 2.303746223449707\n",
            "epoch 929 loss 2.303746223449707\n",
            "epoch 930 loss 2.303746223449707\n",
            "epoch 931 loss 2.303746223449707\n",
            "epoch 932 loss 2.303746223449707\n",
            "epoch 933 loss 2.303746223449707\n",
            "epoch 934 loss 2.303746223449707\n",
            "epoch 935 loss 2.303746223449707\n",
            "epoch 936 loss 2.303746223449707\n",
            "epoch 937 loss 2.303746223449707\n",
            "epoch 938 loss 2.303746223449707\n",
            "epoch 939 loss 2.303746223449707\n",
            "epoch 940 loss 2.303746223449707\n",
            "epoch 941 loss 2.303746223449707\n",
            "epoch 942 loss 2.303746223449707\n",
            "epoch 943 loss 2.303746223449707\n",
            "epoch 944 loss 2.303746223449707\n",
            "epoch 945 loss 2.303746223449707\n",
            "epoch 946 loss 2.303746223449707\n",
            "epoch 947 loss 2.303746223449707\n",
            "epoch 948 loss 2.303746223449707\n",
            "epoch 949 loss 2.303746223449707\n",
            "epoch 950 loss 2.303746223449707\n",
            "epoch 951 loss 2.303746223449707\n",
            "epoch 952 loss 2.303746223449707\n",
            "epoch 953 loss 2.303746223449707\n",
            "epoch 954 loss 2.303746223449707\n",
            "epoch 955 loss 2.303746223449707\n",
            "epoch 956 loss 2.303746223449707\n",
            "epoch 957 loss 2.303746223449707\n",
            "epoch 958 loss 2.303746223449707\n",
            "epoch 959 loss 2.303746223449707\n",
            "epoch 960 loss 2.303746223449707\n",
            "epoch 961 loss 2.303746223449707\n",
            "epoch 962 loss 2.303746223449707\n",
            "epoch 963 loss 2.303746223449707\n",
            "epoch 964 loss 2.303746223449707\n",
            "epoch 965 loss 2.303746223449707\n",
            "epoch 966 loss 2.303746223449707\n",
            "epoch 967 loss 2.303746223449707\n",
            "epoch 968 loss 2.303746223449707\n",
            "epoch 969 loss 2.303746223449707\n",
            "epoch 970 loss 2.303746223449707\n",
            "epoch 971 loss 2.303746223449707\n",
            "epoch 972 loss 2.303746223449707\n",
            "epoch 973 loss 2.303746223449707\n",
            "epoch 974 loss 2.303746223449707\n",
            "epoch 975 loss 2.303746223449707\n",
            "epoch 976 loss 2.303746223449707\n",
            "epoch 977 loss 2.303746223449707\n",
            "epoch 978 loss 2.303746223449707\n",
            "epoch 979 loss 2.303746223449707\n",
            "epoch 980 loss 2.303746223449707\n",
            "epoch 981 loss 2.303746223449707\n",
            "epoch 982 loss 2.303746223449707\n",
            "epoch 983 loss 2.303746223449707\n",
            "epoch 984 loss 2.303746223449707\n",
            "epoch 985 loss 2.303746223449707\n",
            "epoch 986 loss 2.303746223449707\n",
            "epoch 987 loss 2.303746223449707\n",
            "epoch 988 loss 2.303746223449707\n",
            "epoch 989 loss 2.303746223449707\n",
            "epoch 990 loss 2.303746223449707\n",
            "epoch 991 loss 2.303746223449707\n",
            "epoch 992 loss 2.303746223449707\n",
            "epoch 993 loss 2.303746223449707\n",
            "epoch 994 loss 2.303746223449707\n",
            "epoch 995 loss 2.303746223449707\n",
            "epoch 996 loss 2.303746223449707\n",
            "epoch 997 loss 2.303746223449707\n",
            "epoch 998 loss 2.303746223449707\n",
            "epoch 999 loss 2.303746223449707\n",
            "epoch 1000 loss 2.303746223449707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAAKUlEQVR4nO3NMQEAAAjDMIZ/0ZhgXyqgSZJpttU7AAAAAAAAAAAAeOwAtIkAQ5gJy2sAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FA2D8AD40D0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}